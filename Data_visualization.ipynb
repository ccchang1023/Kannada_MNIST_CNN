{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torchsummary import summary\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from imgaug import augmenters as iaa\n",
    "# from imgaug.augmentables.segmaps import SegmentationMapOnImage\n",
    "\n",
    "class ImgAugTransform:\n",
    "    def __init__(self):\n",
    "        self.aug = iaa.Sequential([\n",
    "#         iaa.Scale((640, 480)),\n",
    "#         iaa.Fliplr(0.5),\n",
    "          iaa.Sometimes(0.8,iaa.PerspectiveTransform(scale=(0,0.2)))  \n",
    "#         iaa.Sometimes(0.5, iaa.GaussianBlur(sigma=(0, 0.6))),\n",
    "#         iaa.Sometimes(0.1, iaa.AverageBlur(1.2)),\n",
    "#         iaa.Sometimes(1, iaa.Affine(rotate=(-20, 20),order=[0, 1],translate_px={\"x\":(-2, 2),\"y\":(-2,2)},mode='symmetric')),\n",
    "#         iaa.Sometimes(0.2,iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.25))),\n",
    "#         iaa.Sometimes(0.1, iaa.SaltAndPepper(0.05,False)),\n",
    "#         iaa.Invert(0.5),\n",
    "#         iaa.Add((-5, 5)), # change brightness of images (by -10 to 10 of original value)\n",
    "#         iaa.AdditiveGaussianNoise(-1,1)\n",
    "#         iaa.Sometimes(0.2,iaa.GammaContrast(2))\n",
    "            \n",
    "#         iaa.AddToHueAndSaturation(from_colorspace=\"GRAY\",value=(-20, 20))  #Hue-> color, saturation -> saido\n",
    "    ])\n",
    "    def __call__(self, img, mask=None):\n",
    "        img = np.array(img)        \n",
    "        return self.aug.augment_image(image=img)\n",
    "#         return self.aug(image=img, segmentation_maps=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([\n",
    "        transforms.ColorJitter(0.5,0.5,0,0),\n",
    "#         transforms.RandomAffine(degrees=15,translate=(0.25,0.25),scale=(0.75,1.25),shear=8),  #normal    \n",
    "#         transforms.Resize((224,224)), #For resnet\n",
    "#         transforms.RandomAffine(degrees=2,translate=(0.05,0.05)), #For 01 classifier\n",
    "#         transforms.RandomAffine(degrees=10,translate=(0.1,0.1),scale=(0.9,1.1)),  #Used in Chris Deotte avgpool\n",
    "#         transforms.RandomAffine(degrees=10,translate=(0.2,0.2),scale=[0.9,1.1]), #For native distinguisher\n",
    "#         ImgAugTransform(),\n",
    "#         lambda x: Image.fromarray(x),\n",
    "        transforms.RandomAffine(degrees=15,translate=(0.015,0.015),scale=(0.9,1.1),shear=3),  #for Se_res18    \n",
    "        transforms.ToTensor(),  #Take Image as input and convert to tensor with value from 0 to1  \n",
    "#         transforms.Normalize(mean=[0.08889289],std=[0.24106446])  #train_large dataset distribution\n",
    "#         transforms.Normalize(mean=[0.08229437],std=[0.23876116]) #train dataset dist\n",
    "#         transforms.Normalize(mean=[0.09549136],std=[0.24336776]) #dig_augmented distribution\n",
    "    ])\n",
    "\n",
    "trans_dig = transforms.Compose([\n",
    "        transforms.ToTensor(),  #Take Image as input and convert to tensor with value from 0 to1\n",
    "#         transforms.Normalize(mean=[0.08889289],std=[0.24106446])  #train_large dataset distribution\n",
    "#         transforms.Normalize(mean=[0.08229437],std=[0.23876116]) #train dataset dist\n",
    "#         transforms.Normalize(mean=[0.09549136],std=[0.24336776]) #dig_augmented distribution\n",
    "    ])\n",
    "\n",
    "trans_val = transforms.Compose([\n",
    "        transforms.ToTensor(),  #Take Image as input and convert to tensor with value from 0 to1\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# train distribution: mean=[0.08229437],std=[0.23876116]\n",
    "# dig augmented distribution: mean=[0.09549136],std=[0.24336776]\n",
    "# train large distribution: mean=[0.08889286],std=[0.24106438]\n",
    "\n",
    "def get_dataset_mean_std(dataloader):\n",
    "    print(\"Calculate distribution:\")\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    nb_samples = 0.\n",
    "    for data in dataloader:\n",
    "        img = data[0].to(device)\n",
    "        batch_samples = img.size(0)\n",
    "        img = img.contiguous().view(batch_samples, img.size(1), -1)\n",
    "        mean += img.mean(2).sum(0)\n",
    "        std += img.std(2).sum(0)\n",
    "        nb_samples += batch_samples\n",
    "        if nb_samples%5120 == 0:\n",
    "            print(\"Finished:\", nb_samples)\n",
    "            \n",
    "    print(\"num of samples:\",nb_samples)\n",
    "    mean /= nb_samples\n",
    "    std /= nb_samples\n",
    "#     print(\"Average mean:\",mean)\n",
    "#     print(\"Average std:\", std)\n",
    "    return mean.cpu().numpy(), std.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMnist Dataset Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_data = pd.read_csv(\"./dataset/train.csv\")\n",
    "# global_aug_dig = pd.read_csv(\"./dataset/Dig-Mnist-Augmented.csv\")\n",
    "global_data = pd.read_csv(\"./dataset/Dig-MNIST.csv\")\n",
    "# global_test_data = pd.read_csv(\"./dataset/test.csv\")\n",
    "# global_pseudo_data = pd.read_csv(\"./dataset/train_test_psuedo_65k.csv\")\n",
    "# global_critic01_data = pd.read_csv(\"./dataset/critic01_20k.csv\")\n",
    "# global_data = pd.read_csv(\"./dataset/train_digtop1_69548.csv\")\n",
    "# global_data = pd.read_csv(\"./dataset/digtop1_9548.csv\")\n",
    "\n",
    "\n",
    "class KMnistDataset(Dataset):\n",
    "    def __init__(self,data_len=None, is_validate=False,validate_rate=None,indices=None):\n",
    "        self.is_validate = is_validate\n",
    "        self.data = global_data\n",
    "        if data_len == None:\n",
    "            data_len = len(self.data)\n",
    "        \n",
    "        self.indices = indices\n",
    "        if self.is_validate:\n",
    "            self.len = int(data_len*validate_rate)\n",
    "            self.offset = int(data_len*(1-validate_rate))\n",
    "            self.transform = trans_val\n",
    "        else:\n",
    "            self.len = int(data_len*(1-validate_rate))\n",
    "            self.offset = 0\n",
    "            self.transform = trans\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        idx += self.offset\n",
    "        idx = self.indices[idx]\n",
    "        img = self.data.iloc[idx, 1:].values.astype(np.uint8).reshape((28, 28))  #value: 0~255\n",
    "        label = self.data.iloc[idx, 0]  #(num,)\n",
    "        img = Image.fromarray(img)\n",
    "        img = self.transform(img)     #value: 0~1, shape:(1,28,28)\n",
    "        label = torch.as_tensor(label, dtype=torch.uint8)    #value: 0~9, shape(1)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "num_workers = 0\n",
    "vr = 0\n",
    "k = 5\n",
    "indices_len = 60000\n",
    "# indices_len = 10240\n",
    "# indices_len = 120000\n",
    "\n",
    "###Single dataset\n",
    "indices = np.arange(indices_len)\n",
    "train_dataset = KMnistDataset(data_len=None,is_validate=False,validate_rate=vr,indices=indices)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "# mean, std = get_dataset_mean_std(train_loader)\n",
    "# print(\"train distribution: mean={},std={}\".format(mean, std))\n",
    "\n",
    "for idx,data in enumerate(train_loader):\n",
    "    img, label = data\n",
    "    img, label = img.to(device), label.to(device)\n",
    "#     img = img.cpu().numpy()\n",
    "#     img = np.uint8(img*255).reshape(batch_size,-1)\n",
    "#     label = label.cpu().numpy()\n",
    "    \n",
    "    fig,axes = plt.subplots(1,1,figsize=(2,2))\n",
    "    img = img[0]\n",
    "    origin_img = img.clone()\n",
    "    axes.imshow(origin_img.cpu().numpy().reshape(28,28),cmap='gray')\n",
    "#     img = transforms.functional.adjust_brightness(img., 0.1)\n",
    "#     axes[1].imshow(img.cpu().numpy().reshape(28,28),cmap='gray')\n",
    "    plt.pause(.1)\n",
    "    if idx!=0 and idx%10 ==0 :\n",
    "        stop\n",
    "        input('stop')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_data = pd.read_csv(\"./dataset/critic01_20k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for idx in range(0,100):\n",
    "\n",
    "    i = idx\n",
    "    img = global_data.iloc[i, 1:].values.astype(np.uint8).reshape((28, 28))  #value: 0~255\n",
    "    img = Image.fromarray(img)\n",
    "    label = global_data.iloc[i, 0]\n",
    "    \n",
    "    if i%10==0:\n",
    "        plt.pause(.1)\n",
    "        fig, axes = plt.subplots(1,10,figsize=(16,2))\n",
    "    axes[i%10].imshow(img,cmap=\"gray\")\n",
    "    \n",
    "    #     img = trans(img).cpu().numpy().reshape(28,28)\n",
    "    #     img = transforms.functional.adjust_brightness(img, 0.01)\n",
    "    #     img = transforms.functional.adjust_contrast(img, 0.01)\n",
    "    #     img = transforms.functional.adjust_saturation(img, 5)\n",
    "#         axes[j][1].imshow(img,cmap=\"gray\")\n",
    "    \n",
    "#     img = global_data_dig.iloc[i, 1:].values.astype(np.uint8).reshape((28, 28))  #value: 0~255\n",
    "#     img = Image.fromarray(img)\n",
    "#     label = global_data_dig.iloc[i, 0]\n",
    "#     axes[2].imshow(img,cmap=\"gray\")\n",
    "#     img = trans(img).cpu().numpy().reshape(28,28)\n",
    "#     axes[3].imshow(img,cmap=\"gray\")\n",
    "    \n",
    "\n",
    "    print(\"Label:\",label)\n",
    "\n",
    "# data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Imgaug Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_counter = np.zeros((5,))\n",
    "for i in range(0,1000):\n",
    "    img = global_data.iloc[i, 1:].values.astype(np.uint8).reshape((28, 28))  #value: 0~255\n",
    "    img = Image.fromarray(img)\n",
    "    label = data.iloc[i, 0]\n",
    "    img = np.array(img)\n",
    "    t = time.clock()\n",
    "    iaa.GaussianBlur(sigma=(0, 0.6)).augment_image(img)\n",
    "    time_counter[0] += time.clock()-t\n",
    "    \n",
    "    t = time.clock()\n",
    "    iaa.AverageBlur(2).augment_image(img)\n",
    "    time_counter[1] += time.clock()-t\n",
    "    \n",
    "    \n",
    "    t = time.clock()\n",
    "    iaa.MedianBlur(3).augment_image(img)\n",
    "    time_counter[2] += time.clock()-t\n",
    "    \n",
    "    t = time.clock()\n",
    "    iaa.MotionBlur(180,-1).augment_image(img)\n",
    "    time_counter[3] += time.clock()-t    \n",
    "\n",
    "print(time_counter/1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diff between train_set and Dig_val set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub: Convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class convNet(nn.Module):\n",
    "    def __init__(self,in_channels):\n",
    "        super(convNet,self).__init__()\n",
    "        self.c1 = nn.Conv2d(in_channels=in_channels, out_channels=64,kernel_size=3,stride=1,padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=64,eps=1e-3,momentum=0.01)\n",
    "        self.c2 = nn.Conv2d(64,64,3,1,0)\n",
    "        self.bn2 = nn.BatchNorm2d(64,1e-3,0.01)\n",
    "        self.c3 = nn.Conv2d(64,64,5,1,2)\n",
    "        self.bn3 = nn.BatchNorm2d(64,1e-3,0.01)\n",
    "        self.m1 = nn.MaxPool2d(2)\n",
    "        self.d1 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.c4 = nn.Conv2d(64,128,3,1,0)\n",
    "        self.bn4 = nn.BatchNorm2d(128,1e-3,0.01)\n",
    "        self.c5 = nn.Conv2d(128,128,3,1,0)\n",
    "        self.bn5 = nn.BatchNorm2d(128,1e-3,0.01)\n",
    "        self.c6 = nn.Conv2d(128,128,5,1,2)\n",
    "        self.bn6 = nn.BatchNorm2d(128,1e-3,0.01)        \n",
    "        self.m2 = nn.MaxPool2d(2)\n",
    "        self.d2 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.c7 = nn.Conv2d(128,256,3,1,0)\n",
    "        self.bn7 = nn.BatchNorm2d(256,1e-3,0.01)\n",
    "        self.m3 = nn.MaxPool2d(2)\n",
    "        self.d3 = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc1 = nn.Linear(256*1*1,256)\n",
    "        self.bn8 = nn.BatchNorm1d(256,1e-3,0.01)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256,128)\n",
    "        self.bn9 = nn.BatchNorm1d(128,1e-3,0.01)\n",
    "        \n",
    "        self.out = nn.Linear(128,10)\n",
    "        \n",
    "        self.init_linear_weights()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.bn1(F.relu(self.c1(x)))\n",
    "        x = self.bn2(F.relu(self.c2(x)))\n",
    "        x = self.bn3(F.relu(self.c3(x)))\n",
    "        x = self.m1(x)\n",
    "        x = self.d1(x)\n",
    "        \n",
    "        x = self.bn4(F.relu(self.c4(x)))\n",
    "        x = self.bn5(F.relu(self.c5(x)))\n",
    "        x = self.bn6(F.relu(self.c6(x)))\n",
    "        x = self.m2(x)\n",
    "        x = self.d2(x)\n",
    "        \n",
    "        x = self.bn7(F.relu(self.c7(x)))\n",
    "        x = self.m3(x)\n",
    "        x = self.d3(x)        \n",
    "        \n",
    "        x = x.view(-1, 256*1*1) #reshape\n",
    "        \n",
    "        x = self.bn8(self.fc1(x))\n",
    "        x = self.bn9(self.fc2(x))\n",
    "        \n",
    "        return self.out(x)\n",
    "    \n",
    "    def init_linear_weights(self):\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in')  #default mode: fan_in\n",
    "        nn.init.kaiming_normal_(self.fc2.weight, mode='fan_in')\n",
    "        nn.init.kaiming_normal_(self.out.weight, mode='fan_in')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub: Conv_avgpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class convNet_avp(nn.Module):\n",
    "    def __init__(self,in_channels):\n",
    "        super(convNet_avp,self).__init__()\n",
    "        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, \n",
    "        #                dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "        self.c1 = nn.Conv2d(in_channels=in_channels, out_channels=64,kernel_size=3,stride=1,padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=64,eps=1e-3,momentum=0.01)\n",
    "        self.c2 = nn.Conv2d(64,64,3,1,0)\n",
    "        self.bn2 = nn.BatchNorm2d(64,1e-3,0.01)\n",
    "        self.c3 = nn.Conv2d(64,64,5,2,2)  #Use strides 2 instead of maxpooling\n",
    "        self.bn3 = nn.BatchNorm2d(64,1e-3,0.01)\n",
    "        self.d1 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.c4 = nn.Conv2d(64,128,3,1,0)\n",
    "        self.bn4 = nn.BatchNorm2d(128,1e-3,0.01)\n",
    "        self.c5 = nn.Conv2d(128,128,3,1,0)\n",
    "        self.bn5 = nn.BatchNorm2d(128,1e-3,0.01)\n",
    "        self.c6 = nn.Conv2d(128,128,5,2,2)\n",
    "        self.bn6 = nn.BatchNorm2d(128,1e-3,0.01)        \n",
    "        self.d2 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.c7 = nn.Conv2d(128,256,4,1,0)\n",
    "        self.bn7 = nn.BatchNorm2d(256,1e-3,0.01)\n",
    "        self.d3 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(256*1*1,256)\n",
    "        self.bn8 = nn.BatchNorm1d(256,1e-3,0.01)\n",
    "        \n",
    "        self.out = nn.Linear(256,10)\n",
    "#         self.init_linear_weights()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.bn1(F.relu(self.c1(x)))\n",
    "        x = self.bn2(F.relu(self.c2(x)))\n",
    "        x = self.bn3(F.relu(self.c3(x)))\n",
    "        x = self.d1(x)\n",
    "        \n",
    "        x = self.bn4(F.relu(self.c4(x)))\n",
    "        x = self.bn5(F.relu(self.c5(x)))\n",
    "        x = self.bn6(F.relu(self.c6(x)))\n",
    "        x = self.d2(x)\n",
    "        \n",
    "        x = self.bn7(F.relu(self.c7(x)))\n",
    "        x = self.d3(x)\n",
    "\n",
    "        x = x.view(-1, 256*1*1) #reshape\n",
    "        x = self.bn8(self.fc1(x))\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub: SE_Net3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Seq_Ex_Block(nn.Module):\n",
    "    def __init__(self, in_ch, r=16):\n",
    "        super(Seq_Ex_Block, self).__init__()\n",
    "        self.se = nn.Sequential(\n",
    "            GlobalAvgPool(),\n",
    "            nn.Linear(in_ch, in_ch//r),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_ch//r, in_ch),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        se_weight = self.se(x).unsqueeze(-1).unsqueeze(-1)\n",
    "#         print(f'x:{x.sum()}, x_se:{x.mul(se_weight).sum()}')\n",
    "        return x.mul(se_weight)\n",
    "\n",
    "class GlobalAvgPool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalAvgPool, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x.view(*(x.shape[:-2]),-1).mean(-1)\n",
    "\n",
    "class SE_Net3(nn.Module):\n",
    "    def __init__(self,in_channels):\n",
    "        super(SE_Net3,self).__init__()\n",
    "        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, \n",
    "        #                dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "        self.c1 = nn.Conv2d(in_channels=in_channels, out_channels=64,kernel_size=3,stride=1,padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=64,eps=1e-3,momentum=0.01)\n",
    "        self.c2 = nn.Conv2d(64,64,3,1,0)\n",
    "        self.bn2 = nn.BatchNorm2d(64,1e-3,0.01)\n",
    "        self.c3 = nn.Conv2d(64,64,3,1,1)\n",
    "        self.bn3 = nn.BatchNorm2d(64,1e-3,0.01)\n",
    "        self.c4 = nn.Conv2d(64,64,5,1,2)\n",
    "        self.bn4 = nn.BatchNorm2d(64,1e-3,0.01)        \n",
    "        \n",
    "        self.m1 = nn.MaxPool2d(2)\n",
    "        self.d1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.c5 = nn.Conv2d(64,128,3,1,0)\n",
    "        self.bn5 = nn.BatchNorm2d(128,1e-3,0.01)\n",
    "        self.c6 = nn.Conv2d(128,128,3,1,0)\n",
    "        self.bn6 = nn.BatchNorm2d(128,1e-3,0.01)\n",
    "        self.c7 = nn.Conv2d(128,128,3,1,1)\n",
    "        self.bn7 = nn.BatchNorm2d(128,1e-3,0.01)\n",
    "        self.c8 = nn.Conv2d(128,128,5,1,2)\n",
    "        self.bn8 = nn.BatchNorm2d(128,1e-3,0.01)\n",
    "        \n",
    "        self.m2 = nn.MaxPool2d(2)\n",
    "        self.d2 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.c9 = nn.Conv2d(128,256,3,1,0)\n",
    "        self.bn9 = nn.BatchNorm2d(256,1e-3,0.01)\n",
    "        self.c10 = nn.Conv2d(256,256,3,1,1)\n",
    "        self.bn10 = nn.BatchNorm2d(256,1e-3,0.01)\n",
    "        \n",
    "        self.se1 = Seq_Ex_Block(in_ch=256,r=16)\n",
    "        self.m3 = nn.MaxPool2d(2)\n",
    "        self.d3 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc1 = nn.Linear(256*1*1,256)\n",
    "        self.bn11 = nn.BatchNorm1d(256,1e-3,0.01)\n",
    "        self.out = nn.Linear(256,10)\n",
    "        \n",
    "        self.init_linear_weights()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.bn1(F.leaky_relu(self.c1(x),0.05))\n",
    "        x = self.bn2(F.leaky_relu(self.c2(x),0.05))\n",
    "        x = self.bn3(F.leaky_relu(self.c3(x),0.05))\n",
    "        x = self.bn4(F.leaky_relu(self.c4(x),0.05))\n",
    "        x = self.d1(self.m1(x))\n",
    "        \n",
    "        x = self.bn5(F.leaky_relu(self.c5(x),0.05))\n",
    "        x = self.bn6(F.leaky_relu(self.c6(x),0.05))\n",
    "        x = self.bn7(F.leaky_relu(self.c7(x),0.05))\n",
    "        x = self.bn8(F.leaky_relu(self.c8(x),0.05))\n",
    "        x = self.d2(self.m2(x))\n",
    "        \n",
    "        x = self.bn9(F.leaky_relu(self.c9(x),0.05))\n",
    "        x = self.bn10(F.leaky_relu(self.c10(x),0.05))\n",
    "        x = self.se1(x)\n",
    "        x = self.d3(self.m3(x))\n",
    "        \n",
    "        x = x.view(-1, 256*1*1) #reshape\n",
    "        x = self.bn11(F.leaky_relu(self.fc1(x),0.05))\n",
    "        return self.out(x)\n",
    "    \n",
    "    def init_linear_weights(self):\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in')  #default mode: fan_in\n",
    "        nn.init.kaiming_normal_(self.out.weight, mode='fan_in')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub:SE_Net2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Seq_Ex_Block(nn.Module):\n",
    "    def __init__(self, in_ch, r=16):\n",
    "        super(Seq_Ex_Block, self).__init__()\n",
    "        self.se = nn.Sequential(\n",
    "            GlobalAvgPool(),\n",
    "            nn.Linear(in_ch, in_ch//r),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_ch//r, in_ch),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        se_weight = self.se(x).unsqueeze(-1).unsqueeze(-1)\n",
    "#         print(f'x:{x.sum()}, x_se:{x.mul(se_weight).sum()}')\n",
    "        return x.mul(se_weight)\n",
    "\n",
    "class GlobalAvgPool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalAvgPool, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x.view(*(x.shape[:-2]),-1).mean(-1)\n",
    "\n",
    "class SE_Net2(nn.Module):\n",
    "    def __init__(self,in_channels):\n",
    "        super(SE_Net2,self).__init__()\n",
    "        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, \n",
    "        #                dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "        self.c1 = nn.Conv2d(in_channels=in_channels, out_channels=64,kernel_size=3,stride=1,padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=64,eps=1e-3,momentum=0.01)\n",
    "        self.c2 = nn.Conv2d(64,64,3,1,0)\n",
    "        self.bn2 = nn.BatchNorm2d(64,1e-3,0.01)\n",
    "        self.c3 = nn.Conv2d(64,64,5,1,2)\n",
    "        self.bn3 = nn.BatchNorm2d(64,1e-3,0.01)\n",
    "        self.se1 = Seq_Ex_Block(in_ch=64,r=8)\n",
    "        self.m1 = nn.MaxPool2d(2)\n",
    "        self.d1 = nn.Dropout(0.4)\n",
    "        \n",
    "        self.c4 = nn.Conv2d(64,128,3,1,0)\n",
    "        self.bn4 = nn.BatchNorm2d(128,1e-3,0.01)\n",
    "        self.c5 = nn.Conv2d(128,128,3,1,0)\n",
    "        self.bn5 = nn.BatchNorm2d(128,1e-3,0.01)\n",
    "        self.c6 = nn.Conv2d(128,128,5,1,2)\n",
    "        self.bn6 = nn.BatchNorm2d(128,1e-3,0.01)\n",
    "        self.se2 = Seq_Ex_Block(in_ch=128,r=8)\n",
    "        self.m2 = nn.MaxPool2d(2)\n",
    "        self.d2 = nn.Dropout(0.4)\n",
    "        \n",
    "        self.c7 = nn.Conv2d(128,256,3,1,0)\n",
    "        self.bn7 = nn.BatchNorm2d(256,1e-3,0.01)\n",
    "        self.se3 = Seq_Ex_Block(in_ch=256,r=8)\n",
    "        self.m3 = nn.MaxPool2d(2)\n",
    "        self.d3 = nn.Dropout(0.4)\n",
    "\n",
    "        self.fc1 = nn.Linear(256*1*1,256)\n",
    "        self.bn8 = nn.BatchNorm1d(256,1e-3,0.01)\n",
    "        \n",
    "        self.out = nn.Linear(256,10)\n",
    "        \n",
    "        self.init_linear_weights()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.bn1(F.leaky_relu(self.c1(x),0.1))\n",
    "        x = self.bn2(F.leaky_relu(self.c2(x),0.1))\n",
    "        x = self.bn3(F.leaky_relu(self.c3(x),0.1))\n",
    "        x = self.se1(x)\n",
    "        x = self.d1(self.m1(x))\n",
    "        \n",
    "        x = self.bn4(F.leaky_relu(self.c4(x),0.1))\n",
    "        x = self.bn5(F.leaky_relu(self.c5(x),0.1))\n",
    "        x = self.bn6(F.leaky_relu(self.c6(x),0.1))\n",
    "        x = self.se2(x)\n",
    "        x = self.d2(self.m2(x))\n",
    "        \n",
    "        x = self.bn7(F.leaky_relu(self.c7(x),0.1))\n",
    "        x = self.se3(x)\n",
    "        x = self.d3(self.m3(x))\n",
    "        \n",
    "        x = x.view(-1, 256*1*1) #reshape\n",
    "        x = self.bn8(F.relu(self.fc1(x),0.1))\n",
    "        return self.out(x)\n",
    "    \n",
    "    def init_linear_weights(self):\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in')  #default mode: fan_in\n",
    "        nn.init.kaiming_normal_(self.out.weight, mode='fan_in')\n",
    "\n",
    "\n",
    "class convNet(nn.Module):\n",
    "    def __init__(self,in_channels):\n",
    "        super(convNet,self).__init__()\n",
    "        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, \n",
    "        #                dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "        self.c1 = nn.Conv2d(in_channels=in_channels, out_channels=64,kernel_size=3,stride=1,padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=64,eps=1e-3,momentum=0.01)\n",
    "        self.c2 = nn.Conv2d(64,64,3,1,0)\n",
    "        self.bn2 = nn.BatchNorm2d(64,1e-3,0.01)\n",
    "        self.c3 = nn.Conv2d(64,64,5,1,2)\n",
    "        self.bn3 = nn.BatchNorm2d(64,1e-3,0.01)\n",
    "        self.m1 = nn.MaxPool2d(2)\n",
    "        self.d1 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.c4 = nn.Conv2d(64,128,3,1,0)\n",
    "        self.bn4 = nn.BatchNorm2d(128,1e-3,0.01)\n",
    "        self.c5 = nn.Conv2d(128,128,3,1,0)\n",
    "        self.bn5 = nn.BatchNorm2d(128,1e-3,0.01)\n",
    "        self.c6 = nn.Conv2d(128,128,5,1,2)\n",
    "        self.bn6 = nn.BatchNorm2d(128,1e-3,0.01)        \n",
    "        self.m2 = nn.MaxPool2d(2)\n",
    "        self.d2 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.c7 = nn.Conv2d(128,256,3,1,0)\n",
    "        self.bn7 = nn.BatchNorm2d(256,1e-3,0.01)\n",
    "        self.m3 = nn.MaxPool2d(2)\n",
    "        self.d3 = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc1 = nn.Linear(256*1*1,256)\n",
    "        self.bn8 = nn.BatchNorm1d(256,1e-3,0.01)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256,128)\n",
    "        self.bn9 = nn.BatchNorm1d(128,1e-3,0.01)\n",
    "        \n",
    "        self.out = nn.Linear(128,10)\n",
    "        \n",
    "        self.init_linear_weights()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.bn1(F.relu(self.c1(x)))\n",
    "        x = self.bn2(F.relu(self.c2(x)))\n",
    "        x = self.bn3(F.relu(self.c3(x)))\n",
    "        x = self.m1(x)\n",
    "        x = self.d1(x)\n",
    "        \n",
    "        x = self.bn4(F.relu(self.c4(x)))\n",
    "        x = self.bn5(F.relu(self.c5(x)))\n",
    "        x = self.bn6(F.relu(self.c6(x)))\n",
    "        x = self.m2(x)\n",
    "        x = self.d2(x)\n",
    "        \n",
    "        x = self.bn7(F.relu(self.c7(x)))\n",
    "        x = self.m3(x)\n",
    "        x = self.d3(x)        \n",
    "        \n",
    "        x = x.view(-1, 256*1*1) #reshape\n",
    "        \n",
    "        x = self.bn8(self.fc1(x))\n",
    "        x = self.bn9(self.fc2(x))\n",
    "        \n",
    "        return self.out(x)\n",
    "    \n",
    "    def init_linear_weights(self):\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in')  #default mode: fan_in\n",
    "        nn.init.kaiming_normal_(self.fc2.weight, mode='fan_in')\n",
    "        nn.init.kaiming_normal_(self.out.weight, mode='fan_in')\n",
    "        \n",
    "class convNet_native(nn.Module):\n",
    "    def __init__(self,in_channels):\n",
    "        super(convNet_native,self).__init__()\n",
    "        self.c1 = nn.Conv2d(in_channels=in_channels, out_channels=64,kernel_size=5,stride=1,padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=64,momentum=0.1)\n",
    "        self.c2 = nn.Conv2d(64,64,5,1,2)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=64,momentum=0.1)\n",
    "        self.m1 = nn.MaxPool2d(2)\n",
    "        self.d1 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.c3 = nn.Conv2d(64,128,5,1,2)\n",
    "        self.bn3 = nn.BatchNorm2d(128,0.1)\n",
    "        self.c4 = nn.Conv2d(128,128,5,1,2)\n",
    "        self.bn4 = nn.BatchNorm2d(128,0.1)\n",
    "        self.m2 = nn.MaxPool2d(2)\n",
    "        self.d2 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.c5 = nn.Conv2d(128,256,3,1,1)\n",
    "        self.bn5 = nn.BatchNorm2d(256,0.1)\n",
    "        self.c6 = nn.Conv2d(256,256,3,1,1)\n",
    "        self.bn6 = nn.BatchNorm2d(256,0.1)\n",
    "        self.m3 = nn.MaxPool2d(2)\n",
    "        self.d3 = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc = nn.Linear(256*3*3,256)  #layer for binary entropy\n",
    "        self.d4 = nn.Dropout(0.2)\n",
    "        self.out = nn.Linear(256,2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.leaky_relu(self.bn1(self.c1(x)),negative_slope=0.1)\n",
    "        x = F.leaky_relu(self.bn2(self.c2(x)),0.1)\n",
    "        x = self.m1(x)\n",
    "        x = self.d1(x)\n",
    "        \n",
    "        x = F.leaky_relu(self.bn3(self.c3(x)),0.1)\n",
    "        x = F.leaky_relu(self.bn4(self.c4(x)),0.1)\n",
    "        x = self.m2(x)\n",
    "        x = self.d2(x)\n",
    "        \n",
    "        x = F.leaky_relu(self.bn5(self.c5(x)),0.1)\n",
    "        x = F.leaky_relu(self.bn6(self.c6(x)),0.1)\n",
    "        x = self.m3(x)\n",
    "        x = self.d3(x)\n",
    "        \n",
    "        x = x.view(-1, 256*3*3) #reshape\n",
    "        x_b = F.leaky_relu(self.fc(x),0.1)\n",
    "        x_b = self.d4(x_b)\n",
    "        return self.out(x_b)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# native_model = convNet_native(in_channels=1)\n",
    "# native_model.cuda()\n",
    "# native_model.load_state_dict(torch.load(\"Kmnist_saved_model/ensemble/native_classifier/Fold0_loss0.0242_acc_b99.704_without_aug\"))\n",
    "# native_model.eval()\n",
    "\n",
    "dig_model = convNet(in_channels=1)\n",
    "dig_model.cuda()\n",
    "dig_model.load_state_dict(torch.load(\"Kmnist_saved_model/ensemble_dig/SE_net2_60k/dig_Fold0_loss0.0093_acc99.720\"))\n",
    "dig_model.eval()\n",
    "\n",
    "# se_model = SE_Net2(in_channels=1)\n",
    "# se_model.cuda()\n",
    "# se_model.load_state_dict(torch.load(\"Kmnist_saved_model/ensemble_dig/SE_net2_60k/dig_Fold0_loss0.0093_acc99.720\"))\\\n",
    "# se_model.eval()\n",
    "\n",
    "# model = convNet(in_channels=1)\n",
    "# model.cuda()\n",
    "# model.load_state_dict(torch.load(\"Kmnist_saved_model/ensemble/10_fold_tuned_cnn/adam/Fold8_loss0.0033_acc99.917\"))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model num: 5\n",
      "finished: 10240\n",
      "(10240,) (0,) (10240,)\n"
     ]
    }
   ],
   "source": [
    "vr = 1\n",
    "indices=np.arange(10240)\n",
    "# indices=np.arange(60000)\n",
    "dataset = KMnistDataset(data_len=None,is_validate=True, validate_rate=vr,indices=indices)\n",
    "loader = DataLoader(dataset, batch_size=128, shuffle=False, num_workers=8)\n",
    "\n",
    "## Test Native Classifier\n",
    "# counter = 0\n",
    "# data_num = 0\n",
    "# with torch.no_grad():\n",
    "#     for idx,data in enumerate(loader):\n",
    "#         img, label = data\n",
    "#         img, label = img.to(device), label.to(device)\n",
    "#         img = data[0].to(device)\n",
    "        \n",
    "#         _,pred_native = torch.max(native_model(img),dim=1)\n",
    "#         counter += (pred_native.cpu().numpy()).sum()\n",
    "#         data_num += img.size(0)\n",
    "        \n",
    "# print(\"Native rate:\",counter/data_num)\n",
    "# print(\"Non Native rate:\",1-(counter/data_num))\n",
    "\n",
    "# ### Inference models\n",
    "ensemble_models = []\n",
    "# ensemble_root = \"Kmnist_saved_model/ensemble/10_fold_tuned_cnn/adam_batch1024_baseline\"\n",
    "# ensemble_root = \"Kmnist_saved_model/ensemble/5fold_senet3\"\n",
    "# ensemble_root = \"Kmnist_saved_model/ensemble/5x2fold_65k_senet2_b1024\"\n",
    "# ensemble_root = \"Kmnist_saved_model/ensemble/5x1fold_65k_senet3_b1024\"  #acc 99.00\n",
    "# ensemble_root = \"Kmnist_saved_model/ensemble/15fold-avgpool-batch1024-adam\"\n",
    "# ensemble_root = \"Kmnist_saved_model/ensemble/74k/8fold_74k_senet3_pseudo_digtop1\" #acc 99.04\n",
    "# ensemble_root = \"Kmnist_saved_model/ensemble/74k/5fold_74k_senet3_augv1\"  #acc 99.06\n",
    "# ensemble_root = \"Kmnist_saved_model/ensemble/5fold_74k_senet3_augv2\"  #acc ??\n",
    "# ensemble_root = \"Kmnist_saved_model/ensemble/5fold_75k_senet3_augv1\"  #acc99.10\n",
    "# ensemble_root = \"Kmnist_saved_model/ensemble/5fold_75k_senet3_sgdr\"  #acc\n",
    "# ensemble_root = \"Kmnist_saved_model/ensemble/75k/5fold_senet3_75k_Adamax\"  #acc\n",
    "# ensemble_root = \"Kmnist_saved_model/ensemble/75k/5fold_senet3_75k_AdamW\"  #acc\n",
    "# ensemble_root = \"Kmnist_saved_model/ensemble/75k/5fold_senet3_75k_RMSprop\"  #acc\n",
    "# ensemble_root = \"Kmnist_saved_model/ensemble/75124/5fold_75124_senet3_augv1\"  #acc99.18\n",
    "\n",
    "# ensemble_root = \"Kmnist_saved_model/final_submit/adam\"\n",
    "# ensemble_root = \"Kmnist_saved_model/final_submit/adam2/origin_60k_5fold\"\n",
    "# ensemble_root = \"Kmnist_saved_model/final_submit/optimizer_mix\"\n",
    "# ensemble_root = \"Kmnist_saved_model/final_submit/RMSProp\"\n",
    "# ensemble_root = \"Kmnist_saved_model/final_submit/adam2/65k_5fold_more_iteration\"\n",
    "# ensemble_root = \"Kmnist_saved_model/final_submit/adam2/65k_5fold\"\n",
    "ensemble_root = \"Kmnist_saved_model/Step9\"\n",
    "\n",
    "for file_name in os.listdir(ensemble_root):\n",
    "    if file_name.find(\"Fold\") == -1:\n",
    "        continue\n",
    "#     model = convNet(in_channels=1)\n",
    "#     model = SE_Net2(in_channels=1)\n",
    "    model = SE_Net3(in_channels=1)\n",
    "#     model = convNet_avp(in_channels=1)\n",
    "\n",
    "    model.cuda()\n",
    "    model.load_state_dict(torch.load(\"{}/{}\".format(ensemble_root,file_name)))\n",
    "    model.eval()\n",
    "    ensemble_models.append(model)\n",
    "\n",
    "model_num = len(ensemble_models)\n",
    "print(\"model num:\",model_num)\n",
    "\n",
    "# result = np.empty((0,3))\n",
    "result = np.array([])\n",
    "\n",
    "result_dig = np.array([])\n",
    "labels = np.array([])\n",
    "data_num = 0\n",
    "with torch.no_grad():\n",
    "    for idx,data in enumerate(loader):\n",
    "        img, label = data\n",
    "        img, label = img.to(device), label.to(device)\n",
    "        \n",
    "        ###Average Ensemble\n",
    "        pred_list = torch.Tensor([]).to(device)\n",
    "        for i in range(model_num):\n",
    "            pred = ensemble_models[i](img) #(batch_num,10)\n",
    "            pred_list = torch.cat((pred_list,pred.unsqueeze(2)),dim=2) #pred_list: (batch_num,10,model_num)\n",
    "        pred = torch.mean(pred_list,dim=2)   #(batch,10)\n",
    "        _,pred = torch.max(pred.data, 1)   #(batch_num,)        \n",
    "#         _,pred = torch.topk(pred,3)  #(batch_num,k), get topk result\n",
    "        \n",
    "        result = np.concatenate([result,pred.cpu().numpy()],axis=0)\n",
    "        labels = np.concatenate([labels,label.cpu().numpy()],axis=0)\n",
    "        data_num += img.size(0)\n",
    "\n",
    "print(\"finished:\",data_num)\n",
    "print(np.shape(result),np.shape(result_dig),np.shape(labels))\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#60k test:\n",
    "    #step 1~9: 9992 9994 9993 9994 9993 9994 9994 9994 9991\n",
    "    #5fold 75124: 9990 \n",
    "    #65k_pseudo_only: 9988\n",
    "    \n",
    "#10240 test:\n",
    "    #step1-7: 9773 9880 9896 9909 9912 9912 9914 9917 9920\n",
    "    #5fold 75124: 9907\n",
    "    #65k_pseudo_only: 9045    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9921    0.9854    0.9887      1024\n",
      "         1.0     0.9855    0.9971    0.9913      1024\n",
      "         2.0     0.9903    0.9961    0.9932      1024\n",
      "         3.0     0.9961    0.9922    0.9941      1024\n",
      "         4.0     0.9990    0.9922    0.9956      1024\n",
      "         5.0     0.9951    0.9980    0.9966      1024\n",
      "         6.0     0.9779    0.9951    0.9864      1024\n",
      "         7.0     0.9902    0.9854    0.9878      1024\n",
      "         8.0     0.9971    0.9912    0.9941      1024\n",
      "         9.0     0.9970    0.9873    0.9921      1024\n",
      "\n",
      "    accuracy                         0.9920     10240\n",
      "   macro avg     0.9920    0.9920    0.9920     10240\n",
      "weighted avg     0.9920    0.9920    0.9920     10240\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 69.0, 'Predicted label')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sn\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "###Confusion matrix\n",
    "print(classification_report(labels, result,digits=4))\n",
    "plt.figure(figsize=(10,10))\n",
    "confusion_mat = confusion_matrix(labels, result)\n",
    "sn.heatmap(confusion_mat, annot=True, cmap='YlGnBu',fmt=\"d\",linewidths=.5, linecolor='w')\n",
    "plt.title('Confusion matrix of Real World validation result')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices1 = np.where(result!=labels)[0]       #(num)\n",
    "# indices2 = np.where(result_dig!=labels)[0]\n",
    "# indices3 = np.where((result!=labels)&(result_dig==labels))[0]\n",
    "indices4 = np.where((result[:,0]==labels))[0]\n",
    "indices5 = np.where((result[:,0]==labels)|(result[:,1]==labels))[0]\n",
    "indices6 = np.where((result[:,0]==labels)|(result[:,1]==labels)|(result[:,2]==labels))[0]\n",
    "\n",
    "cirtic_idx_list_ans0 = []\n",
    "idx_list_ans1 = []\n",
    "\n",
    "cirtic_idx_list_ans9 = []\n",
    "label_list = []\n",
    "count = 0\n",
    "\n",
    "data_num = len(labels)\n",
    "print(\"top1 acc:\",len(indices4)/data_num)\n",
    "print(\"top2 acc:\",len(indices5)/data_num)\n",
    "print(\"top3 acc:\",len(indices6)/data_num)\n",
    "stop\n",
    "\n",
    "for i in range(0,len(indices1)):\n",
    "    idx = indices1[i]\n",
    "#     img1 = global_dig.iloc[idx, 1:].values.astype(np.uint8).reshape((28, 28))  #value: 0~255\n",
    "#     img1 = global_pseudo_data.iloc[idx, 1:].values.astype(np.uint8).reshape((28, 28))  #value: 0~255\n",
    "    img1 = global_data.iloc[idx, 1:].values.astype(np.uint8).reshape((28, 28))  #value: 0~255\n",
    "    img1 = Image.fromarray(img1)\n",
    "    label1 = result[idx]\n",
    "#     label2 = result_dig[idx]\n",
    "    label2 = None\n",
    "    label = labels[idx]\n",
    "  \n",
    "\n",
    "#     if label == 0:\n",
    "#         fig, axes = plt.subplots(1,1,figsize=(2,2))\n",
    "# #         cirtic_idx_list_ans0.append(idx)\n",
    "#         axes.imshow(img1,cmap=\"gray\")\n",
    "#         print(idx)\n",
    "#         print(\"Model:\",label1,\" Model2:\",label2,\" Label:\",label)\n",
    "#         plt.pause(.1)\n",
    "#     else:\n",
    "#         continue\n",
    "        \n",
    "\n",
    "# print(idx_list_ans1)\n",
    "# np.save(\"idx_ans1\",idx_list_ans1)\n",
    "\n",
    "# print(cirtic_idx_list_ans0)\n",
    "# print(cirtic_idx_list_ans9)\n",
    "\n",
    "cirtic_idx_list_ans0_v2 = [5520,6150,12280,18560,32730]\n",
    "np.save(\"critic_idx_ans0_v2\",cirtic_idx_list_ans0_v2)\n",
    "\n",
    "# plt.figure(figsize=(8,4))\n",
    "# data = np.uint8(cirtic_label_list)\n",
    "# print(len(data))\n",
    "# plt.hist(data ,density=0,align=\"mid\",bins=10,rwidth=1)\n",
    "# plt.xticks(range(10))\n",
    "# plt.ylabel('frequency')\n",
    "\n",
    "# plt.figure(figsize=(8,4))\n",
    "# data = np.uint8(label_list)\n",
    "# print(len(data))\n",
    "# plt.hist(data ,density=0,align=\"mid\",bins=10,rwidth=1)\n",
    "# plt.xticks(range(10))\n",
    "# plt.ylabel('frequency')\n",
    "\n",
    "\n",
    "# 31 1 0 59969"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect top-1 corrected data dig dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(9340,)\n"
     ]
    }
   ],
   "source": [
    "dig_idx_list = np.array([])\n",
    "# dig_idx_list = np.load('top1_digidx3.npy')\n",
    "print(len(dig_idx_list))\n",
    "\n",
    "indices = np.where((result[:,0]==labels))[0]  #get top1 corrected index\n",
    "# indices = np.where((result[:,0]==labels)|(result[:,1]==labels))[0]  #get top2 corrected index\n",
    "dig_idx_list = np.hstack([dig_idx_list,indices])\n",
    "dig_idx_list = np.unique(dig_idx_list).astype(int)\n",
    "print(np.shape(dig_idx_list))\n",
    "np.save(\"final_digidx_9340_s1.npy\",dig_idx_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine train.csv with dig dataset in top k index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10240, 785)\n",
      "(9340, 785)\n"
     ]
    }
   ],
   "source": [
    "pix_str = \"pixel0\"\n",
    "for i in range(1,784):\n",
    "    pix_str = pix_str + \",pixel\" + str(i)\n",
    "numpy_header = \"label,\" + pix_str\n",
    "\n",
    "# top1_digidx = np.load(\"top1_digidx_len9367.npy\")\n",
    "# top1_digidx = np.load(\"top1_digidx_10k_acc9906.npy\")\n",
    "top1_digidx = np.load(\"final_digidx_9340_s1.npy\")\n",
    "origin_data = pd.read_csv(\"./dataset/train.csv\")\n",
    "dig_data = pd.read_csv(\"./dataset/Dig-MNIST.csv\")\n",
    "\n",
    "train_csv = np.array(origin_data).astype(int)\n",
    "tmp_csv = np.array(dig_data).astype(int)\n",
    "print(np.shape(tmp_csv))\n",
    "top1_dig_csv = []\n",
    "for idx in top1_digidx:\n",
    "    top1_dig_csv.append(tmp_csv[idx])\n",
    "\n",
    "top1_dig_csv = np.array(top1_dig_csv).astype(int)\n",
    "print(np.shape(top1_dig_csv))\n",
    "np.savetxt(\"./dataset_final/digtop1_9340_s1.csv\", top1_dig_csv, delimiter=\",\",fmt=\"%d\",header=numpy_header,comments='')\n",
    "\n",
    "# new_csv = np.vstack([train_csv,top1_dig_csv])\n",
    "# np.random.shuffle(new_csv)\n",
    "# np.random.shuffle(new_csv)\n",
    "# np.random.shuffle(new_csv)\n",
    "# print(\"new_csv_shape:\",np.shape(new_csv))\n",
    "# np.savetxt(\"./dataset/train_digtop1_69367.csv\", new_csv, delimiter=\",\",fmt=\"%d\",header=numpy_header,comments='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine train.csv and Dig augmented, add additional \"native writer label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pix_str = \"pixel0\"\n",
    "for i in range(1,784):\n",
    "    pix_str = pix_str + \",pixel\" + str(i)\n",
    "numpy_header = \"native_label,label,\" + pix_str\n",
    "\n",
    "origin_data = pd.read_csv(\"./dataset/train.csv\")\n",
    "dig_aug_data = pd.read_csv(\"./dataset/Dig-Mnist-Augmented.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = np.array(origin_data).astype(int)\n",
    "dig_aug_csv = np.array(dig_aug_data).astype(int)\n",
    "native_label0 = np.zeros((60000,1)).astype(int)\n",
    "native_label1 = np.ones((60000,1)).astype(int)\n",
    "\n",
    "train_csv = np.concatenate([native_label1,train_csv],axis=1)\n",
    "dig_aug_csv = np.concatenate([native_label0,dig_aug_csv],axis=1)\n",
    "\n",
    "print(np.shape(train_csv))\n",
    "print(np.shape(dig_aug_csv))\n",
    "new_csv = np.vstack([train_csv,dig_aug_csv])\n",
    "print(np.shape(new_csv))\n",
    "np.savetxt(\"./dataset/train_large.csv\", new_csv, delimiter=\",\",fmt=\"%d\",header=numpy_header,comments='')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment dig dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vr = 0\n",
    "batch_size = 1024\n",
    "dataset = KMnistDataset(data_len=None,is_validate=False, validate_rate=vr)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "num_per_class = 6000  # 6000 * 10 =60000\n",
    "\n",
    "pix_str = \"pixel0\"\n",
    "for i in range(1,784):\n",
    "    pix_str = pix_str + \",pixel\" + str(i)\n",
    "numpy_header = \"label,\" + pix_str\n",
    "\n",
    "dig_data = pd.read_csv(\"./dataset/Dig-MNIST.csv\")\n",
    "# dig_data.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_list = np.empty((0,785))\n",
    "counter_list = np.zeros((10,)).astype(int)\n",
    "\n",
    "###Load original 10240 data\n",
    "origin_img = dig_data.iloc[:, 1:].values.astype(np.uint8).reshape((-1,784))  #value: 0~255\n",
    "origin_label = np.array(dig_data.iloc[:,0]).astype(int)\n",
    "\n",
    "for i in range(len(origin_img)):\n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "    tmp_img = origin_img[i]    #(784,)\n",
    "    tmp_label = origin_label[i].reshape(-1) #(1,)\n",
    "    csv_arr = np.concatenate([tmp_label,tmp_img])   #(785,)\n",
    "    aug_list = np.vstack((aug_list,csv_arr))\n",
    "    counter_list[tmp_label] += 1\n",
    "\n",
    "print(\"shape of aug_list:\",np.shape(aug_list))\n",
    "print(\"counter_list:\",counter_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Augment data by random select and affine\n",
    "keep_loop = True\n",
    "while keep_loop:\n",
    "    for idx,data in enumerate(loader):\n",
    "        img, label = data\n",
    "        img, label = img.to(device), label.to(device)\n",
    "\n",
    "        img = img.cpu().numpy()\n",
    "        img = np.uint8(img*255).reshape(batch_size,-1)\n",
    "        label = label.cpu().numpy()\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            tmp_img = img[i]    #(784,)\n",
    "            tmp_label = label[i].reshape(-1) #(1,)\n",
    "            if counter_list[tmp_label] >= num_per_class:\n",
    "                continue\n",
    "#             print(np.shape(tmp_label),np.shape(tmp_img))\n",
    "            csv_arr = np.concatenate([tmp_label,tmp_img])   #(785,)\n",
    "#             print(\"csv arr shape:\",np.shape(csv_arr))\n",
    "            \n",
    "            aug_list = np.vstack((aug_list,csv_arr))\n",
    "#             print(\"aug_list shape:\",np.shape(aug_list))\n",
    "            counter_list[tmp_label] += 1\n",
    "\n",
    "        print(\"shape of aug_list:\",np.shape(aug_list))\n",
    "        print(\"counter_list:\",counter_list)\n",
    "        if (counter_list>=num_per_class).all() == True:\n",
    "            print(\"Augment Finished\")\n",
    "            print(np.shape(aug_list))\n",
    "            print(counter_list)\n",
    "            np.savetxt(\"./dataset/Dig-Mnist-Augmented.csv\", aug_list, delimiter=\",\",fmt=\"%d\",header=numpy_header,comments='')\n",
    "            keep_loop = False\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (5000, 785)\n",
      "model num: 5\n",
      "Inference complete: (5000,)\n"
     ]
    }
   ],
   "source": [
    "trans_test = transforms.Compose([\n",
    "        transforms.ToTensor(),  #Take Image as input and convert to tensor with value from 0 to1\n",
    "    ])\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,data_len=None):\n",
    "        self.data = pd.read_csv(\"./dataset/test.csv\")\n",
    "        print(\"data shape:\", np.shape(self.data))\n",
    "        self.transform = trans_test\n",
    "        if data_len == None:\n",
    "            self.len = len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.data.iloc[idx, 1:].values.astype(np.uint8).reshape((28, 28))  #value: 0~255\n",
    "        img = Image.fromarray(img)\n",
    "        img = self.transform(img)     #value: 0~1, shape:(1,28,28)\n",
    "        return img, torch.Tensor([])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "test_dataset = TestDataset(data_len=None)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=0)\n",
    "\n",
    "### Inference models\n",
    "ensemble_models = []\n",
    "# ensemble_root = \"Kmnist_saved_model/ensemble/5x1fold_65k_senet3_b1024\"   #99.00%\n",
    "# ensemble_root = \"Kmnist_saved_model/ensemble/5fold_74k_senet3_augv1\"       #99.06%\n",
    "# ensemble_root = \"Kmnist_saved_model/ensemble/5fold_75k_senet3_augv1\"  #acc99.10\n",
    "\n",
    "# ensemble_root = \"Kmnist_saved_model/final_submit/origin_60k_5fold\"\n",
    "ensemble_root = \"Kmnist_saved_model/final_submit/adam2/origin_60k_5fold\"\n",
    "\n",
    "for file_name in os.listdir(ensemble_root):\n",
    "    if file_name.find(\"Fold\") == -1:\n",
    "        continue\n",
    "    model = SE_Net3(in_channels=1)\n",
    "    model.cuda()\n",
    "    model.load_state_dict(torch.load(\"{}/{}\".format(ensemble_root,file_name)))\n",
    "    model.eval()\n",
    "    ensemble_models.append(model)\n",
    "\n",
    "model_num = len(ensemble_models)\n",
    "print(\"model num:\",model_num)\n",
    "\n",
    "psuedo_labels = np.array([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx,data in enumerate(test_loader):\n",
    "            img, label = data\n",
    "            img, label = img.to(device), label.to(device)\n",
    "\n",
    "            ###Average Ensemble\n",
    "            pred_list = torch.Tensor([]).to(device)\n",
    "            for i in range(model_num):\n",
    "                pred = ensemble_models[i](img) #(batch_num,10)\n",
    "                pred_list = torch.cat((pred_list,pred.unsqueeze(2)),dim=2) #pred_list: (batch_num,10,model_num)\n",
    "            pred = torch.mean(pred_list,dim=2)   #(batch,10)\n",
    "            _,pred = torch.max(pred.data, 1)   #(batch_num,)        \n",
    "    #         _,pred = torch.max(model(img),dim=1)\n",
    "\n",
    "            psuedo_labels = np.concatenate([psuedo_labels,pred.cpu().numpy()],axis=0)\n",
    "            data_num += img.size(0)\n",
    "            \n",
    "print(\"Inference complete:\",np.shape(psuedo_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 784)\n",
      "shape psuedo_labels: (5000,)\n",
      "(5000, 785)\n"
     ]
    }
   ],
   "source": [
    "pix_str = \"pixel0\"\n",
    "for i in range(1,784):\n",
    "    pix_str = pix_str + \",pixel\" + str(i)\n",
    "numpy_header = \"label,\" + pix_str\n",
    "\n",
    "test_csv = np.array(pd.read_csv(\"./dataset/test.csv\")).astype(int)[:,1:]\n",
    "\n",
    "print(np.shape(test_csv))\n",
    "\n",
    "pseudo_labels = psuedo_labels.reshape(-1,1).astype(int)\n",
    "print(\"shape psuedo_labels:\",np.shape(psuedo_labels))\n",
    "test_csv = np.concatenate([pseudo_labels,test_csv],axis=1)\n",
    "\n",
    "print(np.shape(test_csv))\n",
    "np.savetxt(\"./dataset_final/test_pseu_s1.csv\", test_csv, delimiter=\",\",fmt=\"%d\",header=numpy_header,comments='')\n",
    "# test_csv = pd.read_csv(\"./dataset_final/test_pseu_s1.csv\")\n",
    "# test_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmented psuedo_label test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_aug = transforms.Compose([\n",
    "        transforms.ColorJitter(0.5,0.2,0,0),\n",
    "        transforms.RandomAffine(degrees=10,translate=(0.1,0.1),scale=(0.9,1.1)),\n",
    "        transforms.ToTensor(),  #Take Image as input and convert to tensor with value from 0 to1\n",
    "    ])\n",
    "\n",
    "class AugDataset(Dataset):\n",
    "    def __init__(self,data_len=None):\n",
    "        self.data = pd.read_csv(\"./dataset/test_psuedo_label.csv\")\n",
    "        print(\"data shape:\", np.shape(self.data))\n",
    "        self.transform = trans_aug\n",
    "        if data_len == None:\n",
    "            self.len = len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.data.iloc[idx, 1:].values.astype(np.uint8).reshape((28, 28))  #value: 0~255\n",
    "        label = self.data.iloc[idx, 0]  #(num,)\n",
    "        img = Image.fromarray(img)\n",
    "        img = self.transform(img)     #value: 0~1, shape:(1,28,28)\n",
    "        label = torch.as_tensor(label, dtype=torch.uint8)    #value: 0~9, shape(1)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "vr = 0\n",
    "batch_size = 1024\n",
    "dataset = AugDataset(data_len=None)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=10)\n",
    "num_per_class = 6000  # 6000 * 10 =60000\n",
    "aug_list = np.empty((0,785))\n",
    "counter_list = np.zeros((10,)).astype(int)\n",
    "pix_str = \"pixel0\"\n",
    "for i in range(1,784):\n",
    "    pix_str = pix_str + \",pixel\" + str(i)\n",
    "numpy_header = \"label,\" + pix_str\n",
    "    \n",
    "###Augment data by random select and affine\n",
    "keep_loop = True\n",
    "while keep_loop:\n",
    "    for idx,data in enumerate(loader):\n",
    "        img, label = data\n",
    "        img, label = img.to(device), label.to(device)\n",
    "\n",
    "        img = img.cpu().numpy()\n",
    "        img = np.uint8(img*255).reshape(-1,784)\n",
    "        label = label.cpu().numpy()\n",
    "        \n",
    "        for i in range(img.shape[0]):\n",
    "            tmp_img = img[i]    #(784,)\n",
    "            tmp_label = label[i].reshape(-1) #(1,)\n",
    "            if counter_list[tmp_label] >= num_per_class:\n",
    "                continue\n",
    "#             print(np.shape(tmp_label),np.shape(tmp_img))\n",
    "            csv_arr = np.concatenate([tmp_label,tmp_img])   #(785,)\n",
    "#             print(\"csv arr shape:\",np.shape(csv_arr))\n",
    "            \n",
    "            aug_list = np.vstack((aug_list,csv_arr))\n",
    "#             print(\"aug_list shape:\",np.shape(aug_list))\n",
    "            counter_list[tmp_label] += 1\n",
    "\n",
    "        print(\"shape of aug_list:\",np.shape(aug_list))\n",
    "        print(\"counter_list:\",counter_list)\n",
    "        if (counter_list>=num_per_class).all() == True:\n",
    "            print(\"Augment Finished\")\n",
    "            print(np.shape(aug_list))\n",
    "            print(counter_list)\n",
    "            np.savetxt(\"./dataset/test_psuedo_augmented.csv\", aug_list, delimiter=\",\",fmt=\"%d\",header=numpy_header,comments='')\n",
    "            keep_loop = False\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine train.csv with test_psuedo_aug.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 785)\n",
      "(5000, 785)\n",
      "(9340, 785)\n",
      "(65000, 785)\n",
      "(74340, 785)\n"
     ]
    }
   ],
   "source": [
    "pix_str = \"pixel0\"\n",
    "for i in range(1,784):\n",
    "    pix_str = pix_str + \",pixel\" + str(i)\n",
    "numpy_header = \"label,\" + pix_str\n",
    "\n",
    "train_csv = np.array(pd.read_csv(\"./dataset_final/train.csv\")).astype(int)\n",
    "test_csv = np.array(pd.read_csv(\"./dataset_final/test_pseu_s1.csv\")).astype(int)\n",
    "digtop1_csv = np.array(pd.read_csv(\"./dataset_final/digtop1_9340_s1.csv\")).astype(int)\n",
    "\n",
    "# indices = np.where(test_csv[:,0]!=test_csv2[:,0])[0]\n",
    "# print(len(indices))\n",
    "\n",
    "print(np.shape(train_csv))\n",
    "print(np.shape(test_csv))\n",
    "print(np.shape(digtop1_csv))\n",
    "\n",
    "\n",
    "###Combine train, pseudo\n",
    "new_csv = np.vstack([train_csv,test_csv])\n",
    "for i in range(5):\n",
    "    np.random.shuffle(new_csv)  #Multi-dimensional arrays are only shuffled along the first axis:\n",
    "print(np.shape(new_csv))\n",
    "np.savetxt(\"./dataset_final/train_pseu_65k_s1.csv\", new_csv, delimiter=\",\",fmt=\"%d\",header=numpy_header,comments='')\n",
    "\n",
    "###Combine train, pseudo and digtop1 \n",
    "new_csv = np.vstack([train_csv,test_csv,digtop1_csv])\n",
    "for i in range(5):\n",
    "    np.random.shuffle(new_csv)  #Multi-dimensional arrays are only shuffled along the first axis:\n",
    "print(np.shape(new_csv))\n",
    "np.savetxt(\"./dataset_final/train_pseu_dig_74340_s1.csv\", new_csv, delimiter=\",\",fmt=\"%d\",header=numpy_header,comments='')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmented train dataset with critic example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " global_data = pd.read_csv(\"./dataset/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_aug = transforms.Compose([\n",
    "        transforms.ColorJitter(0.9,0.2,0,0),\n",
    "        transforms.RandomAffine(degrees=20,translate=(0.1,0.05),scale=(0.8,1.05)),\n",
    "        transforms.ToTensor(),  #Take Image as input and convert to tensor with value from 0 to1\n",
    "    ])\n",
    "\n",
    "class AugDataset(Dataset):\n",
    "    def __init__(self,critic_idx_list=None):\n",
    "        self.data = global_data\n",
    "        self.transform = trans_aug\n",
    "        self.critic_idx_list = critic_idx_list\n",
    "        print(\"data len:\", np.shape(self.critic_idx_list))\n",
    "        self.len = len(critic_idx_list)\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        idx = self.critic_idx_list[idx]\n",
    "        img = self.data.iloc[idx, 1:].values.astype(np.uint8).reshape((28, 28))  #value: 0~255\n",
    "        label = self.data.iloc[idx, 0]  #(num,)\n",
    "        img = Image.fromarray(img)\n",
    "        img = self.transform(img)     #value: 0~1, shape:(1,28,28)\n",
    "        label = torch.as_tensor(label, dtype=torch.uint8)    #value: 0~9, shape(1)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "batch_size = 10\n",
    "critic_idx_list = np.load(\"./critic_idx_ans0.npy\")\n",
    "# critic_idx_list = np.load(\"./idx_ans1.npy\")\n",
    "dataset = AugDataset(critic_idx_list)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "aug_list = np.empty((0,785))\n",
    "counter_list = np.zeros((10,)).astype(int)\n",
    "pix_str = \"pixel0\"\n",
    "for i in range(1,784):\n",
    "    pix_str = pix_str + \",pixel\" + str(i)\n",
    "numpy_header = \"label,\" + pix_str\n",
    "    \n",
    "###Augment data by random select and affine\n",
    "data_num = 0\n",
    "for ep in range(50000):\n",
    "    for idx,data in enumerate(loader):\n",
    "        img, label = data\n",
    "        img, label = img.to(device), label.to(device)\n",
    "\n",
    "        img = img.cpu().numpy()\n",
    "        img = np.uint8(img*255).reshape(-1,784)\n",
    "        label = label.cpu().numpy().reshape(-1,1)\n",
    "        \n",
    "#         batch_num = np.shape(img)[0]\n",
    "#         fig,axes = plt.subplots(1,batch_num,figsize=(10,2))\n",
    "#         for i in range(batch_num):\n",
    "#             axes[i].imshow(img[i].reshape(28,28),cmap='gray')\n",
    "#         plt.pause(.1)\n",
    "        \n",
    "        csv_arr = np.hstack([label,img])\n",
    "        aug_list = np.vstack([aug_list,csv_arr])\n",
    "        data_num += img.shape[0]\n",
    "        if data_num %2000==0:\n",
    "            print(data_num)\n",
    "        if data_num > 10000:\n",
    "            print(\"shape of aug_list:\",np.shape(aug_list))\n",
    "            aug_list = aug_list[:10000]\n",
    "            np.savetxt(\"./dataset/critic0_10k_complicated.csv\", aug_list, delimiter=\",\",fmt=\"%d\",header=numpy_header,comments='')\n",
    "            stop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine label1_10k with critic0_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pix_str = \"pixel0\"\n",
    "for i in range(1,784):\n",
    "    pix_str = pix_str + \",pixel\" + str(i)\n",
    "numpy_header = \"label,\" + pix_str\n",
    "\n",
    "critic0_csv = np.array(pd.read_csv(\"./dataset/critic0_10k_complicated.csv\")).astype(int)\n",
    "label1_csv = np.array(pd.read_csv(\"./dataset/label1_10k.csv\")).astype(int)\n",
    "\n",
    "print(np.shape(critic0_csv))\n",
    "print(np.shape(label1_csv))\n",
    "new_csv = np.vstack([critic0_csv,label1_csv])\n",
    "np.random.shuffle(new_csv)  #Multi-dimensional arrays are only shuffled along the first axis:\n",
    "np.random.shuffle(new_csv)\n",
    "np.random.shuffle(new_csv)\n",
    "\n",
    "print(np.shape(new_csv))\n",
    "np.savetxt(\"./dataset/critic01_20k_complicated.csv\", new_csv, delimiter=\",\",fmt=\"%d\",header=numpy_header,comments='')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.random.random([1000,])\n",
    "# global_data = pd.read_csv(\"./dataset/train_large.csv\")\n",
    "global_data = pd.read_csv(\"./dataset/train.csv\")\n",
    "# global_data_test = pd.read_csv(\"./dataset/train_test_psuedo_aug.csv\")\n",
    "global_data_test = pd.read_csv(\"./dataset/train_test_psuedo_65k.csv\")\n",
    "\n",
    "# data1 = global_data.iloc[:10,1:]\n",
    "# data2 = global_data_test.iloc[:,0]\n",
    "\n",
    "train_digtop1= pd.read_csv(\"./dataset/digtop1_9548.csv\")\n",
    "plt.hist(train_digtop1 ,density=0,label=True,rwidth=0.3)\n",
    "plt.xticks(range(0,10))\n",
    "\n",
    "# print((label==5).sum().item())\n",
    "# plt.ylabel('frequency')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
