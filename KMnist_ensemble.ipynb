{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torchsummary import summary\n",
    "device = \"cuda\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class convNet(nn.Module):\n",
    "    def __init__(self,in_channels):\n",
    "        super(convNet,self).__init__()\n",
    "        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, \n",
    "        #                dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "        self.c1 = nn.Conv2d(in_channels=in_channels, out_channels=64,kernel_size=5,stride=1,padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=64,momentum=0.1)\n",
    "        self.c2 = nn.Conv2d(64,64,5,1,2)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=64,momentum=0.1)\n",
    "        self.m1 = nn.MaxPool2d(2)\n",
    "        self.d1 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.c3 = nn.Conv2d(64,128,5,1,2)\n",
    "        self.bn3 = nn.BatchNorm2d(128,0.1)\n",
    "        self.c4 = nn.Conv2d(128,128,5,1,2)\n",
    "        self.bn4 = nn.BatchNorm2d(128,0.1)\n",
    "        self.m2 = nn.MaxPool2d(2)\n",
    "        self.d2 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.c5 = nn.Conv2d(128,256,3,1,1)\n",
    "        self.bn5 = nn.BatchNorm2d(256,0.1)\n",
    "        self.c6 = nn.Conv2d(256,256,3,1,1)\n",
    "        self.bn6 = nn.BatchNorm2d(256,0.1)\n",
    "        self.m3 = nn.MaxPool2d(2)\n",
    "        self.d3 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(256*3*3,256)\n",
    "        self.d4 = nn.Dropout(0.1)\n",
    "        self.fc2 = nn.Linear(256,128)\n",
    "        self.d5 = nn.Dropout(0.1)\n",
    "        self.out = nn.Linear(128,10)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.leaky_relu(self.bn1(self.c1(x)),negative_slope=0.1)\n",
    "        x = F.leaky_relu(self.bn2(self.c2(x)),0.1)\n",
    "        x = self.m1(x)\n",
    "        x = self.d1(x)\n",
    "        \n",
    "        x = F.leaky_relu(self.bn3(self.c3(x)),0.1)\n",
    "        x = F.leaky_relu(self.bn4(self.c4(x)),0.1)\n",
    "        x = self.m2(x)\n",
    "        x = self.d2(x)\n",
    "        \n",
    "        x = F.leaky_relu(self.bn5(self.c5(x)),0.1)\n",
    "        x = F.leaky_relu(self.bn6(self.c6(x)),0.1)\n",
    "        x = self.m3(x)\n",
    "        x = self.d3(x)\n",
    "        \n",
    "        x = x.view(-1, 256*3*3) #reshape\n",
    "        x = F.leaky_relu(self.fc1(x),0.1)\n",
    "        x = self.d4(x)\n",
    "        x = F.leaky_relu(self.fc2(x),0.1)\n",
    "        x = self.d5(x)\n",
    "        return self.out(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    #Advance model\n",
    "    # model_name = 'efficientnet-b0'\n",
    "    # image_size = EfficientNet.get_image_size(model_name)\n",
    "    # model = EfficientNet.from_pretrained(model_name, num_classes=10)\n",
    "    # model = model.to(device)\n",
    "\n",
    "    #Basic cnn\n",
    "#     model = convNet(in_channels=1)\n",
    "#     model.cuda()\n",
    "#     # model.load_state_dict(torch.load(\"./Kmnist_saved_model/ep20_acc0.9910\"))\n",
    "\n",
    "    #pretrained model\n",
    "    model = models.resnet18()\n",
    "    model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,bias=False)\n",
    "    model.cuda()\n",
    "# #     summary(model, input_size=(1, 28, 28))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([\n",
    "#         transforms.RandomResizedCrop(28),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.RandomVerticalFlip(),\n",
    "        transforms.ColorJitter(0.5,0.2,0.2,0.5),\n",
    "        transforms.RandomAffine(degrees=10,translate=(0.2,0.2),scale=[0.7,1.1],shear=15),\n",
    "        transforms.ToTensor(),  #Take Image as input and convert to tensor with value from 0 to1  \n",
    "        transforms.Normalize(mean=[0.06525399], std=[0.20466233])  #comment this line when calculate ditribution\n",
    "#         transforms.Normalize(mean=[0.1126489],std=[0.28132638]) ################Temp Revised Caution##############\n",
    "    ])\n",
    "\n",
    "trans_val = transforms.Compose([\n",
    "        transforms.ToTensor(),  #Take Image as input and convert to tensor with value from 0 to1\n",
    "#         transforms.Normalize(mean=[0.1126489],std=[0.28132638])  #dig_val distribution\n",
    "        transforms.Normalize(mean=[0.06525399], std=[0.20466233])  #comment this line when calculate ditribution    \n",
    "    ])\n",
    "\n",
    "trans_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.06525399], std=[0.20466233])  #average distribution between train and test\n",
    "\n",
    "])\n",
    "\n",
    "global_data = pd.read_csv(\"./dataset/train.csv\")\n",
    "global_dig_val_data = pd.read_csv(\"./dataset/Dig-MNIST.csv\")\n",
    "\n",
    "class KMnistDataset(Dataset):\n",
    "    def __init__(self,data_len=None, is_validate=False,validate_rate=None,indices=None):\n",
    "        self.is_validate = is_validate\n",
    "        self.data = global_data\n",
    "#         self.data = global_dig_val_data    ################Temp Revised Caution##############\n",
    "        \n",
    "#         print(\"data shape:\", np.shape(self.data))\n",
    "        if data_len == None:\n",
    "            data_len = len(self.data)\n",
    "        \n",
    "        self.indices = indices\n",
    "        if self.is_validate:\n",
    "            self.len = int(data_len*validate_rate)\n",
    "            self.offset = int(data_len*(1-validate_rate))\n",
    "            self.transform = trans_val\n",
    "        else:\n",
    "            self.len = int(data_len*(1-validate_rate))\n",
    "            self.offset = 0\n",
    "            self.transform = trans\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        idx += self.offset\n",
    "        idx = self.indices[idx]\n",
    "#         print(idx)\n",
    "        img = self.data.iloc[idx, 1:].values.astype(np.uint8).reshape((28, 28))  #value: 0~255\n",
    "        label = self.data.iloc[idx, 0]  #(num,)\n",
    "        img = Image.fromarray(img)\n",
    "        img = self.transform(img)     #value: 0~1, shape:(1,28,28)\n",
    "        label = torch.as_tensor(label, dtype=torch.uint8)    #value: 0~9, shape(1)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "class DigValDataset(Dataset):\n",
    "    def __init__(self,data_len=None,indices=None):\n",
    "        self.data = global_dig_val_data\n",
    "        print(\"data shape:\", np.shape(self.data))\n",
    "        if data_len == None:\n",
    "            data_len = len(self.data)\n",
    "        self.indices = indices\n",
    "        self.len = int(data_len)\n",
    "        self.transform = trans_val\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        idx = self.indices[idx]\n",
    "        img = self.data.iloc[idx, 1:].values.astype(np.uint8).reshape((28, 28))  #value: 0~255\n",
    "        label = self.data.iloc[idx, 0]  #(num,)\n",
    "        img = Image.fromarray(img)\n",
    "        img = self.transform(img)     #value: 0~1, shape:(1,28,28)\n",
    "        label = torch.as_tensor(label, dtype=torch.uint8)    #value: 0~9, shape(1)\n",
    "        return img, label    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len    \n",
    "    \n",
    "    \n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,data_len=None):\n",
    "        self.data = pd.read_csv(\"./dataset/test.csv\")\n",
    "        print(\"data shape:\", np.shape(self.data))\n",
    "        self.transform = trans_test\n",
    "        if data_len == None:\n",
    "            self.len = len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.data.iloc[idx, 1:].values.astype(np.uint8).reshape((28, 28))  #value: 0~255\n",
    "        img = Image.fromarray(img)\n",
    "        img = self.transform(img)     #value: 0~1, shape:(1,28,28)\n",
    "        return img, torch.Tensor([])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# train distribution: [0.06464077] [0.20316151]\n",
    "# test distribution: [0.0726126] [0.22267213]\n",
    "# dist1 = np.array([0.06464077,0.20316151])\n",
    "# dist2 = np.array([0.0726126,0.22267213])\n",
    "# dist = (dist1*60000 + dist2*5000)/65000\n",
    "# print(dist)\n",
    "\n",
    "def get_dataset_mean_std(dataloader):\n",
    "    print(\"Calculate distribution:\")\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    nb_samples = 0.\n",
    "    for data in dataloader:\n",
    "        img, label = data\n",
    "        img, label = img.to(device), label.to(device)\n",
    "        batch_samples = img.size(0)\n",
    "        img = img.contiguous().view(batch_samples, img.size(1), -1)\n",
    "        mean += img.mean(2).sum(0)\n",
    "        std += img.std(2).sum(0)\n",
    "        nb_samples += batch_samples\n",
    "        if nb_samples%5120 == 0:\n",
    "            print(\"Finished:\", nb_samples)\n",
    "            \n",
    "    print(\"num of samples:\",nb_samples)\n",
    "    mean /= nb_samples\n",
    "    std /= nb_samples\n",
    "#     print(\"Average mean:\",mean)\n",
    "#     print(\"Average std:\", std)\n",
    "    return mean.cpu().numpy(), std.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kfold_dataset_loader(k=5,val_rate=0.1,indices_len=None, batch_size=None,num_workers=None, dig_val=False):\n",
    "    ###Return [list of train dataset_loader, list of val dataset_loader]\n",
    "    train_loader_list = []\n",
    "    val_loader_list = []\n",
    "    for i in range(k):\n",
    "        indices = np.arange(indices_len)\n",
    "        np.random.shuffle(indices)\n",
    "                    \n",
    "        train_dataset = KMnistDataset(data_len=None,is_validate=False, validate_rate=val_rate,indices=indices)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "        \n",
    "        if dig_val==True:\n",
    "            indices = np.arange(10240)\n",
    "            val_dataset = DigValDataset(data_len=None,indices=indices)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "        else:\n",
    "            val_dataset = KMnistDataset(data_len=None,is_validate=True, validate_rate=val_rate, indices=indices)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "        \n",
    "        train_loader_list.append(train_loader)\n",
    "        val_loader_list.append(val_loader)\n",
    "        \n",
    "    return train_loader_list, val_loader_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "num_workers = 0\n",
    "vr = 0.02\n",
    "k = 5\n",
    "indices_len = 60000\n",
    "# indices_len = 10240  ################Temp Revised Caution##############\n",
    "\n",
    "###Single dataset\n",
    "# indices = np.arange(indices_len)\n",
    "# train_dataset = KMnistDataset(data_len=None,is_validate=False, validate_rate=vr,indices=indices)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "# mean, std = get_dataset_mean_std(train_loader)\n",
    "# print(\"train distribution:\",mean, std)\n",
    "\n",
    "# indices = np.arange(10240)\n",
    "# dig_val_dataset = DigValDataset(data_len=None,indices=indices)\n",
    "# dig_val_loader = DataLoader(dig_val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "# mean, std = get_dataset_mean_std(dig_val_loader)\n",
    "# print(\"validate distribution:\",mean, std)\n",
    "\n",
    "# test_dataset = TestDataset(data_len=None)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "# mean, std = get_dataset_mean_std(test_loader)\n",
    "# print(\"test distribution:\",mean, std)\n",
    "\n",
    "###K-fold dataset\n",
    "train_loaders, val_loaders = get_kfold_dataset_loader(k, vr, indices_len, batch_size, num_workers, dig_val=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(len(train_loaders)):\n",
    "    train_loader = train_loaders[fold]\n",
    "    val_loader = val_loaders[fold]\n",
    "    \n",
    "    print(\"=======fold:\",fold)\n",
    "    print(\"train:\")\n",
    "    idx = 0\n",
    "    for data in train_loader:\n",
    "        idx += data[0].size(0)\n",
    "        None\n",
    "    print(\"train len:\",idx)\n",
    "    \n",
    "    print(\"val:\")\n",
    "    idx = 0\n",
    "    for data in val_loader:\n",
    "        idx += data[0].size(0)\n",
    "        None\n",
    "    print(\"val len:\",idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./dataset/train.csv\")\n",
    "\n",
    "for i in range(1000,1050):\n",
    "    fig, axes = plt.subplots(1,2,figsize=(8,4))\n",
    "    img2 = data.iloc[i, 1:].values.astype(np.uint8).reshape((28, 28))  #value: 0~255\n",
    "    img2 = Image.fromarray(img2)\n",
    "    axes[0].imshow(img2,cmap=\"gray\")\n",
    "    img2 = trans(img2).cpu().numpy().reshape(28,28)\n",
    "    axes[1].imshow(img2,cmap=\"gray\")\n",
    "    plt.pause(.1)\n",
    "\n",
    "kmnist_dataset.data.head(100)\n",
    "\n",
    "type(kmnist_dataset.data.iloc[20,1:].values)  #numpy ndarray\n",
    "type(kmnist_dataset.data.iloc[20,0])  #numpy int64\n",
    "kmnist_dataset.data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "ep = 300\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=lr)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=50,T_mult=1,eta_min=1e-6) #original \n",
    "# lr_scheduler = CosineAnnealingWarmUpRestarts(optimizer,T_0=20,T_mult=3,eta_max=lr,T_up=10)  #advance\n",
    "plt.figure()\n",
    "x = list(range(ep))\n",
    "y = []\n",
    "for epoch in range(ep):\n",
    "    lr_scheduler.step()\n",
    "    lr = lr_scheduler.get_lr()\n",
    "    y.append(lr_scheduler.get_lr()[0])\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 5\n",
      "Episode:1, Validation Loss:0.00017836382903624326, Acc:96.8333%\n",
      "Episode:2, Validation Loss:0.0002792152517940849, Acc:96.5000%\n",
      "Episode:3, Validation Loss:7.548589928774163e-05, Acc:99.0833%\n",
      "Episode:4, Validation Loss:9.534245327813551e-05, Acc:98.5833%\n",
      "Episode:5, Validation Loss:0.00013908011897001415, Acc:98.3333%\n",
      "Episode:6, Validation Loss:4.4960524974158034e-05, Acc:99.3333%\n",
      "Episode:7, Validation Loss:5.2939994930056855e-05, Acc:99.2500%\n",
      "Episode:8, Validation Loss:6.379438855219632e-05, Acc:99.0833%\n",
      "Episode:9, Validation Loss:4.107864879188128e-05, Acc:99.4167%\n",
      "Episode:10, Validation Loss:4.7581987018929794e-05, Acc:99.2500%\n",
      "Episode:11, Validation Loss:4.7745677875354886e-05, Acc:99.2500%\n",
      "Episode:12, Validation Loss:3.907701830030419e-05, Acc:99.5000%\n",
      "Episode:13, Validation Loss:6.329479947453365e-05, Acc:98.9167%\n",
      "Episode:14, Validation Loss:4.1824026993708685e-05, Acc:99.3333%\n",
      "Episode:15, Validation Loss:3.742194894584827e-05, Acc:99.5000%\n",
      "Episode:16, Validation Loss:4.50443294539582e-05, Acc:99.2500%\n",
      "Episode:17, Validation Loss:3.702992034959607e-05, Acc:99.4167%\n",
      "Episode:18, Validation Loss:9.206648246617988e-05, Acc:98.8333%\n",
      "Episode:19, Validation Loss:3.572991045075469e-05, Acc:99.3333%\n",
      "Episode:20, Validation Loss:2.5951050702133216e-05, Acc:99.7500%\n",
      "Episode:21, Validation Loss:1.9070590496994555e-05, Acc:99.8333%\n",
      "Episode:22, Validation Loss:2.9680903026019223e-05, Acc:99.4167%\n",
      "Episode:23, Validation Loss:4.680499841924757e-05, Acc:99.5000%\n",
      "Episode:24, Validation Loss:4.155801798333414e-05, Acc:99.6667%\n",
      "Episode:25, Validation Loss:6.714711344102398e-05, Acc:99.1667%\n",
      "Episode:26, Validation Loss:3.096076034125872e-05, Acc:99.4167%\n",
      "Episode:27, Validation Loss:2.7328962460160255e-05, Acc:99.6667%\n",
      "Episode:28, Validation Loss:2.8976131943636574e-05, Acc:99.5000%\n",
      "Episode:29, Validation Loss:4.501928196987137e-05, Acc:99.3333%\n",
      "Episode:30, Validation Loss:4.598376472131349e-05, Acc:99.2500%\n",
      "Episode:31, Validation Loss:3.3706030080793425e-05, Acc:99.4167%\n",
      "Episode:32, Validation Loss:4.285192699171603e-05, Acc:99.5000%\n",
      "Episode:33, Validation Loss:1.6653062630211934e-05, Acc:99.6667%\n",
      "Episode:34, Validation Loss:3.938293593819253e-05, Acc:99.5000%\n",
      "Episode:35, Validation Loss:3.5266028135083616e-05, Acc:99.3333%\n",
      "Episode:36, Validation Loss:1.6228983440669253e-05, Acc:99.6667%\n",
      "Episode:37, Validation Loss:3.2861622457858175e-05, Acc:99.4167%\n",
      "Episode:38, Validation Loss:1.6048372344812378e-05, Acc:99.7500%\n",
      "Episode:39, Validation Loss:4.424431608640589e-05, Acc:99.2500%\n",
      "Episode:40, Validation Loss:2.9411829018499702e-05, Acc:99.6667%\n",
      "Episode:41, Validation Loss:2.8699379981844686e-05, Acc:99.6667%\n",
      "Episode:42, Validation Loss:1.3575157026934903e-05, Acc:99.7500%\n",
      "Episode:43, Validation Loss:1.72193867911119e-05, Acc:99.6667%\n",
      "Episode:44, Validation Loss:1.6417743609054014e-05, Acc:99.7500%\n",
      "Episode:45, Validation Loss:4.21908225689549e-05, Acc:99.3333%\n",
      "Episode:46, Validation Loss:1.3809696611133404e-05, Acc:99.7500%\n",
      "Episode:47, Validation Loss:1.8531334717408754e-05, Acc:99.8333%\n",
      "Episode:48, Validation Loss:4.496252950048074e-05, Acc:99.4167%\n",
      "Episode:49, Validation Loss:2.508406396373175e-05, Acc:99.6667%\n",
      "Episode:50, Validation Loss:1.3696881069336087e-05, Acc:99.7500%\n",
      "Episode:51, Validation Loss:1.8228085536975414e-05, Acc:99.6667%\n",
      "Episode:52, Validation Loss:2.737355862336699e-05, Acc:99.5833%\n",
      "Episode:53, Validation Loss:1.644567419134546e-05, Acc:99.7500%\n",
      "Episode:54, Validation Loss:2.22048292926047e-05, Acc:99.6667%\n",
      "Episode:55, Validation Loss:2.7621883418760262e-05, Acc:99.6667%\n",
      "Episode:56, Validation Loss:2.519326881156303e-05, Acc:99.5000%\n",
      "Episode:57, Validation Loss:2.3771857740939595e-05, Acc:99.5833%\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Episode:58, Validation Loss:2.553811646066606e-05, Acc:99.5833%\n",
      "Episode:59, Validation Loss:1.835209877754096e-05, Acc:99.5833%\n",
      "Episode:60, Validation Loss:2.1071828086860478e-05, Acc:99.5000%\n",
      "Episode:61, Validation Loss:2.132030567736365e-05, Acc:99.5000%\n",
      "Episode:62, Validation Loss:1.9275857994216494e-05, Acc:99.5833%\n",
      "Episode:63, Validation Loss:2.3480448362533934e-05, Acc:99.5000%\n",
      "Episode:64, Validation Loss:2.423604382784106e-05, Acc:99.5000%\n",
      "Episode:65, Validation Loss:1.9905077351722866e-05, Acc:99.5000%\n",
      "Episode:66, Validation Loss:2.1828906028531492e-05, Acc:99.5000%\n",
      "Episode:67, Validation Loss:2.066680826828815e-05, Acc:99.5000%\n",
      "Episode:68, Validation Loss:2.0066638171556406e-05, Acc:99.5000%\n",
      "Episode:69, Validation Loss:1.994430203922093e-05, Acc:99.5000%\n",
      "Episode:70, Validation Loss:1.825566687330138e-05, Acc:99.5000%\n",
      "Episode:71, Validation Loss:1.792718467186205e-05, Acc:99.5000%\n",
      "Episode:72, Validation Loss:1.8745340639725327e-05, Acc:99.5000%\n",
      "Episode:73, Validation Loss:2.2788670321460813e-05, Acc:99.5000%\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-05.\n",
      "===================Best Fold:0 Saved, Acc:0.9983333333333333==================\n",
      "======================================================\n",
      "Episode:1, Validation Loss:0.0001373497216263786, Acc:97.7500%\n",
      "Episode:2, Validation Loss:8.916625665733591e-05, Acc:98.5833%\n",
      "Episode:3, Validation Loss:8.117091783788055e-05, Acc:98.8333%\n",
      "Episode:4, Validation Loss:6.247017154237255e-05, Acc:99.0000%\n",
      "Episode:5, Validation Loss:5.9780646552098915e-05, Acc:98.8333%\n",
      "Episode:6, Validation Loss:7.226689922390506e-05, Acc:99.0833%\n",
      "Episode:7, Validation Loss:8.177661220543087e-05, Acc:98.6667%\n",
      "Episode:8, Validation Loss:0.00010965068213408813, Acc:99.0000%\n",
      "Episode:9, Validation Loss:2.801397022267338e-05, Acc:99.5000%\n",
      "Episode:10, Validation Loss:5.716159284929745e-05, Acc:99.2500%\n",
      "Episode:11, Validation Loss:2.88444607576821e-05, Acc:99.4167%\n",
      "Episode:12, Validation Loss:4.116460695513524e-05, Acc:99.2500%\n",
      "Episode:13, Validation Loss:5.6935474276542664e-05, Acc:99.4167%\n",
      "Episode:14, Validation Loss:5.122615038999356e-05, Acc:99.5000%\n",
      "Episode:15, Validation Loss:6.859783752588555e-05, Acc:99.0000%\n",
      "Episode:16, Validation Loss:6.415214011212811e-05, Acc:99.4167%\n",
      "Episode:17, Validation Loss:2.923186002590228e-05, Acc:99.2500%\n",
      "Episode:18, Validation Loss:3.525436841300689e-05, Acc:99.3333%\n",
      "Episode:19, Validation Loss:4.0950399124994874e-05, Acc:99.4167%\n",
      "Episode:20, Validation Loss:6.316646613413468e-05, Acc:98.8333%\n",
      "Episode:21, Validation Loss:3.208282942068763e-05, Acc:99.5833%\n",
      "Episode:22, Validation Loss:2.4892089641070925e-05, Acc:99.6667%\n",
      "Episode:23, Validation Loss:2.3263582988874987e-05, Acc:99.5833%\n",
      "Episode:24, Validation Loss:2.5341352738905698e-05, Acc:99.5000%\n",
      "Episode:25, Validation Loss:1.8192506104242057e-05, Acc:99.6667%\n",
      "Episode:26, Validation Loss:2.7981664970866404e-05, Acc:99.6667%\n",
      "Episode:27, Validation Loss:2.0155554011580534e-05, Acc:99.6667%\n",
      "Episode:28, Validation Loss:2.5429846573388204e-05, Acc:99.5000%\n",
      "Episode:29, Validation Loss:1.2786032129952218e-05, Acc:99.7500%\n",
      "Episode:30, Validation Loss:1.9932705981773324e-05, Acc:99.5833%\n",
      "Episode:31, Validation Loss:2.2833906768937595e-05, Acc:99.6667%\n",
      "Episode:32, Validation Loss:2.563845737313386e-05, Acc:99.7500%\n",
      "Episode:33, Validation Loss:2.7358495572116226e-05, Acc:99.5000%\n",
      "Episode:34, Validation Loss:3.0458284527412616e-05, Acc:99.5833%\n",
      "Episode:35, Validation Loss:1.526833511888981e-05, Acc:99.7500%\n",
      "Episode:36, Validation Loss:1.856862763816025e-05, Acc:99.6667%\n",
      "Episode:37, Validation Loss:1.6075486200861633e-05, Acc:99.5833%\n",
      "Episode:38, Validation Loss:1.3332159142009914e-05, Acc:99.6667%\n",
      "Episode:39, Validation Loss:1.9905162844224833e-05, Acc:99.5833%\n",
      "Episode:40, Validation Loss:9.021994628710672e-06, Acc:99.8333%\n",
      "Episode:41, Validation Loss:2.204520205850713e-05, Acc:99.5833%\n",
      "Episode:42, Validation Loss:1.3200771718402393e-05, Acc:99.6667%\n",
      "Episode:43, Validation Loss:1.4750580703548621e-05, Acc:99.6667%\n",
      "Episode:44, Validation Loss:1.2924179827678017e-05, Acc:99.8333%\n",
      "Episode:45, Validation Loss:1.797769800759852e-05, Acc:99.5833%\n",
      "Episode:46, Validation Loss:3.448513598414138e-05, Acc:99.6667%\n",
      "Episode:47, Validation Loss:2.6444209652254358e-05, Acc:99.7500%\n",
      "Episode:48, Validation Loss:1.1218291547265835e-05, Acc:99.9167%\n",
      "Episode:49, Validation Loss:1.8458737031323835e-05, Acc:99.6667%\n",
      "Episode:50, Validation Loss:1.1444703886809293e-05, Acc:99.7500%\n",
      "Episode:51, Validation Loss:1.3235070582595654e-05, Acc:99.7500%\n",
      "Episode:52, Validation Loss:1.023893946694443e-05, Acc:99.9167%\n",
      "Episode:53, Validation Loss:1.1976297173532657e-05, Acc:99.6667%\n",
      "Episode:54, Validation Loss:1.1011759852408431e-05, Acc:99.8333%\n",
      "Episode:55, Validation Loss:8.797470400168095e-06, Acc:100.0000%\n",
      "Episode:56, Validation Loss:5.137439984537195e-06, Acc:100.0000%\n",
      "Episode:57, Validation Loss:1.2232664630573709e-05, Acc:99.8333%\n",
      "Episode:58, Validation Loss:8.997778422781266e-06, Acc:99.8333%\n",
      "Episode:59, Validation Loss:2.926445631601382e-05, Acc:99.5833%\n",
      "Episode:60, Validation Loss:1.2506773600762244e-05, Acc:99.7500%\n",
      "Episode:61, Validation Loss:6.734790076734498e-06, Acc:99.9167%\n",
      "Episode:62, Validation Loss:8.226074896811042e-06, Acc:99.7500%\n",
      "Episode:63, Validation Loss:9.851246431935579e-06, Acc:99.8333%\n",
      "Episode:64, Validation Loss:7.24348728908808e-06, Acc:100.0000%\n",
      "Episode:65, Validation Loss:1.2484402759582736e-05, Acc:99.7500%\n",
      "Episode:66, Validation Loss:1.7201122318510897e-05, Acc:99.7500%\n",
      "Episode:67, Validation Loss:1.1853531759697944e-05, Acc:99.7500%\n",
      "Episode:68, Validation Loss:9.611767382011749e-06, Acc:99.9167%\n",
      "Episode:69, Validation Loss:1.8117685613106005e-05, Acc:99.5833%\n",
      "Episode:70, Validation Loss:1.080872243619524e-05, Acc:99.8333%\n",
      "Episode:71, Validation Loss:9.752663572726306e-06, Acc:99.8333%\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Episode:72, Validation Loss:1.5464827811229043e-05, Acc:99.8333%\n",
      "Episode:73, Validation Loss:1.3459191904985346e-05, Acc:99.8333%\n",
      "Episode:74, Validation Loss:1.3023243809584528e-05, Acc:99.8333%\n",
      "Episode:75, Validation Loss:1.0780268894450273e-05, Acc:99.8333%\n",
      "Episode:76, Validation Loss:1.1264800377830397e-05, Acc:99.9167%\n",
      "Episode:77, Validation Loss:1.0065447895613033e-05, Acc:99.9167%\n",
      "Episode:78, Validation Loss:1.3373739420785569e-05, Acc:99.9167%\n",
      "Episode:79, Validation Loss:1.0448366083437577e-05, Acc:99.8333%\n",
      "Episode:80, Validation Loss:8.99429687706288e-06, Acc:99.8333%\n",
      "Episode:81, Validation Loss:8.722438906261232e-06, Acc:99.8333%\n",
      "Episode:82, Validation Loss:1.13706319098128e-05, Acc:99.7500%\n",
      "Episode:83, Validation Loss:7.99590543465456e-06, Acc:99.9167%\n",
      "Episode:84, Validation Loss:7.649632607353851e-06, Acc:99.9167%\n",
      "Episode:85, Validation Loss:7.529773483838653e-06, Acc:99.9167%\n",
      "Episode:86, Validation Loss:8.41807559481822e-06, Acc:99.8333%\n",
      "Episode:87, Validation Loss:6.455147286033025e-06, Acc:100.0000%\n",
      "Epoch    87: reducing learning rate of group 0 to 1.0000e-05.\n",
      "===================Best Fold:1 Saved, Acc:1.0==================\n",
      "======================================================\n",
      "Episode:1, Validation Loss:0.00019049675029236823, Acc:97.5000%\n",
      "Episode:2, Validation Loss:0.0002631884126458317, Acc:96.4167%\n",
      "Episode:3, Validation Loss:0.00013853599375579506, Acc:98.0833%\n",
      "Episode:4, Validation Loss:0.0001139756350312382, Acc:98.5833%\n",
      "Episode:5, Validation Loss:9.158252942143008e-05, Acc:98.6667%\n",
      "Episode:6, Validation Loss:0.00010250933701172471, Acc:98.6667%\n",
      "Episode:7, Validation Loss:3.137619569315575e-05, Acc:99.6667%\n",
      "Episode:8, Validation Loss:8.64210887812078e-05, Acc:99.0000%\n",
      "Episode:9, Validation Loss:3.717850268003531e-05, Acc:99.4167%\n",
      "Episode:10, Validation Loss:2.4469571144436486e-05, Acc:99.6667%\n",
      "Episode:11, Validation Loss:6.559642497450113e-05, Acc:99.0833%\n",
      "Episode:12, Validation Loss:3.809248664765619e-05, Acc:99.5000%\n",
      "Episode:13, Validation Loss:3.5725908674066886e-05, Acc:99.3333%\n",
      "Episode:14, Validation Loss:2.9653643650817685e-05, Acc:99.5833%\n",
      "Episode:15, Validation Loss:2.782044975901954e-05, Acc:99.6667%\n",
      "Episode:16, Validation Loss:2.6903146135737188e-05, Acc:99.6667%\n",
      "Episode:17, Validation Loss:2.69160791503964e-05, Acc:99.5833%\n",
      "Episode:18, Validation Loss:2.5319324777228758e-05, Acc:99.6667%\n",
      "Episode:19, Validation Loss:2.3953865820658393e-05, Acc:99.5833%\n",
      "Episode:20, Validation Loss:3.774618380703032e-05, Acc:99.5833%\n",
      "Episode:21, Validation Loss:1.6685036825947464e-05, Acc:99.7500%\n",
      "Episode:22, Validation Loss:2.379420584475156e-05, Acc:99.6667%\n",
      "Episode:23, Validation Loss:3.204302993253805e-05, Acc:99.7500%\n",
      "Episode:24, Validation Loss:5.737923856941052e-05, Acc:99.2500%\n",
      "Episode:25, Validation Loss:1.944200266734697e-05, Acc:99.7500%\n",
      "Episode:26, Validation Loss:2.563020825618878e-05, Acc:99.6667%\n",
      "Episode:27, Validation Loss:2.321014653716702e-05, Acc:99.7500%\n",
      "Episode:28, Validation Loss:2.3155274902819656e-05, Acc:99.5833%\n",
      "Episode:29, Validation Loss:2.066578781523276e-05, Acc:99.6667%\n",
      "Episode:30, Validation Loss:2.409534681646619e-05, Acc:99.7500%\n",
      "Episode:31, Validation Loss:1.914177482831292e-05, Acc:99.7500%\n",
      "Episode:32, Validation Loss:2.9166938475100324e-05, Acc:99.6667%\n",
      "Episode:33, Validation Loss:2.39632063312456e-05, Acc:99.6667%\n",
      "Episode:34, Validation Loss:2.2185686248121783e-05, Acc:99.5833%\n",
      "Episode:35, Validation Loss:1.997587787627708e-05, Acc:99.6667%\n",
      "Episode:36, Validation Loss:2.3121021513361484e-05, Acc:99.5833%\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Episode:37, Validation Loss:1.872462235041894e-05, Acc:99.7500%\n",
      "Episode:38, Validation Loss:1.214016265294049e-05, Acc:99.8333%\n",
      "Episode:39, Validation Loss:1.306654121435713e-05, Acc:99.7500%\n",
      "Episode:40, Validation Loss:1.5205999261524994e-05, Acc:99.7500%\n",
      "Episode:41, Validation Loss:1.3879983271181118e-05, Acc:99.7500%\n",
      "Episode:42, Validation Loss:1.4734049727849197e-05, Acc:99.7500%\n",
      "Episode:43, Validation Loss:1.3508537449524738e-05, Acc:99.7500%\n",
      "Episode:44, Validation Loss:1.4088735952100251e-05, Acc:99.7500%\n",
      "Episode:45, Validation Loss:1.7460277376812883e-05, Acc:99.7500%\n",
      "Episode:46, Validation Loss:1.606320438440889e-05, Acc:99.7500%\n",
      "Episode:47, Validation Loss:1.4553364962921478e-05, Acc:99.7500%\n",
      "Episode:48, Validation Loss:1.603159398655407e-05, Acc:99.7500%\n",
      "Episode:49, Validation Loss:1.55012276081834e-05, Acc:99.7500%\n",
      "Episode:50, Validation Loss:2.0947605662513524e-05, Acc:99.7500%\n",
      "Episode:51, Validation Loss:1.5236370927595999e-05, Acc:99.7500%\n",
      "Episode:52, Validation Loss:1.4042680959391873e-05, Acc:99.7500%\n",
      "Episode:53, Validation Loss:1.7198899513459764e-05, Acc:99.8333%\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-05.\n",
      "===================Best Fold:2 Saved, Acc:0.9983333333333333==================\n",
      "======================================================\n",
      "Episode:1, Validation Loss:0.00021234358428046107, Acc:97.5000%\n",
      "Episode:2, Validation Loss:0.000159387884195894, Acc:98.2500%\n",
      "Episode:3, Validation Loss:0.00013512768782675266, Acc:98.9167%\n",
      "Episode:4, Validation Loss:0.00037443230394273996, Acc:95.5833%\n",
      "Episode:5, Validation Loss:8.654245903017e-05, Acc:98.8333%\n",
      "Episode:6, Validation Loss:0.00011111983621958643, Acc:99.0000%\n",
      "Episode:7, Validation Loss:0.0001617128582438454, Acc:98.5000%\n",
      "Episode:8, Validation Loss:5.903827695874497e-05, Acc:99.3333%\n",
      "Episode:9, Validation Loss:0.00011568377522053197, Acc:98.5000%\n",
      "Episode:10, Validation Loss:9.454048267798498e-05, Acc:99.2500%\n",
      "Episode:11, Validation Loss:7.758668652968481e-05, Acc:99.1667%\n",
      "Episode:12, Validation Loss:0.00012288759171497077, Acc:99.0000%\n",
      "Episode:13, Validation Loss:6.732527253916487e-05, Acc:99.1667%\n",
      "Episode:14, Validation Loss:7.309433567570522e-05, Acc:99.0833%\n",
      "Episode:15, Validation Loss:9.478512220084667e-05, Acc:98.8333%\n",
      "Episode:16, Validation Loss:7.779127918183804e-05, Acc:98.9167%\n",
      "Episode:17, Validation Loss:6.748141458956525e-05, Acc:99.4167%\n",
      "Episode:18, Validation Loss:9.798452811082825e-05, Acc:99.0000%\n",
      "Episode:19, Validation Loss:0.00011207183706574142, Acc:99.0833%\n",
      "Episode:20, Validation Loss:4.646686647902243e-05, Acc:99.4167%\n",
      "Episode:21, Validation Loss:5.735389277106151e-05, Acc:99.4167%\n",
      "Episode:22, Validation Loss:9.618912736186758e-05, Acc:99.0000%\n",
      "Episode:23, Validation Loss:9.50070025282912e-05, Acc:99.0833%\n",
      "Episode:24, Validation Loss:8.307594544021413e-05, Acc:99.4167%\n",
      "Episode:25, Validation Loss:7.307173655135557e-05, Acc:99.4167%\n",
      "Episode:26, Validation Loss:8.830731530906633e-05, Acc:99.1667%\n",
      "Episode:27, Validation Loss:7.00834279996343e-05, Acc:99.5000%\n",
      "Episode:28, Validation Loss:4.9587757530389354e-05, Acc:99.4167%\n",
      "Episode:29, Validation Loss:6.025706170476042e-05, Acc:99.5000%\n",
      "Episode:30, Validation Loss:7.817275036359206e-05, Acc:99.0833%\n",
      "Episode:31, Validation Loss:6.863543239887804e-05, Acc:99.4167%\n",
      "Episode:32, Validation Loss:5.433542901300825e-05, Acc:99.5000%\n",
      "Episode:33, Validation Loss:6.887181370984763e-05, Acc:99.5000%\n",
      "Episode:34, Validation Loss:6.198050687089562e-05, Acc:99.5833%\n",
      "Episode:35, Validation Loss:5.595746188191697e-05, Acc:99.6667%\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Episode:36, Validation Loss:4.8721718485467136e-05, Acc:99.6667%\n",
      "Episode:37, Validation Loss:3.488317088340409e-05, Acc:99.6667%\n",
      "Episode:38, Validation Loss:3.7178921047598124e-05, Acc:99.6667%\n",
      "Episode:39, Validation Loss:3.464188921498135e-05, Acc:99.6667%\n",
      "Episode:40, Validation Loss:4.026231908937916e-05, Acc:99.5833%\n",
      "Episode:41, Validation Loss:3.86132815037854e-05, Acc:99.6667%\n",
      "Episode:42, Validation Loss:4.150652603129856e-05, Acc:99.6667%\n",
      "Episode:43, Validation Loss:4.159914533374831e-05, Acc:99.6667%\n",
      "Episode:44, Validation Loss:3.850159191642888e-05, Acc:99.6667%\n",
      "Episode:45, Validation Loss:4.092508243047632e-05, Acc:99.6667%\n",
      "Episode:46, Validation Loss:3.789071706705727e-05, Acc:99.6667%\n",
      "Episode:47, Validation Loss:3.965389259974472e-05, Acc:99.6667%\n",
      "Episode:48, Validation Loss:3.5458419006317854e-05, Acc:99.5833%\n",
      "Episode:49, Validation Loss:3.503180050756782e-05, Acc:99.6667%\n",
      "Episode:50, Validation Loss:3.933212428819388e-05, Acc:99.6667%\n",
      "Episode:51, Validation Loss:3.6777681089006364e-05, Acc:99.5833%\n",
      "Episode:52, Validation Loss:3.937499786843546e-05, Acc:99.5833%\n",
      "Episode:53, Validation Loss:3.752483826247044e-05, Acc:99.5833%\n",
      "Episode:54, Validation Loss:3.88232474506367e-05, Acc:99.6667%\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-05.\n",
      "===================Best Fold:3 Saved, Acc:0.9966666666666667==================\n",
      "======================================================\n",
      "Episode:1, Validation Loss:0.000127049905131571, Acc:98.1667%\n",
      "Episode:2, Validation Loss:0.00011992097279289737, Acc:98.4167%\n",
      "Episode:3, Validation Loss:0.00012759245873894542, Acc:98.5833%\n",
      "Episode:4, Validation Loss:6.0713529819622636e-05, Acc:99.1667%\n",
      "Episode:5, Validation Loss:9.375788795296103e-05, Acc:98.4167%\n",
      "Episode:6, Validation Loss:9.368203609483317e-05, Acc:99.0000%\n",
      "Episode:7, Validation Loss:0.00011863686086144298, Acc:98.3333%\n",
      "Episode:8, Validation Loss:6.781276169931516e-05, Acc:99.0833%\n",
      "Episode:9, Validation Loss:5.1263901696074754e-05, Acc:99.4167%\n",
      "Episode:10, Validation Loss:9.286661224905401e-05, Acc:99.0833%\n",
      "Episode:11, Validation Loss:7.038362673483789e-05, Acc:99.2500%\n",
      "Episode:12, Validation Loss:6.594746082555503e-05, Acc:99.4167%\n",
      "Episode:13, Validation Loss:5.096399036119692e-05, Acc:99.6667%\n",
      "Episode:14, Validation Loss:6.70314984745346e-05, Acc:99.3333%\n",
      "Episode:15, Validation Loss:5.378340938477777e-05, Acc:99.1667%\n",
      "Episode:16, Validation Loss:6.769228639313951e-05, Acc:99.4167%\n",
      "Episode:17, Validation Loss:5.7960987760452554e-05, Acc:99.0833%\n",
      "Episode:18, Validation Loss:6.879466673126444e-05, Acc:99.4167%\n",
      "Episode:19, Validation Loss:6.627203401876613e-05, Acc:99.3333%\n",
      "Episode:20, Validation Loss:6.425995525205508e-05, Acc:99.3333%\n",
      "Episode:21, Validation Loss:7.266696775332093e-05, Acc:99.3333%\n",
      "Episode:22, Validation Loss:5.3863161156186834e-05, Acc:99.4167%\n",
      "Episode:23, Validation Loss:5.942548523307778e-05, Acc:99.5000%\n",
      "Episode:24, Validation Loss:4.803168121725321e-05, Acc:99.4167%\n",
      "Episode:25, Validation Loss:4.12931913160719e-05, Acc:99.5833%\n",
      "Episode:26, Validation Loss:5.089996557217091e-05, Acc:99.4167%\n",
      "Episode:27, Validation Loss:4.811281178263016e-05, Acc:99.3333%\n",
      "Episode:28, Validation Loss:6.200258940225467e-05, Acc:99.4167%\n",
      "Episode:29, Validation Loss:3.9183702028822154e-05, Acc:99.8333%\n",
      "Episode:30, Validation Loss:5.6903510994743556e-05, Acc:99.4167%\n",
      "Episode:31, Validation Loss:4.3370491766836494e-05, Acc:99.4167%\n",
      "Episode:32, Validation Loss:4.1132527258014306e-05, Acc:99.6667%\n",
      "Episode:33, Validation Loss:4.4011314457748085e-05, Acc:99.5833%\n",
      "Episode:34, Validation Loss:6.07143847446423e-05, Acc:99.4167%\n",
      "Episode:35, Validation Loss:4.3738418753491715e-05, Acc:99.5000%\n",
      "Episode:36, Validation Loss:4.271250145393424e-05, Acc:99.6667%\n",
      "Episode:37, Validation Loss:5.602863529929891e-05, Acc:99.5000%\n",
      "Episode:38, Validation Loss:3.91483299608808e-05, Acc:99.5833%\n",
      "Episode:39, Validation Loss:5.5363714636769146e-05, Acc:99.5833%\n",
      "Episode:40, Validation Loss:4.212636122247204e-05, Acc:99.4167%\n",
      "Episode:41, Validation Loss:5.3016276069683954e-05, Acc:99.4167%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    epochs = 100\n",
    "    period = 100\n",
    "    ensemble_models = []\n",
    "    lr = 1e-3\n",
    "    val_period = 1\n",
    "    \n",
    "    print(\"Fold:\",len(train_loaders))\n",
    "    \n",
    "    for fold in range(len(train_loaders)):\n",
    "        train_loader = train_loaders[fold]\n",
    "        val_loader = val_loaders[fold]\n",
    "        \n",
    "        model = get_model()\n",
    "        torch.cuda.empty_cache()    #Need further check\n",
    "            \n",
    "        max_acc = 0\n",
    "        min_loss = 10000\n",
    "        best_model_dict = None\n",
    "        data_num = 0\n",
    "        loss_avg = 0\n",
    "#         optimizer = torch.optim.SGD(model.parameters(),lr=lr)\n",
    "#         optimizer = torch.optim.RMSprop(model.parameters(),lr=lr,alpha=0.9)\n",
    "        optimizer = torch.optim.Adam(model.parameters(),lr=lr,betas=(0.9,0.99))\n",
    "#         optimizer = torch.optim.Adagrad(model.parameters(),lr=lr)\n",
    "#         lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=period,T_mult=1,eta_min=1e-7) #original \n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, patience=15)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "        for ep in range(0,epochs+1):\n",
    "            model.train()\n",
    "            for idx, data in enumerate(train_loader):\n",
    "                img, target = data\n",
    "                img, target = img.to(device), target.to(device,dtype=torch.long)\n",
    "    #             print(np.shape(img),np.shape(target)) #Tensor(4,1,28,28), Tensor(4)\n",
    "    #             print(np.max(img.cpu().numpy()),np.min(img.cpu().numpy())) #1.0 0.0\n",
    "                pred = model(img)\n",
    "    #             print(pred.size())   #(32,10)\n",
    "    #             print(target.size()) #(32,)\n",
    "                ###Input shape must be pred:, target:\n",
    "                loss = criterion(pred,target)\n",
    "                loss_avg += loss.item()\n",
    "                data_num += img.size(0)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            ###Cosine annealing\n",
    "    #         lr_scheduler.step()\n",
    "\n",
    "            ###Evaluate Train Loss \n",
    "#             if ep%2 == 0:\n",
    "#                 loss_avg /= data_num\n",
    "#                 print(\"Ep:{}, loss:{}, lr:{}\".format(ep, loss_avg,optimizer.param_groups[0]['lr']))\n",
    "#                 loss_avg = 0\n",
    "#                 data_num = 0\n",
    "\n",
    "            ###Validation\n",
    "            if ep!=0 and ep%val_period == 0:\n",
    "                model.eval()\n",
    "                acc = 0\n",
    "                val_loss = 0\n",
    "                data_num  = 0\n",
    "                with torch.no_grad():\n",
    "                    for idx, data in enumerate(val_loader):\n",
    "                        img, target = data\n",
    "                        img, target = img.to(device), target.to(device,dtype=torch.long)\n",
    "                        pred = model(img)\n",
    "                        val_loss += criterion(pred, target)\n",
    "                        # print(pred)\n",
    "                        _,pred_class = torch.max(pred.data, 1)\n",
    "    #                     print(pred_class)\n",
    "                        acc += (pred_class == target).sum().item()\n",
    "                        data_num += img.size(0)\n",
    "\n",
    "                acc /= data_num\n",
    "                val_loss /= data_num\n",
    "\n",
    "                ###Plateau\n",
    "                lr_scheduler.step(val_loss)\n",
    "#                 lr_scheduler.step(-1*acc)\n",
    "                if optimizer.param_groups[0]['lr'] < 1e-4:\n",
    "                    break                    \n",
    "\n",
    "                if acc >= max_acc:\n",
    "                    max_acc = acc\n",
    "                    best_model_dict = model.state_dict()\n",
    "                \n",
    "                if val_loss <= min_loss:\n",
    "                    min_loss = val_loss\n",
    "#                     best_model_dict = model.state_dict()\n",
    "                \n",
    "                print(\"Episode:{}, Validation Loss:{}, Acc:{:.4f}%\".format(ep,val_loss,acc*100))\n",
    "            \n",
    "            if ep!=0 and ep%5 == 0:\n",
    "                torch.save(best_model_dict, \"./Kmnist_saved_model/Fold{}_current_acc{:.4f}\".format(fold,max_acc))\n",
    "            \n",
    "            \n",
    "        ###K-Fold ensemble: Saved k best model for k dataloader\n",
    "        print(\"===================Best Fold:{} Saved, Acc:{}==================\".format(fold,max_acc))\n",
    "        torch.save(best_model_dict, \"./Kmnist_saved_model/Fold{}_ep{}_loss{:.3f}_acc{:.4f}\".format(fold,ep,min_loss,max_acc))\n",
    "        print(\"======================================================\")\n",
    "\n",
    "            \n",
    "            ###Snapshot ensemble: saved model\n",
    "#             if ep!=0 and ep%period == 0:\n",
    "# #                 ensemble_models.append(best_model_dict)\n",
    "#                 model_id = ep//period\n",
    "#                 print(\"===================Best Model{} Saved, Acc:{}==================\".format(model_id,max_acc))\n",
    "#                 torch.save(best_model_dict, \"./Kmnist_saved_model/model{}_ep{}_acc{:.4f}\".format(model_id,ep,max_acc))\n",
    "#                 print(\"======================================================\")\n",
    "#                 max_acc = 0\n",
    "        \n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_root = \"/home/ccchang/localization_net/Kmnist_saved_model/emsemble/5_fold_ep80_lr1e-2\"\n",
    "ensemble_models = []\n",
    "epochs = 500\n",
    "period = 100\n",
    "model_num = epochs//period\n",
    "model = 5\n",
    "data_num = 0\n",
    "acc = 0\n",
    "\n",
    "for file_name in os.listdir(ensemble_root):\n",
    "    model = convNet(in_channels=1)\n",
    "    model.cuda()\n",
    "    model.load_state_dict(torch.load(\"{}/{}\".format(ensemble_root,file_name)))\n",
    "    model.eval()\n",
    "    ensemble_models.append(model)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, data in enumerate(validate_loader):\n",
    "        img, target = data\n",
    "        img, target = img.to(device), target.to(device,dtype=torch.long)\n",
    "\n",
    "        ###Single model\n",
    "#         pred = model(img)\n",
    "#         _,pred_class = torch.max(pred.data, 1)\n",
    "        \n",
    "        ###Average Ensemble\n",
    "#         pred_list = torch.Tensor([]).to(device)\n",
    "#         for i in range(model_num):\n",
    "#             pred = ensemble_models[i](img) #(batch_num,10)\n",
    "#             pred_list = torch.cat((pred_list,pred.unsqueeze(2)),dim=2) #pred_list: (batch_num,10,model_num)\n",
    "#         pred = torch.mean(pred_list,dim=2)   #(batch,10)\n",
    "        \n",
    "#         _,pred_class = torch.max(pred.data, 1)   #(batch_num,)\n",
    "#         val_loss += criterion(pred, target)\n",
    "\n",
    "        ###Voting Ensemble\n",
    "        pred_list = torch.LongTensor([]).to(device)\n",
    "        for i in range(model_num):\n",
    "            pred = ensemble_models[i](img) #(batch_num,10)\n",
    "            _,pred_class = torch.max(pred.data, 1)   #(batch_num,)\n",
    "            pred_list = torch.cat((pred_list,pred_class.unsqueeze(1)),dim=1)\n",
    "            \n",
    "        pred_class_list = torch.LongTensor([]).to(device)\n",
    "        for i in range(img.size(0)):\n",
    "            pred_np = pred_list[i].cpu().numpy()\n",
    "            unique_class,count = np.unique(pred_np,return_counts=True)\n",
    "            unique_class = np.array(unique_class[np.argmax(count)]).reshape(-1)   #unique class shape(1,)\n",
    "            class_voted= torch.from_numpy(unique_class).to(device)    #(1,)\n",
    "            pred_class_list = torch.cat((pred_class_list,class_voted))    \n",
    "    \n",
    "#         acc += (pred_class == target).sum().item()\n",
    "        acc += (pred_class_list == target).sum().item()\n",
    "        data_num += img.size(0)\n",
    "\n",
    "#     val_loss /= data_num\n",
    "    acc /= data_num\n",
    "    print(\"Acc:{:.4f}%\".format(acc*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.data = global_dataimport torch\n",
    "import numpy as np\n",
    "\n",
    "t1 = torch.Tensor([[1,2,3,4],[4,3,2,1],[1,5,3,3]])  #(3,4)\n",
    "t1 = t1.unsqueeze(2)\n",
    "\n",
    "t_list = torch.Tensor([])\n",
    "\n",
    "for i in range(3):\n",
    "    t_list = torch.cat((t_list,t1),dim=2)\n",
    "\n",
    "print(t_list.size())\n",
    "print(t_list)\n",
    "t_list = torch.mean(t_list,dim=2)\n",
    "print(t_list.size())\n",
    "print(t_list)\n",
    "\n",
    "\n",
    "# n1 = t1.cpu().numpy()\n",
    "\n",
    "# n1, count = np.unique(n1,return_counts=True,axis=0)\n",
    "# print(count)\n",
    "# n1 = np.argmax(count)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
