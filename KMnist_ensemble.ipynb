{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torchsummary import summary\n",
    "from efficientnet import EfficientNet\n",
    "device = \"cuda\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_mean_std(dataloader):\n",
    "    print(\"Calculate distribution:\")\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    nb_samples = 0.\n",
    "    for data in dataloader:\n",
    "        img, label = data\n",
    "        batch_samples = img.size(0)\n",
    "        img = img.view(batch_samples, img.size(1), -1)\n",
    "        mean += img.mean(2).sum(0)\n",
    "        std += img.std(2).sum(0)\n",
    "        nb_samples += batch_samples\n",
    "        if nb_samples%5000 == 0:\n",
    "            print(\"Finished:\", nb_samples)\n",
    "    print(\"num of samples:\",nb_samples)\n",
    "    mean /= nb_samples\n",
    "    std /= nb_samples\n",
    "#     print(\"Average mean:\",mean)\n",
    "#     print(\"Average std:\", std)\n",
    "    return mean.cpu().numpy(), std.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_t = time.clock()\n",
    "indices = np.arange(10)\n",
    "np.random.shuffle(indices)\n",
    "print(\"suffle cost:\",time.clock()-start_t)\n",
    "\n",
    "train_dataset = KMnistDataset(data_len=10,is_validate=False, validate_rate=0.1, indices=indices)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "val_dataset = KMnistDataset(data_len=10,is_validate=True, validate_rate=0.1, indices=indices)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kfold_dataset_loader(k=5,val_rate=0.1,indices_len=None, batch_size=None,num_workers=None):\n",
    "    ###Return [list of train dataset_loader, list of val dataset_loader]\n",
    "    train_loader_list = []\n",
    "    val_loader_list = []\n",
    "    for i in range(k):\n",
    "        indices = np.arange(indices_len)\n",
    "        np.random.shuffle(indices)\n",
    "                    \n",
    "        train_dataset = KMnistDataset(data_len=None,is_validate=False, validate_rate=val_rate, indices=indices)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "        \n",
    "        val_dataset = KMnistDataset(data_len=None,is_validate=True, validate_rate=val_rate, indices=indices)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "        \n",
    "        train_loader_list.append(train_loader)\n",
    "        val_loader_list.append(val_loader)\n",
    "        \n",
    "    return train_loader_list, val_loader_list\n",
    "    \n",
    "    \n",
    "class KFoldModel():\n",
    "    def __init__(self,model_list):\n",
    "        self.K = len(model_list)\n",
    "        self.models = model_list\n",
    "        \n",
    "    def pred(mode=\"vote\"):\n",
    "        None       \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class convNet(nn.Module):\n",
    "    def __init__(self,in_channels):\n",
    "        super(convNet,self).__init__()\n",
    "        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, \n",
    "        #                dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "        self.c1 = nn.Conv2d(in_channels=in_channels, out_channels=64,kernel_size=5,stride=1,padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=64,momentum=0.1)\n",
    "        self.c2 = nn.Conv2d(64,64,5,1,2)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=64,momentum=0.1)\n",
    "        self.m1 = nn.MaxPool2d(2)\n",
    "        self.d1 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.c3 = nn.Conv2d(64,128,5,1,2)\n",
    "        self.bn3 = nn.BatchNorm2d(128,0.1)\n",
    "        self.c4 = nn.Conv2d(128,128,5,1,2)\n",
    "        self.bn4 = nn.BatchNorm2d(128,0.1)\n",
    "        self.m2 = nn.MaxPool2d(2)\n",
    "        self.d2 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.c5 = nn.Conv2d(128,256,3,1,1)\n",
    "        self.bn5 = nn.BatchNorm2d(256,0.1)\n",
    "        self.c6 = nn.Conv2d(256,256,3,1,1)\n",
    "        self.bn6 = nn.BatchNorm2d(256,0.1)\n",
    "        self.m3 = nn.MaxPool2d(2)\n",
    "        self.d3 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(256*3*3,256)\n",
    "        self.out = nn.Linear(256,10)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.leaky_relu(self.bn1(self.c1(x)),negative_slope=0.1)\n",
    "        x = F.leaky_relu(self.bn2(self.c2(x)),0.1)\n",
    "        x = self.m1(x)\n",
    "        x = self.d1(x)\n",
    "        \n",
    "        x = F.leaky_relu(self.bn3(self.c3(x)),0.1)\n",
    "        x = F.leaky_relu(self.bn4(self.c4(x)),0.1)\n",
    "        x = self.m2(x)\n",
    "        x = self.d2(x)\n",
    "        \n",
    "        x = F.leaky_relu(self.bn5(self.c5(x)),0.1)\n",
    "        x = F.leaky_relu(self.bn6(self.c6(x)),0.1)\n",
    "        x = self.m3(x)\n",
    "        x = self.d3(x)\n",
    "        \n",
    "        x = x.view(-1, 256*3*3) #reshape\n",
    "        x = F.leaky_relu(self.fc1(x),0.1)\n",
    "        return self.out(x)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([\n",
    "#         transforms.RandomResizedCrop(28),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.RandomVerticalFlip(),\n",
    "        transforms.ColorJitter(0.2,0.2,0.2,0.5),\n",
    "        transforms.RandomAffine(degrees=180,translate=(0.2,0.2),scale=[0.7,1.1],shear=15),\n",
    "        transforms.ToTensor(),  #Take Image as input and convert to tensor with value from 0 to1  \n",
    "#         transforms.Normalize(mean=[0.08194405],std=[0.238141])  #comment this line when calculate ditribution\n",
    "    ])\n",
    "\n",
    "trans_val = transforms.Compose([\n",
    "        transforms.ToTensor(),  #Take Image as input and convert to tensor with value from 0 to1\n",
    "#         transforms.Normalize(mean=[0.08544743],std=[0.24434312]) #comment this line when calculate ditribution\n",
    "    ])\n",
    "\n",
    "trans_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.07261261],std=[0.22267216])\n",
    "])\n",
    "\n",
    "global_data = pd.read_csv(\"./dataset/train.csv\")\n",
    "\n",
    "class KMnistDataset(Dataset):\n",
    "    def __init__(self,data_len=None, is_validate=False,validate_rate=None,indices=None):\n",
    "        self.is_validate = is_validate\n",
    "        self.data = global_data\n",
    "#         print(\"data shape:\", np.shape(self.data))\n",
    "        if data_len == None:\n",
    "            data_len = len(self.data)\n",
    "        \n",
    "        self.indices = indices\n",
    "        if self.is_validate:\n",
    "            self.len = int(data_len*validate_rate)\n",
    "            self.offset = int(data_len*(1-validate_rate))\n",
    "            self.transform = trans_val\n",
    "        else:\n",
    "            self.len = int(data_len*(1-validate_rate))\n",
    "            self.offset = 0\n",
    "            self.transform = trans\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        idx += self.offset\n",
    "        idx = self.indices[idx]\n",
    "        img = self.data.iloc[idx, 1:].values.astype(np.uint8).reshape((28, 28))  #value: 0~255\n",
    "        label = self.data.iloc[idx, 0]  #(num,)\n",
    "        img = Image.fromarray(img)\n",
    "        img = self.transform(img)     #value: 0~1, shape:(1,28,28)\n",
    "        label = torch.as_tensor(label, dtype=torch.uint8)    #value: 0~9, shape(1)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,data_len=None):\n",
    "        self.data = pd.read_csv(\"./dataset/test.csv\")\n",
    "        print(\"data shape:\", np.shape(self.data))\n",
    "        self.transform = trans_val\n",
    "        if data_len == None:\n",
    "            self.len = len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.data.iloc[idx, 1:].values.astype(np.uint8).reshape((28, 28))  #value: 0~255\n",
    "        img = Image.fromarray(img)\n",
    "        img = self.transform(img)     #value: 0~1, shape:(1,28,28)\n",
    "        return img, torch.Tensor([])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (5000, 785)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "num_workers = 10\n",
    "vr = 0.2\n",
    "k = 5\n",
    "indices_len = 60000\n",
    "\n",
    "###Single dataset\n",
    "indices = np.arange(indices_len)\n",
    "# train_dataset = KMnistDataset(data_len=None,is_validate=False, validate_rate=vr,indices=indices)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers,indices=indices)\n",
    "# mean, std = get_dataset_mean_std(train_loader)\n",
    "# print(\"train distribution:\",mean, std)\n",
    "\n",
    "validate_dataset = KMnistDataset(data_len=None,is_validate=True, validate_rate=vr,indices=indices)\n",
    "validate_loader = DataLoader(validate_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "# mean, std = get_dataset_mean_std(validate_loader)\n",
    "# print(\"validate distribution:\",mean, std)\n",
    "\n",
    "test_dataset = TestDataset(data_len=None)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "# mean, std = get_dataset_mean_std(test_loader)\n",
    "# print(\"validate distribution:\",mean, std)\n",
    "\n",
    "###K-fold dataset\n",
    "train_loaders, val_loaders = get_kfold_dataset_loader(k, vr, indices_len, batch_size, num_workers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(len(train_loaders)):\n",
    "    train_loader = train_loaders[fold]\n",
    "    val_loader = val_loaders[fold]\n",
    "    \n",
    "    print(\"=======fold:\",fold)\n",
    "    print(\"train:\")\n",
    "    for data in train_loader:\n",
    "        None\n",
    "    print(\"val:\")\n",
    "    for data in val_loader:\n",
    "        None        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./dataset/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000,1050):\n",
    "    fig, axes = plt.subplots(1,2,figsize=(8,4))\n",
    "    img2 = data.iloc[i, 1:].values.astype(np.uint8).reshape((28, 28))  #value: 0~255\n",
    "    img2 = Image.fromarray(img2)\n",
    "    axes[0].imshow(img2,cmap=\"gray\")\n",
    "    img2 = trans(img2).cpu().numpy().reshape(28,28)\n",
    "    axes[1].imshow(img2,cmap=\"gray\")\n",
    "    plt.pause(.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmnist_dataset.data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(kmnist_dataset.data.iloc[20,1:].values)  #numpy ndarray\n",
    "type(kmnist_dataset.data.iloc[20,0])  #numpy int64\n",
    "kmnist_dataset.data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 28, 28]           1,664\n",
      "       BatchNorm2d-2           [-1, 64, 28, 28]             128\n",
      "            Conv2d-3           [-1, 64, 28, 28]         102,464\n",
      "       BatchNorm2d-4           [-1, 64, 28, 28]             128\n",
      "         MaxPool2d-5           [-1, 64, 14, 14]               0\n",
      "           Dropout-6           [-1, 64, 14, 14]               0\n",
      "            Conv2d-7          [-1, 128, 14, 14]         204,928\n",
      "       BatchNorm2d-8          [-1, 128, 14, 14]             256\n",
      "            Conv2d-9          [-1, 128, 14, 14]         409,728\n",
      "      BatchNorm2d-10          [-1, 128, 14, 14]             256\n",
      "        MaxPool2d-11            [-1, 128, 7, 7]               0\n",
      "          Dropout-12            [-1, 128, 7, 7]               0\n",
      "           Conv2d-13            [-1, 256, 7, 7]         295,168\n",
      "      BatchNorm2d-14            [-1, 256, 7, 7]             512\n",
      "           Conv2d-15            [-1, 256, 7, 7]         590,080\n",
      "      BatchNorm2d-16            [-1, 256, 7, 7]             512\n",
      "        MaxPool2d-17            [-1, 256, 3, 3]               0\n",
      "          Dropout-18            [-1, 256, 3, 3]               0\n",
      "           Linear-19                  [-1, 256]         590,080\n",
      "           Linear-20                   [-1, 10]           2,570\n",
      "================================================================\n",
      "Total params: 2,198,474\n",
      "Trainable params: 2,198,474\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 3.00\n",
      "Params size (MB): 8.39\n",
      "Estimated Total Size (MB): 11.39\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Advance model\n",
    "# model_name = 'efficientnet-b0'\n",
    "# image_size = EfficientNet.get_image_size(model_name)\n",
    "# model = EfficientNet.from_pretrained(model_name, num_classes=10)\n",
    "# model = model.to(device)\n",
    "\n",
    "#Basic cnn\n",
    "model = convNet(in_channels=1)\n",
    "model.cuda()\n",
    "# model.load_state_dict(torch.load(\"./Kmnist_saved_model/ep20_acc0.9910\"))\n",
    "\n",
    "summary(model, input_size=(1, 28, 28))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "ep = 300\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=lr)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=50,T_mult=1,eta_min=1e-6) #original \n",
    "# lr_scheduler = CosineAnnealingWarmUpRestarts(optimizer,T_0=20,T_mult=3,eta_max=lr,T_up=10)  #advance\n",
    "plt.figure()\n",
    "x = list(range(ep))\n",
    "y = []\n",
    "for epoch in range(ep):\n",
    "    lr_scheduler.step()\n",
    "    lr = lr_scheduler.get_lr()\n",
    "    y.append(lr_scheduler.get_lr()[0])\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1, Validation Loss:0.0011520234402269125, Acc:89.5500%\n",
      "Episode:2, Validation Loss:0.0005160228465683758, Acc:95.9250%\n",
      "Episode:3, Validation Loss:0.0004982446553185582, Acc:95.8333%\n",
      "Episode:4, Validation Loss:0.0005213764379732311, Acc:95.9583%\n",
      "Episode:5, Validation Loss:0.00039184559136629105, Acc:96.8083%\n",
      "Episode:6, Validation Loss:0.00035945841227658093, Acc:97.2000%\n",
      "Episode:7, Validation Loss:0.0004080832877662033, Acc:97.0083%\n",
      "Episode:8, Validation Loss:0.00023338418395724148, Acc:98.1417%\n",
      "Episode:9, Validation Loss:0.0003493675321806222, Acc:97.2833%\n",
      "Episode:10, Validation Loss:0.0004082717059645802, Acc:96.4333%\n",
      "Episode:11, Validation Loss:0.0002135006507160142, Acc:98.2583%\n",
      "Episode:12, Validation Loss:0.00025327742332592607, Acc:97.9417%\n",
      "Episode:13, Validation Loss:0.0001897531474241987, Acc:98.4500%\n",
      "Episode:14, Validation Loss:0.00027436125674284995, Acc:97.7583%\n",
      "Episode:15, Validation Loss:0.0002605930785648525, Acc:98.0000%\n",
      "Episode:16, Validation Loss:0.00021945746266283095, Acc:98.2583%\n",
      "Episode:17, Validation Loss:0.00018205314700026065, Acc:98.5250%\n",
      "Episode:18, Validation Loss:0.00017414869216736406, Acc:98.6250%\n",
      "Episode:19, Validation Loss:0.0001545178092783317, Acc:98.6917%\n",
      "Episode:20, Validation Loss:0.00020669815421570092, Acc:98.3583%\n",
      "Episode:21, Validation Loss:0.0001302167511312291, Acc:98.9167%\n",
      "Episode:22, Validation Loss:0.00016221053374465555, Acc:98.6667%\n",
      "Episode:23, Validation Loss:0.00013458554167300463, Acc:98.9167%\n",
      "Episode:24, Validation Loss:0.0001400754990754649, Acc:98.8917%\n",
      "Episode:25, Validation Loss:0.00020201285951770842, Acc:98.3917%\n",
      "Episode:26, Validation Loss:0.00013853388372808695, Acc:98.9667%\n",
      "Episode:27, Validation Loss:0.00015288832946680486, Acc:98.8000%\n",
      "Episode:28, Validation Loss:0.00013950129505246878, Acc:98.9000%\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Episode:29, Validation Loss:0.00015231251018121839, Acc:98.9000%\n",
      "Episode:30, Validation Loss:9.760555258253589e-05, Acc:99.1750%\n",
      "Episode:31, Validation Loss:9.862083243206143e-05, Acc:99.2667%\n",
      "Episode:32, Validation Loss:9.591860725777224e-05, Acc:99.2500%\n",
      "Episode:33, Validation Loss:9.520255116512999e-05, Acc:99.2833%\n",
      "Episode:34, Validation Loss:9.090932144317776e-05, Acc:99.3167%\n",
      "Episode:35, Validation Loss:9.0145091235172e-05, Acc:99.3333%\n",
      "Episode:36, Validation Loss:8.839483052724972e-05, Acc:99.2833%\n",
      "Episode:37, Validation Loss:8.686711953487247e-05, Acc:99.3000%\n",
      "Episode:38, Validation Loss:8.833108586259186e-05, Acc:99.3250%\n",
      "Episode:39, Validation Loss:8.503953722538427e-05, Acc:99.3667%\n",
      "Episode:40, Validation Loss:8.760108175920323e-05, Acc:99.3250%\n",
      "Episode:41, Validation Loss:9.026773477671668e-05, Acc:99.3583%\n",
      "Episode:42, Validation Loss:9.08154106582515e-05, Acc:99.2917%\n",
      "Episode:43, Validation Loss:8.14777085906826e-05, Acc:99.3583%\n",
      "Episode:44, Validation Loss:8.197536953957751e-05, Acc:99.3500%\n",
      "Episode:45, Validation Loss:7.890293636592105e-05, Acc:99.3667%\n",
      "Episode:46, Validation Loss:8.883760165190324e-05, Acc:99.3000%\n",
      "Episode:47, Validation Loss:8.700948092155159e-05, Acc:99.3167%\n",
      "Episode:48, Validation Loss:7.963051029946655e-05, Acc:99.4167%\n",
      "Episode:49, Validation Loss:7.889922562753782e-05, Acc:99.4000%\n",
      "Episode:50, Validation Loss:8.272816921817139e-05, Acc:99.3417%\n",
      "Episode:51, Validation Loss:7.918381743365899e-05, Acc:99.4167%\n",
      "Episode:52, Validation Loss:8.09614357422106e-05, Acc:99.3250%\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Episode:53, Validation Loss:8.201236778404564e-05, Acc:99.3583%\n",
      "Episode:54, Validation Loss:7.949554128572345e-05, Acc:99.3667%\n",
      "Episode:55, Validation Loss:7.887770334491506e-05, Acc:99.3417%\n",
      "Episode:56, Validation Loss:7.906657992862165e-05, Acc:99.3667%\n",
      "Episode:57, Validation Loss:7.754392572678626e-05, Acc:99.3833%\n",
      "Episode:58, Validation Loss:7.787028152961284e-05, Acc:99.3750%\n",
      "Episode:59, Validation Loss:7.827334047760814e-05, Acc:99.3583%\n",
      "Episode:60, Validation Loss:7.938776252558455e-05, Acc:99.3833%\n",
      "Episode:61, Validation Loss:7.789066876284778e-05, Acc:99.3917%\n",
      "Episode:62, Validation Loss:7.825708598829806e-05, Acc:99.3667%\n",
      "Episode:63, Validation Loss:7.709530473221093e-05, Acc:99.3833%\n",
      "Episode:64, Validation Loss:7.73258216213435e-05, Acc:99.3917%\n",
      "Episode:65, Validation Loss:7.742182060610503e-05, Acc:99.4083%\n",
      "Episode:66, Validation Loss:7.677059329580516e-05, Acc:99.4250%\n",
      "Episode:67, Validation Loss:7.748277857899666e-05, Acc:99.3833%\n",
      "Episode:68, Validation Loss:7.671670755371451e-05, Acc:99.4250%\n",
      "Episode:69, Validation Loss:7.682361319893971e-05, Acc:99.4000%\n",
      "Episode:70, Validation Loss:7.716986146988347e-05, Acc:99.4083%\n",
      "===================Best Fold:0 Saved, Acc:0.99425==================\n",
      "======================================================\n",
      "Episode:1, Validation Loss:0.0008476295624859631, Acc:93.3083%\n",
      "Episode:2, Validation Loss:0.0005925697623752058, Acc:95.2083%\n",
      "Episode:3, Validation Loss:0.0004053114971611649, Acc:96.7583%\n",
      "Episode:4, Validation Loss:0.00039852975169196725, Acc:96.9167%\n",
      "Episode:5, Validation Loss:0.00044791458640247583, Acc:96.3500%\n",
      "Episode:6, Validation Loss:0.000339483842253685, Acc:97.1583%\n",
      "Episode:7, Validation Loss:0.00028493496938608587, Acc:97.8083%\n",
      "Episode:8, Validation Loss:0.0003388275799807161, Acc:97.3000%\n",
      "Episode:9, Validation Loss:0.00021951166854705662, Acc:98.3750%\n",
      "Episode:10, Validation Loss:0.00021008129988331348, Acc:98.2750%\n",
      "Episode:11, Validation Loss:0.00018792969058267772, Acc:98.4917%\n",
      "Episode:12, Validation Loss:0.00022765749599784613, Acc:98.2250%\n",
      "Episode:13, Validation Loss:0.00020870794833172113, Acc:98.3750%\n",
      "Episode:14, Validation Loss:0.00021116866264492273, Acc:98.3667%\n",
      "Episode:15, Validation Loss:0.00019780563889071345, Acc:98.5083%\n",
      "Episode:16, Validation Loss:0.00020604526798706502, Acc:98.4000%\n",
      "Episode:17, Validation Loss:0.00018436288519296795, Acc:98.6833%\n",
      "Episode:18, Validation Loss:0.00016047823010012507, Acc:98.7167%\n",
      "Episode:19, Validation Loss:0.00019377452554181218, Acc:98.5833%\n",
      "Episode:20, Validation Loss:0.00014413168537430465, Acc:98.9583%\n",
      "Episode:21, Validation Loss:0.00018382308189757168, Acc:98.5500%\n",
      "Episode:22, Validation Loss:0.00013262275024317205, Acc:98.9833%\n",
      "Episode:23, Validation Loss:0.00015057565178722143, Acc:98.8500%\n",
      "Episode:24, Validation Loss:0.0001575385977048427, Acc:98.8000%\n",
      "Episode:25, Validation Loss:0.00017576530808582902, Acc:98.7083%\n",
      "Episode:26, Validation Loss:0.00015893987438175827, Acc:98.7583%\n",
      "Episode:27, Validation Loss:0.00014739151811227202, Acc:98.8833%\n",
      "Episode:28, Validation Loss:0.00017122886492870748, Acc:98.7917%\n",
      "Episode:29, Validation Loss:0.00013934382877778262, Acc:98.9167%\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Episode:30, Validation Loss:0.0001457085454603657, Acc:98.9417%\n",
      "Episode:31, Validation Loss:0.00011737831664504483, Acc:99.1333%\n",
      "Episode:32, Validation Loss:0.00010760584700619802, Acc:99.1750%\n",
      "Episode:33, Validation Loss:0.0001069403879228048, Acc:99.2333%\n",
      "Episode:34, Validation Loss:0.00010962202213704586, Acc:99.1917%\n",
      "Episode:35, Validation Loss:9.815123485168442e-05, Acc:99.2750%\n",
      "Episode:36, Validation Loss:9.765327558852732e-05, Acc:99.2167%\n",
      "Episode:37, Validation Loss:9.721150127006695e-05, Acc:99.2667%\n",
      "Episode:38, Validation Loss:8.706023072591051e-05, Acc:99.3500%\n",
      "Episode:39, Validation Loss:9.434543608222157e-05, Acc:99.3167%\n",
      "Episode:40, Validation Loss:9.430512727703899e-05, Acc:99.2917%\n",
      "Episode:41, Validation Loss:9.083862096304074e-05, Acc:99.3000%\n",
      "Episode:42, Validation Loss:9.548445086693391e-05, Acc:99.2500%\n",
      "Episode:43, Validation Loss:8.758768672123551e-05, Acc:99.3083%\n",
      "Episode:44, Validation Loss:8.836793131195009e-05, Acc:99.3417%\n",
      "Episode:45, Validation Loss:9.649720595916733e-05, Acc:99.2667%\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Episode:46, Validation Loss:9.42448532441631e-05, Acc:99.2750%\n",
      "Episode:47, Validation Loss:9.248613059753552e-05, Acc:99.2833%\n",
      "Episode:48, Validation Loss:9.058682917384431e-05, Acc:99.3000%\n",
      "Episode:49, Validation Loss:9.086295176530257e-05, Acc:99.2917%\n",
      "Episode:50, Validation Loss:9.161016350844875e-05, Acc:99.2833%\n",
      "Episode:51, Validation Loss:9.100054739974439e-05, Acc:99.3083%\n",
      "Episode:52, Validation Loss:9.047071216627955e-05, Acc:99.3083%\n",
      "Episode:53, Validation Loss:9.114077693084255e-05, Acc:99.3083%\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Episode:54, Validation Loss:8.999613783089444e-05, Acc:99.3167%\n",
      "Episode:55, Validation Loss:9.096405119635165e-05, Acc:99.3250%\n",
      "Episode:56, Validation Loss:9.038166899699718e-05, Acc:99.3250%\n",
      "Episode:57, Validation Loss:9.083120676223189e-05, Acc:99.3250%\n",
      "Episode:58, Validation Loss:9.03512118384242e-05, Acc:99.3167%\n",
      "Episode:59, Validation Loss:8.969859482022002e-05, Acc:99.3250%\n",
      "Episode:60, Validation Loss:9.055741247721016e-05, Acc:99.3333%\n",
      "Episode:61, Validation Loss:9.08536312635988e-05, Acc:99.3000%\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Episode:62, Validation Loss:9.040917211677879e-05, Acc:99.3250%\n",
      "Episode:63, Validation Loss:9.10558519535698e-05, Acc:99.3250%\n",
      "Episode:64, Validation Loss:9.110457904171199e-05, Acc:99.3000%\n",
      "Episode:65, Validation Loss:9.013264207169414e-05, Acc:99.3250%\n",
      "Episode:66, Validation Loss:9.12657196749933e-05, Acc:99.3083%\n",
      "Episode:67, Validation Loss:9.055645205080509e-05, Acc:99.3333%\n",
      "Episode:68, Validation Loss:9.03544423636049e-05, Acc:99.3250%\n",
      "Episode:69, Validation Loss:8.951768541010097e-05, Acc:99.3417%\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Episode:70, Validation Loss:8.984384476207197e-05, Acc:99.3333%\n",
      "===================Best Fold:1 Saved, Acc:0.9935==================\n",
      "======================================================\n",
      "Episode:1, Validation Loss:0.0015255112666636705, Acc:87.0083%\n",
      "Episode:2, Validation Loss:0.0005611598608084023, Acc:95.2917%\n",
      "Episode:3, Validation Loss:0.00035399175249040127, Acc:97.2417%\n",
      "Episode:4, Validation Loss:0.00029971153708174825, Acc:97.5417%\n",
      "Episode:5, Validation Loss:0.000569063879083842, Acc:95.5917%\n",
      "Episode:6, Validation Loss:0.0003765934961847961, Acc:96.8417%\n",
      "Episode:7, Validation Loss:0.0003325081488583237, Acc:97.2917%\n",
      "Episode:8, Validation Loss:0.00020292599219828844, Acc:98.4417%\n",
      "Episode:9, Validation Loss:0.0001578351075295359, Acc:98.7167%\n",
      "Episode:10, Validation Loss:0.0002245877549285069, Acc:98.0167%\n",
      "Episode:11, Validation Loss:0.00018505101616028696, Acc:98.4000%\n",
      "Episode:12, Validation Loss:0.0001935964246513322, Acc:98.3500%\n",
      "Episode:13, Validation Loss:0.00021545318304561079, Acc:98.2500%\n",
      "Episode:14, Validation Loss:0.00016627686272840947, Acc:98.6833%\n",
      "Episode:15, Validation Loss:0.00015809075557626784, Acc:98.6333%\n",
      "Episode:16, Validation Loss:0.00011243106564506888, Acc:99.0417%\n",
      "Episode:17, Validation Loss:0.0001387138181598857, Acc:98.8667%\n",
      "Episode:18, Validation Loss:0.00014629751967731863, Acc:98.7000%\n",
      "Episode:19, Validation Loss:0.00017751385166775435, Acc:98.5250%\n",
      "Episode:20, Validation Loss:0.0001203875508508645, Acc:99.0083%\n",
      "Episode:21, Validation Loss:0.00011239431478315964, Acc:99.1083%\n",
      "Episode:22, Validation Loss:0.00011561014980543405, Acc:99.0000%\n",
      "Episode:23, Validation Loss:0.00022663992422167212, Acc:98.3000%\n",
      "Episode:24, Validation Loss:9.721420065034181e-05, Acc:99.2083%\n",
      "Episode:25, Validation Loss:0.00012617606262210757, Acc:98.9833%\n",
      "Episode:26, Validation Loss:9.735507774166763e-05, Acc:99.2167%\n",
      "Episode:27, Validation Loss:0.00011367650586180389, Acc:99.0917%\n",
      "Episode:28, Validation Loss:0.00010808473598444834, Acc:99.1583%\n",
      "Episode:29, Validation Loss:9.367995517095551e-05, Acc:99.2417%\n",
      "Episode:30, Validation Loss:0.00015087808424141258, Acc:98.7333%\n",
      "Episode:31, Validation Loss:9.755770588526502e-05, Acc:99.2250%\n",
      "Episode:32, Validation Loss:8.255521970568225e-05, Acc:99.3167%\n",
      "Episode:33, Validation Loss:0.00012377387611195445, Acc:98.9833%\n",
      "Episode:34, Validation Loss:0.00014451361494138837, Acc:98.8417%\n",
      "Episode:35, Validation Loss:0.00011803111556218937, Acc:98.9000%\n",
      "Episode:36, Validation Loss:8.52115626912564e-05, Acc:99.2833%\n",
      "Episode:37, Validation Loss:9.137402958003804e-05, Acc:99.2583%\n",
      "Episode:38, Validation Loss:8.567864279029891e-05, Acc:99.2500%\n",
      "Episode:39, Validation Loss:0.00011538091348484159, Acc:98.9667%\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Episode:40, Validation Loss:8.377920312341303e-05, Acc:99.3083%\n",
      "Episode:41, Validation Loss:6.839303387096152e-05, Acc:99.3917%\n",
      "Episode:42, Validation Loss:6.576321175089106e-05, Acc:99.4583%\n",
      "Episode:43, Validation Loss:5.864943523192778e-05, Acc:99.5000%\n",
      "Episode:44, Validation Loss:5.800858707516454e-05, Acc:99.5167%\n",
      "Episode:45, Validation Loss:5.784428503829986e-05, Acc:99.5000%\n",
      "Episode:46, Validation Loss:6.109799141995609e-05, Acc:99.4917%\n",
      "Episode:47, Validation Loss:5.9902358771068975e-05, Acc:99.4750%\n",
      "Episode:48, Validation Loss:5.9348298236727715e-05, Acc:99.4833%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    epochs = 70\n",
    "    period = 100\n",
    "    ensemble_models = []\n",
    "    lr = 1e-3\n",
    "    val_period = 1\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for fold in range(len(train_loaders)):\n",
    "        train_loader = train_loaders[fold]\n",
    "        val_loader = val_loaders[fold]\n",
    "        \n",
    "        model = convNet(in_channels=1)\n",
    "        model.cuda()\n",
    "        torch.cuda.empty_cache()    #Need further check\n",
    "            \n",
    "        max_acc = 0\n",
    "        best_model_dict = None\n",
    "        data_num = 0\n",
    "        loss_avg = 0\n",
    "#         optimizer = torch.optim.SGD(model.parameters(),lr=lr)\n",
    "#         optimizer = torch.optim.RMSprop(model.parameters(),lr=lr,alpha=0.9)\n",
    "        optimizer = torch.optim.Adam(model.parameters(),lr=lr,betas=(0.9,0.99))\n",
    "#         optimizer = torch.optim.Adagrad(model.parameters(),lr=lr)\n",
    "#         lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=period,T_mult=1,eta_min=1e-7) #original \n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, patience=7)\n",
    "    \n",
    "        for ep in range(0,epochs+1):\n",
    "            model.train()\n",
    "            for idx, data in enumerate(train_loader):\n",
    "                img, target = data\n",
    "                img, target = img.to(device), target.to(device,dtype=torch.long)\n",
    "    #             print(np.shape(img),np.shape(target)) #Tensor(4,1,28,28), Tensor(4)\n",
    "    #             print(np.max(img.cpu().numpy()),np.min(img.cpu().numpy())) #1.0 0.0\n",
    "                pred = model(img)\n",
    "    #             print(pred.size())   #(32,10)\n",
    "    #             print(target.size()) #(32,)\n",
    "                ###Input shape must be pred:, target:\n",
    "                loss = criterion(pred,target)\n",
    "                loss_avg += loss.item()\n",
    "                data_num += img.size(0)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            ###Cosine annealing\n",
    "    #         lr_scheduler.step()\n",
    "\n",
    "            ###Evaluate Train Loss \n",
    "#             if ep%2 == 0:\n",
    "#                 loss_avg /= data_num\n",
    "#                 print(\"Ep:{}, loss:{}, lr:{}\".format(ep, loss_avg,optimizer.param_groups[0]['lr']))\n",
    "#                 loss_avg = 0\n",
    "#                 data_num = 0\n",
    "\n",
    "            ###Validation\n",
    "            if ep!=0 and ep%val_period == 0:\n",
    "                model.eval()\n",
    "                acc = 0\n",
    "                val_loss = 0\n",
    "                data_num  = 0\n",
    "                with torch.no_grad():\n",
    "                    for idx, data in enumerate(val_loader):\n",
    "                        img, target = data\n",
    "                        img, target = img.to(device), target.to(device,dtype=torch.long)\n",
    "                        pred = model(img)\n",
    "                        val_loss += criterion(pred, target)\n",
    "                        # print(pred)\n",
    "                        _,pred_class = torch.max(pred.data, 1)\n",
    "    #                     print(pred_class)\n",
    "                        acc += (pred_class == target).sum().item()\n",
    "                        data_num += img.size(0)\n",
    "\n",
    "                acc /= data_num\n",
    "                val_loss /= data_num\n",
    "\n",
    "                ###Plateau\n",
    "                lr_scheduler.step(val_loss)\n",
    "#                 lr_scheduler.step(-1*acc)\n",
    "\n",
    "                if acc > max_acc:\n",
    "                    max_acc = acc\n",
    "                    best_model_dict = model.state_dict()\n",
    "                print(\"Episode:{}, Validation Loss:{}, Acc:{:.4f}%\".format(ep,val_loss,acc*100))\n",
    "\n",
    "        ###K-Fold ensemble: Saved k best model for k dataloader\n",
    "        print(\"===================Best Fold:{} Saved, Acc:{}==================\".format(fold,max_acc))\n",
    "        torch.save(best_model_dict, \"./Kmnist_saved_model/Fold{}_ep{}_acc{:.4f}\".format(fold,ep,max_acc))\n",
    "        print(\"======================================================\")\n",
    "\n",
    "            \n",
    "            ###Snapshot ensemble: saved model\n",
    "#             if ep!=0 and ep%period == 0:\n",
    "# #                 ensemble_models.append(best_model_dict)\n",
    "#                 model_id = ep//period\n",
    "#                 print(\"===================Best Model{} Saved, Acc:{}==================\".format(model_id,max_acc))\n",
    "#                 torch.save(best_model_dict, \"./Kmnist_saved_model/model{}_ep{}_acc{:.4f}\".format(model_id,ep,max_acc))\n",
    "#                 print(\"======================================================\")\n",
    "#                 max_acc = 0\n",
    "        \n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_root = \"/home/ccchang/localization_net/Kmnist_saved_model/emsemble/5_fold_ep80_lr1e-2\"\n",
    "ensemble_models = []\n",
    "epochs = 500\n",
    "period = 100\n",
    "model_num = epochs//period\n",
    "model = 5\n",
    "data_num = 0\n",
    "acc = 0\n",
    "\n",
    "for file_name in os.listdir(ensemble_root):\n",
    "    model = convNet(in_channels=1)\n",
    "    model.cuda()\n",
    "    model.load_state_dict(torch.load(\"{}/{}\".format(ensemble_root,file_name)))\n",
    "    model.eval()\n",
    "    ensemble_models.append(model)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, data in enumerate(validate_loader):\n",
    "        img, target = data\n",
    "        img, target = img.to(device), target.to(device,dtype=torch.long)\n",
    "\n",
    "        ###Single model\n",
    "#         pred = model(img)\n",
    "#         _,pred_class = torch.max(pred.data, 1)\n",
    "        \n",
    "        ###Average Ensemble\n",
    "#         pred_list = torch.Tensor([]).to(device)\n",
    "#         for i in range(model_num):\n",
    "#             pred = ensemble_models[i](img) #(batch_num,10)\n",
    "#             pred_list = torch.cat((pred_list,pred.unsqueeze(2)),dim=2) #pred_list: (batch_num,10,model_num)\n",
    "#         pred = torch.mean(pred_list,dim=2)   #(batch,10)\n",
    "        \n",
    "#         _,pred_class = torch.max(pred.data, 1)   #(batch_num,)\n",
    "#         val_loss += criterion(pred, target)\n",
    "\n",
    "        ###Voting Ensemble\n",
    "        pred_list = torch.LongTensor([]).to(device)\n",
    "        for i in range(model_num):\n",
    "            pred = ensemble_models[i](img) #(batch_num,10)\n",
    "            _,pred_class = torch.max(pred.data, 1)   #(batch_num,)\n",
    "            pred_list = torch.cat((pred_list,pred_class.unsqueeze(1)),dim=1)\n",
    "            \n",
    "        pred_class_list = torch.LongTensor([]).to(device)\n",
    "        for i in range(img.size(0)):\n",
    "            pred_np = pred_list[i].cpu().numpy()\n",
    "            unique_class,count = np.unique(pred_np,return_counts=True)\n",
    "            unique_class = np.array(unique_class[np.argmax(count)]).reshape(-1)   #unique class shape(1,)\n",
    "            class_voted= torch.from_numpy(unique_class).to(device)    #(1,)\n",
    "            pred_class_list = torch.cat((pred_class_list,class_voted))    \n",
    "    \n",
    "#         acc += (pred_class == target).sum().item()\n",
    "        acc += (pred_class_list == target).sum().item()\n",
    "        data_num += img.size(0)\n",
    "\n",
    "#     val_loss /= data_num\n",
    "    acc /= data_num\n",
    "    print(\"Acc:{:.4f}%\".format(acc*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "t1 = torch.Tensor([[1,2,3,4],[4,3,2,1],[1,5,3,3]])  #(3,4)\n",
    "t1 = t1.unsqueeze(2)\n",
    "\n",
    "t_list = torch.Tensor([])\n",
    "\n",
    "for i in range(3):\n",
    "    t_list = torch.cat((t_list,t1),dim=2)\n",
    "\n",
    "print(t_list.size())\n",
    "print(t_list)\n",
    "t_list = torch.mean(t_list,dim=2)\n",
    "print(t_list.size())\n",
    "print(t_list)\n",
    "\n",
    "\n",
    "# n1 = t1.cpu().numpy()\n",
    "\n",
    "# n1, count = np.unique(n1,return_counts=True,axis=0)\n",
    "# print(count)\n",
    "# n1 = np.argmax(count)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
