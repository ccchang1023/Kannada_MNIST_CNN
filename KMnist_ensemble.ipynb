{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torchsummary import summary\n",
    "device = \"cuda\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Squeeze and Excitation Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Seq_Ex_Block(nn.Module):\n",
    "    def __init__(self, in_ch, r=16):\n",
    "        super(Seq_Ex_Block, self).__init__()\n",
    "        self.se = nn.Sequential(\n",
    "            GlobalAvgPool(),\n",
    "            nn.Linear(in_ch, in_ch//r),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_ch//r, in_ch),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        se_weight = self.se(x).unsqueeze(-1).unsqueeze(-1)\n",
    "#         print(f'x:{x.sum()}, x_se:{x.mul(se_weight).sum()}')\n",
    "        return x.mul(se_weight)\n",
    "\n",
    "class GlobalAvgPool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalAvgPool, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x.view(*(x.shape[:-2]),-1).mean(-1)\n",
    "\n",
    "class SE_Net(nn.Module):\n",
    "    def __init__(self,in_channels):\n",
    "        super(SE_Net,self).__init__()\n",
    "        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, \n",
    "        #                dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "        self.c1 = nn.Conv2d(in_channels=in_channels, out_channels=64,kernel_size=3,stride=1,padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=64,eps=1e-3,momentum=0.01)\n",
    "        self.c2 = nn.Conv2d(64,64,3,1,0)\n",
    "        self.bn2 = nn.BatchNorm2d(64,1e-3,0.01)\n",
    "        self.c3 = nn.Conv2d(64,64,5,1,2)\n",
    "        self.bn3 = nn.BatchNorm2d(64,1e-3,0.01)\n",
    "        self.m1 = nn.MaxPool2d(2)\n",
    "        self.d1 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.c4 = nn.Conv2d(64,128,3,1,0)\n",
    "        self.bn4 = nn.BatchNorm2d(128,1e-3,0.01)\n",
    "        self.c5 = nn.Conv2d(128,128,3,1,0)\n",
    "        self.bn5 = nn.BatchNorm2d(128,1e-3,0.01)\n",
    "        self.c6 = nn.Conv2d(128,128,5,1,2)\n",
    "        self.bn6 = nn.BatchNorm2d(128,1e-3,0.01)        \n",
    "        self.m2 = nn.MaxPool2d(2)\n",
    "        self.d2 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.c7 = nn.Conv2d(128,256,3,1,0)\n",
    "        self.bn7 = nn.BatchNorm2d(256,1e-3,0.01)\n",
    "        self.se1 = Seq_Ex_Block(in_ch=256,r=16)\n",
    "        self.d3 = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc1 = nn.Linear(256*2*2,256)\n",
    "        self.bn8 = nn.BatchNorm1d(256,1e-3,0.01)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256,128)\n",
    "        self.bn9 = nn.BatchNorm1d(128,1e-3,0.01)\n",
    "        \n",
    "        self.out = nn.Linear(128,10)\n",
    "        \n",
    "        self.init_linear_weights()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.bn1(F.relu(self.c1(x)))\n",
    "        x = self.bn2(F.relu(self.c2(x)))\n",
    "        x = self.bn3(F.relu(self.c3(x)))\n",
    "        x = self.m1(x)\n",
    "        x = self.d1(x)\n",
    "        \n",
    "        x = self.bn4(F.relu(self.c4(x)))\n",
    "        x = self.bn5(F.relu(self.c5(x)))\n",
    "        x = self.bn6(F.relu(self.c6(x)))\n",
    "        x = self.m2(x)\n",
    "        x = self.d2(x)\n",
    "        \n",
    "        x = self.bn7(F.relu(self.c7(x)))\n",
    "        x = self.se1(x)\n",
    "        \n",
    "        x = self.d3(x)        \n",
    "        \n",
    "        x = x.view(-1, 256*2*2) #reshape\n",
    "        \n",
    "        x = self.bn8(self.fc1(x))\n",
    "        x = self.bn9(self.fc2(x))\n",
    "        \n",
    "        return self.out(x)\n",
    "    \n",
    "    def init_linear_weights(self):\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in')  #default mode: fan_in\n",
    "        nn.init.kaiming_normal_(self.fc2.weight, mode='fan_in')\n",
    "        nn.init.kaiming_normal_(self.out.weight, mode='fan_in')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class convNet(nn.Module):\n",
    "    def __init__(self,in_channels):\n",
    "        super(convNet,self).__init__()\n",
    "        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, \n",
    "        #                dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "        self.c1 = nn.Conv2d(in_channels=in_channels, out_channels=64,kernel_size=3,stride=1,padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=64,eps=1e-3,momentum=0.01)\n",
    "        self.c2 = nn.Conv2d(64,64,3,1,0)\n",
    "        self.bn2 = nn.BatchNorm2d(64,1e-3,0.01)\n",
    "        self.c3 = nn.Conv2d(64,64,5,1,2)\n",
    "        self.bn3 = nn.BatchNorm2d(64,1e-3,0.01)\n",
    "        self.m1 = nn.MaxPool2d(2)\n",
    "        self.d1 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.c4 = nn.Conv2d(64,128,3,1,0)\n",
    "        self.bn4 = nn.BatchNorm2d(128,1e-3,0.01)\n",
    "        self.c5 = nn.Conv2d(128,128,3,1,0)\n",
    "        self.bn5 = nn.BatchNorm2d(128,1e-3,0.01)\n",
    "        self.c6 = nn.Conv2d(128,128,5,1,2)\n",
    "        self.bn6 = nn.BatchNorm2d(128,1e-3,0.01)        \n",
    "        self.m2 = nn.MaxPool2d(2)\n",
    "        self.d2 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.c7 = nn.Conv2d(128,256,3,1,0)\n",
    "        self.bn7 = nn.BatchNorm2d(256,1e-3,0.01)\n",
    "        self.m3 = nn.MaxPool2d(2)\n",
    "        self.d3 = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc1 = nn.Linear(256*1*1,256)\n",
    "        self.bn8 = nn.BatchNorm1d(256,1e-3,0.01)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256,128)\n",
    "        self.bn9 = nn.BatchNorm1d(128,1e-3,0.01)\n",
    "        \n",
    "        self.out = nn.Linear(128,10)\n",
    "        \n",
    "        self.init_linear_weights()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.bn1(F.relu(self.c1(x)))\n",
    "        x = self.bn2(F.relu(self.c2(x)))\n",
    "        x = self.bn3(F.relu(self.c3(x)))\n",
    "        x = self.m1(x)\n",
    "        x = self.d1(x)\n",
    "        \n",
    "        x = self.bn4(F.relu(self.c4(x)))\n",
    "        x = self.bn5(F.relu(self.c5(x)))\n",
    "        x = self.bn6(F.relu(self.c6(x)))\n",
    "        x = self.m2(x)\n",
    "        x = self.d2(x)\n",
    "        \n",
    "        x = self.bn7(F.relu(self.c7(x)))\n",
    "        x = self.m3(x)\n",
    "        x = self.d3(x)        \n",
    "        \n",
    "        x = x.view(-1, 256*1*1) #reshape\n",
    "        \n",
    "        x = self.bn8(self.fc1(x))\n",
    "        x = self.bn9(self.fc2(x))\n",
    "        \n",
    "        return self.out(x)\n",
    "    \n",
    "    def init_linear_weights(self):\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in')  #default mode: fan_in\n",
    "        nn.init.kaiming_normal_(self.fc2.weight, mode='fan_in')\n",
    "        nn.init.kaiming_normal_(self.out.weight, mode='fan_in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = convNet(in_channels=1)\n",
    "# model = SE_Net(in_channels=1)\n",
    "model.cuda()\n",
    "summary(model, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from imgaug import augmenters as iaa\n",
    "# from imgaug.augmentables.segmaps import SegmentationMapOnImage\n",
    "\n",
    "class ImgAugTransform:\n",
    "    def __init__(self):\n",
    "        self.aug = iaa.Sequential([\n",
    "#         iaa.Scale((640, 480)),\n",
    "#         iaa.Fliplr(0.5),\n",
    "            \n",
    "#         iaa.Sometimes(0.5, iaa.GaussianBlur(sigma=(0, 0.6))),\n",
    "        iaa.Sometimes(0.1, iaa.AverageBlur(1.2)),\n",
    "        iaa.Sometimes(0.5, iaa.Affine(rotate=(-35, 35),order=[0, 1],translate_px={\"x\":(-3, 3),\"y\":(-4,4)},mode='symmetric')),\n",
    "        iaa.Sometimes(0.5,iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.25))),\n",
    "        iaa.Sometimes(0.1, iaa.SaltAndPepper(0.05,False)),\n",
    "        iaa.Invert(0.5),\n",
    "#         iaa.Add((-5, 5)), # change brightness of images (by -10 to 10 of original value)\n",
    "#         iaa.AdditiveGaussianNoise(-1,1)\n",
    "        iaa.Sometimes(0.2,iaa.GammaContrast(2))\n",
    "            \n",
    "#         iaa.AddToHueAndSaturation(from_colorspace=\"GRAY\",value=(-20, 20))  #Hue-> color, saturation -> saido\n",
    "    ])\n",
    "    def __call__(self, img, mask=None):\n",
    "        img = np.array(img)        \n",
    "        return self.aug.augment_image(image=img)\n",
    "#         return self.aug(image=img, segmentation_maps=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trans and Dataset definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "trans = transforms.Compose([\n",
    "#         transforms.ColorJitter(0.9,0.2,0.2,0.5),\n",
    "        transforms.RandomAffine(degrees=10,translate=(0.25,0.25),scale=(0.75,1.25),shear=5),\n",
    "#         transforms.RandomAffine(degrees=10,translate=(0.2,0.2),scale=[0.9,1.1]), #For native distinguisher\n",
    "#         ImgAugTransform(),\n",
    "#         lambda x: Image.fromarray(x),\n",
    "        transforms.ToTensor(),  #Take Image as input and convert to tensor with value from 0 to1  \n",
    "#         transforms.Normalize(mean=[0.08889289],std=[0.24106446])  #train_large dataset distribution\n",
    "#         transforms.Normalize(mean=[0.08229437],std=[0.23876116]) #train dataset dist\n",
    "#         transforms.Normalize(mean=[0.09549136],std=[0.24336776]) #dig_augmented distribution\n",
    "    ])\n",
    "\n",
    "trans_val = transforms.Compose([\n",
    "        transforms.ToTensor(),  #Take Image as input and convert to tensor with value from 0 to1\n",
    "#         transforms.Normalize(mean=[0.08889289],std=[0.24106446])  #train_large dataset distribution\n",
    "#         transforms.Normalize(mean=[0.08229437],std=[0.23876116]) #train dataset dist\n",
    "#         transforms.Normalize(mean=[0.09549136],std=[0.24336776]) #dig_augmented distribution\n",
    "    ])\n",
    "\n",
    "trans_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.08889289],std=[0.24106446])  #train_large dataset distribution\n",
    "#         transforms.Normalize(mean=[0.08229437],std=[0.23876116]) #train dataset dist\n",
    "#         transforms.Normalize(mean=[0.09549136],std=[0.24336776]) #dig_augmented distribution\n",
    "])\n",
    "\n",
    "global_data = pd.read_csv(\"./dataset/train.csv\")\n",
    "global_dig_aug_data = pd.read_csv(\"./dataset/Dig-Mnist-Augmented.csv\")\n",
    "# global_dig_data = pd.read_csv(\"./dataset/Dig-MNIST.csv\")\n",
    "global_data_large = pd.read_csv(\"./dataset/train_large.csv\")\n",
    "\n",
    "class KMnistDataset(Dataset):\n",
    "    def __init__(self,data_len=None, is_validate=False,validate_rate=None,indices=None):\n",
    "        self.is_validate = is_validate\n",
    "        self.data = global_data\n",
    "#         self.data = global_dig_aug_data    ################Temp Revised Caution##############\n",
    "        \n",
    "#         print(\"data shape:\", np.shape(self.data))\n",
    "        if data_len == None:\n",
    "            data_len = len(self.data)\n",
    "        \n",
    "        self.indices = indices\n",
    "        if self.is_validate:\n",
    "            self.len = int(data_len*validate_rate)\n",
    "            self.offset = int(data_len*(1-validate_rate))\n",
    "            self.transform = trans_val\n",
    "        else:\n",
    "            self.len = int(data_len*(1-validate_rate))\n",
    "            self.offset = 0\n",
    "            self.transform = trans\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        idx += self.offset\n",
    "        idx = self.indices[idx]\n",
    "#         print(idx)\n",
    "        img = self.data.iloc[idx, 1:].values.astype(np.uint8).reshape((28, 28))  #value: 0~255\n",
    "        label = self.data.iloc[idx, 0]  #(num,)\n",
    "        img = Image.fromarray(img)\n",
    "        img = self.transform(img)     #value: 0~1, shape:(1,28,28)\n",
    "        label = torch.as_tensor(label, dtype=torch.uint8)    #value: 0~9, shape(1)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "class KMnistDataset_binary_aid(Dataset):\n",
    "    def __init__(self,data_len=None, is_validate=False,validate_rate=None,indices=None):\n",
    "        self.is_validate = is_validate\n",
    "        self.data = global_data_large\n",
    "        \n",
    "        if data_len == None:\n",
    "            data_len = len(self.data)\n",
    "        \n",
    "        self.indices = indices\n",
    "        if self.is_validate:\n",
    "            self.len = int(data_len*validate_rate)\n",
    "            self.offset = int(data_len*(1-validate_rate))\n",
    "            self.transform = trans_val\n",
    "        else:\n",
    "            self.len = int(data_len*(1-validate_rate))\n",
    "            self.offset = 0\n",
    "            self.transform = trans\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        idx += self.offset\n",
    "        idx = self.indices[idx]\n",
    "#         print(idx)\n",
    "        img = self.data.iloc[idx, 2:].values.astype(np.uint8).reshape((28, 28))  #value: 0~255\n",
    "        native_label = self.data.iloc[idx, 0]  #(num,)\n",
    "        label = self.data.iloc[idx, 1]  #(num,)\n",
    "        img = Image.fromarray(img)\n",
    "        img = self.transform(img)     #value: 0~1, shape:(1,28,28)\n",
    "#         native_label = torch.as_tensor(native_label, dtype=torch.uint8).unsqueeze(0)    #value: 0~9, shape(1,1) for BCE loss\n",
    "        native_label = torch.as_tensor(native_label, dtype=torch.uint8)    #value: 0~9, shape(1) for CSE loss\n",
    "        label = torch.as_tensor(label, dtype=torch.uint8)    #value: 0~9, shape(1)\n",
    "        return img, native_label, label\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    \n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,data_len=None):\n",
    "        self.data = pd.read_csv(\"./dataset/test.csv\")\n",
    "        print(\"data shape:\", np.shape(self.data))\n",
    "        self.transform = trans_test\n",
    "        if data_len == None:\n",
    "            self.len = len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.data.iloc[idx, 1:].values.astype(np.uint8).reshape((28, 28))  #value: 0~255\n",
    "        img = Image.fromarray(img)\n",
    "        img = self.transform(img)     #value: 0~1, shape:(1,28,28)\n",
    "        return img, torch.Tensor([])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get kfold dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kfold_dataset_loader(k=5,val_rate=0.1,indices_len=None, batch_size=None,num_workers=None, binary_aid=False):\n",
    "    ###Return [list of train dataset_loader, list of val dataset_loader]\n",
    "    train_loader_list = []\n",
    "    val_loader_list = []\n",
    "    indices = np.arange(indices_len)\n",
    "    val_len = indices_len//k\n",
    "    idx = 0\n",
    "    \n",
    "    for i in range(k):\n",
    "#         np.random.shuffle(indices)  #Random cross validation\n",
    "        ind = np.concatenate([indices[:idx],indices[idx+val_len:],indices[idx:idx+val_len]])\n",
    "        idx += val_len\n",
    "#         print(ind)\n",
    "        \n",
    "        if binary_aid == True:\n",
    "            train_dataset = KMnistDataset_binary_aid(data_len=None,is_validate=False, validate_rate=val_rate,indices=ind)\n",
    "            val_dataset = KMnistDataset_binary_aid(data_len=None,is_validate=True, validate_rate=val_rate, indices=ind)\n",
    "        else:\n",
    "            train_dataset = KMnistDataset(data_len=None,is_validate=False, validate_rate=val_rate,indices=ind)\n",
    "            val_dataset = KMnistDataset(data_len=None,is_validate=True, validate_rate=val_rate, indices=ind)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "        \n",
    "        train_loader_list.append(train_loader)\n",
    "        val_loader_list.append(val_loader)\n",
    "        \n",
    "    return train_loader_list, val_loader_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_model(native_net=False):\n",
    "    #Advance model\n",
    "    # model_name = 'efficientnet-b0'\n",
    "    # image_size = EfficientNet.get_image_size(model_name)\n",
    "    # model = EfficientNet.from_pretrained(model_name, num_classes=10)\n",
    "    # model = model.to(device)\n",
    "\n",
    "    #Basic cnn\n",
    "    if native_net == True:\n",
    "        model = convNet_native(in_channels=1)\n",
    "    else:\n",
    "#         model = SE_Net(in_channels=1)\n",
    "        model = convNet(in_channels=1)\n",
    "    \n",
    "    # model.load_state_dict(torch.load(\"./Kmnist_saved_model/ep20_acc0.9910\"))\n",
    "\n",
    "    #pretrained model\n",
    "#     model = models.resnet18()\n",
    "#     model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,bias=False)\n",
    "# #     summary(model, input_size=(1, 28, 28))\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        model.cuda()\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get dataset distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# train distribution: mean=[0.08229437],std=[0.23876116]\n",
    "# dig augmented distribution: mean=[0.09549136],std=[0.24336776]\n",
    "# train large distribution: mean=[0.08889286],std=[0.24106438]\n",
    "\n",
    "def get_dataset_mean_std(dataloader):\n",
    "    print(\"Calculate distribution:\")\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    nb_samples = 0.\n",
    "    for data in dataloader:\n",
    "        img = data[0].to(device)\n",
    "        batch_samples = img.size(0)\n",
    "        img = img.contiguous().view(batch_samples, img.size(1), -1)\n",
    "        mean += img.mean(2).sum(0)\n",
    "        std += img.std(2).sum(0)\n",
    "        nb_samples += batch_samples\n",
    "        if nb_samples%5120 == 0:\n",
    "            print(\"Finished:\", nb_samples)\n",
    "            \n",
    "    print(\"num of samples:\",nb_samples)\n",
    "    mean /= nb_samples\n",
    "    std /= nb_samples\n",
    "#     print(\"Average mean:\",mean)\n",
    "#     print(\"Average std:\", std)\n",
    "    return mean.cpu().numpy(), std.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get train and val loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1536\n",
    "num_workers = 12\n",
    "vr = 0.05\n",
    "k = 20\n",
    "indices_len = 60000\n",
    "# indices_len = 10240  ################Temp Revised Caution##############\n",
    "# indices_len = 120000\n",
    "\n",
    "###Single dataset\n",
    "# indices = np.arange(indices_len)\n",
    "# train_dataset = KMnistDataset(data_len=None,is_validate=False,validate_rate=vr,indices=indices)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "# mean, std = get_dataset_mean_std(train_loader)\n",
    "# print(\"train distribution: mean={},std={}\".format(mean, std))\n",
    "\n",
    "# indices = np.arange(10240)\n",
    "# dig_val_dataset = DigValDataset(data_len=None,indices=indices)\n",
    "# dig_val_loader = DataLoader(dig_val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "# mean, std = get_dataset_mean_std(dig_val_loader)\n",
    "# print(\"validate distribution:\",mean, std)\n",
    "\n",
    "# test_dataset = TestDataset(data_len=None)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "# mean, std = get_dataset_mean_std(test_loader)\n",
    "# print(\"test distribution:\",mean, std)\n",
    "\n",
    "###K-fold dataset\n",
    "train_loaders, val_loaders = get_kfold_dataset_loader(k, vr, indices_len, batch_size, num_workers, binary_aid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train native classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    epochs = 120\n",
    "    period = 40\n",
    "    ensemble_models = []\n",
    "    lr = 1e-3\n",
    "    val_period = 1\n",
    "    \n",
    "    criterion_b = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"Fold:\",len(train_loaders))\n",
    "    \n",
    "    for fold in range(len(train_loaders)):\n",
    "        train_loader = train_loaders[fold]\n",
    "        val_loader = val_loaders[fold]\n",
    "        \n",
    "        model = get_model(native_net=True)\n",
    "        torch.cuda.empty_cache()    #Need further check\n",
    "            \n",
    "        max_acc_b = 0\n",
    "        min_loss_b = 10000\n",
    "        best_model_dict = None\n",
    "        data_num = 0\n",
    "        loss_avg_b = 0\n",
    "\n",
    "#         optimizer = torch.optim.SGD(model.parameters(),lr=lr)\n",
    "#         optimizer = torch.optim.RMSprop(model.parameters(),lr=lr,alpha=0.9)\n",
    "        optimizer = torch.optim.Adam(model.parameters(),lr=lr,betas=(0.9,0.99))\n",
    "#         optimizer = torch.optim.Adagrad(model.parameters(),lr=lr)\n",
    "#         lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=period,T_mult=1,eta_min=1e-5) #original \n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, patience=15)\n",
    "        \n",
    "        tmp_count = 0\n",
    "        for ep in range(0,epochs+1):\n",
    "            model.train()\n",
    "            for idx, data in enumerate(train_loader):\n",
    "                img, target_b, target = data\n",
    "                img, target_b, target = img.to(device), target_b.to(device,dtype=torch.long), target.to(device,dtype=torch.long)\n",
    "                pred_b = model(img)\n",
    "                  \n",
    "                loss_b = criterion_b(pred_b,target_b) \n",
    "                loss_avg_b += loss_b.item()\n",
    "                \n",
    "                data_num += img.size(0)\n",
    "                optimizer.zero_grad()\n",
    "                loss_b.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            ###Cosine annealing\n",
    "#             lr_scheduler.step()\n",
    "\n",
    "            ###Evaluate Train Loss \n",
    "#             if ep%2 == 0:\n",
    "#                 loss_avg /= data_num\n",
    "#                 print(\"Ep:{}, loss:{}, lr:{}\".format(ep, loss_avg,optimizer.param_groups[0]['lr']))\n",
    "#                 loss_avg = 0\n",
    "#                 data_num = 0\n",
    "\n",
    "            ###Validation\n",
    "            if ep!=0 and ep%val_period == 0:\n",
    "                model.eval()\n",
    "                acc_b = 0\n",
    "                val_loss_b = 0\n",
    "                data_num  = 0\n",
    "                with torch.no_grad():\n",
    "                    for idx, data in enumerate(val_loader):\n",
    "                        img, target_b, target = data\n",
    "                        img, target_b, target = img.to(device), target_b.to(device,dtype=torch.long), target.to(device,dtype=torch.long)\n",
    "                        pred_b = model(img)\n",
    "\n",
    "                        val_loss_b += criterion_b(pred_b,target_b).item()\n",
    "                        \n",
    "                        # print(pred) \n",
    "                        ########\n",
    "                        _,pred_native_class = torch.max(pred_b.data, 1)\n",
    "                        \n",
    "                        acc_b += (pred_native_class == target_b).sum().item()\n",
    "                        data_num += img.size(0)\n",
    "\n",
    "                acc_b /= data_num\n",
    "                val_loss_b /= data_num\n",
    "\n",
    "                ###Plateau\n",
    "                lr_scheduler.step(val_loss_b)\n",
    "                if optimizer.param_groups[0]['lr'] < 1e-4:\n",
    "                    break                    \n",
    "\n",
    "                if acc_b >= max_acc_b:\n",
    "                    max_acc_b = acc_b\n",
    "                    best_model_dict = model.state_dict()\n",
    "                    \n",
    "                if val_loss_b <= min_loss:\n",
    "                    min_loss_b = val_loss_b\n",
    "#                     best_model_dict = model.state_dict()\n",
    "                \n",
    "                print(\"Episode:{}, Validation Loss:{},Acc_b:{:.3f}%,lr:{}\"\n",
    "                      .format(ep,val_loss_b,acc_b*100,optimizer.param_groups[0]['lr']))\n",
    "            \n",
    "            if ep!=0 and ep%10 == 0:\n",
    "                torch.save(best_model_dict, \"./Kmnist_saved_model/tmp_Fold{}_acc_b{:.3f}\".format(fold,max_acc_b*1e2))\n",
    "            \n",
    "        ###K-Fold ensemble: Saved k best model for k dataloader\n",
    "        print(\"===================Best Fold:{} Saved, Acc:{}==================\".format(fold,max_acc_b))\n",
    "        torch.save(best_model_dict, \"./Kmnist_saved_model/Fold{}_loss{:.4f}_acc_b{:.3f}\".format(fold,min_loss_b*1e3,max_acc_b*1e2))\n",
    "        print(\"======================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train digit classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 20\n",
      "Episode:1, Validation Loss:0.00175231138865153,Acc:10.0000%,lr:0.001\n",
      "Episode:2, Validation Loss:0.0016267937024434407,Acc:14.2667%,lr:0.001\n",
      "Episode:3, Validation Loss:0.001746397097905477,Acc:18.7000%,lr:0.001\n",
      "Episode:4, Validation Loss:0.0011035808324813842,Acc:44.2667%,lr:0.001\n",
      "Episode:5, Validation Loss:0.0007912615140279134,Acc:56.9667%,lr:0.001\n",
      "Episode:6, Validation Loss:0.0013170596361160279,Acc:46.4667%,lr:0.001\n",
      "Episode:7, Validation Loss:0.0008130243221918742,Acc:63.3000%,lr:0.001\n",
      "Episode:8, Validation Loss:0.0005110407869021098,Acc:74.8000%,lr:0.001\n",
      "Episode:9, Validation Loss:0.00015571356316407522,Acc:94.2667%,lr:0.001\n",
      "Episode:10, Validation Loss:0.00010288108636935552,Acc:95.2000%,lr:0.001\n",
      "Episode:11, Validation Loss:4.562097166975339e-05,Acc:97.7000%,lr:0.001\n",
      "Episode:12, Validation Loss:6.735260784626008e-05,Acc:97.3000%,lr:0.001\n",
      "Episode:13, Validation Loss:6.455925852060318e-05,Acc:96.9667%,lr:0.001\n",
      "Episode:14, Validation Loss:5.564892664551735e-05,Acc:97.5667%,lr:0.001\n",
      "Episode:15, Validation Loss:5.6926878790060676e-05,Acc:97.4000%,lr:0.001\n",
      "Episode:16, Validation Loss:3.629338058332602e-05,Acc:98.5333%,lr:0.001\n",
      "Episode:17, Validation Loss:3.942586729923884e-05,Acc:98.4333%,lr:0.001\n",
      "Episode:18, Validation Loss:5.495338390270869e-05,Acc:98.0000%,lr:0.001\n",
      "Episode:19, Validation Loss:5.1745778570572535e-05,Acc:98.2000%,lr:0.001\n",
      "Episode:20, Validation Loss:4.659337177872658e-05,Acc:98.3667%,lr:0.001\n",
      "Episode:21, Validation Loss:6.14275187253952e-05,Acc:98.0667%,lr:0.001\n",
      "Episode:22, Validation Loss:4.993841548760732e-05,Acc:98.2333%,lr:0.001\n",
      "Episode:23, Validation Loss:4.772780835628509e-05,Acc:98.2667%,lr:0.001\n",
      "Episode:24, Validation Loss:5.969629188378652e-05,Acc:98.4333%,lr:0.001\n",
      "Episode:25, Validation Loss:4.392628992597262e-05,Acc:98.5000%,lr:0.001\n",
      "Episode:26, Validation Loss:3.414429475863775e-05,Acc:98.8000%,lr:0.001\n",
      "Episode:27, Validation Loss:6.33003090818723e-05,Acc:98.3000%,lr:0.001\n",
      "Episode:28, Validation Loss:6.214791908860207e-05,Acc:98.0333%,lr:0.001\n",
      "Episode:29, Validation Loss:3.9205923676490785e-05,Acc:98.9333%,lr:0.001\n",
      "Episode:30, Validation Loss:5.293221026659012e-05,Acc:98.3667%,lr:0.001\n",
      "Episode:31, Validation Loss:3.988963489731153e-05,Acc:98.7333%,lr:0.001\n",
      "Episode:32, Validation Loss:4.883726810415586e-05,Acc:98.4333%,lr:0.001\n",
      "Episode:33, Validation Loss:6.334406634171804e-05,Acc:98.2000%,lr:0.001\n",
      "Episode:34, Validation Loss:3.981656084458033e-05,Acc:98.7000%,lr:0.001\n",
      "Episode:35, Validation Loss:3.6085822929938634e-05,Acc:98.6000%,lr:0.001\n",
      "Episode:36, Validation Loss:5.146763722101847e-05,Acc:98.4000%,lr:0.001\n",
      "Episode:37, Validation Loss:3.965061157941818e-05,Acc:98.6333%,lr:0.001\n",
      "Episode:38, Validation Loss:4.83339453736941e-05,Acc:98.4000%,lr:0.001\n",
      "Episode:39, Validation Loss:6.0814417898654935e-05,Acc:97.7667%,lr:0.001\n",
      "Episode:40, Validation Loss:4.442230736215909e-05,Acc:98.6667%,lr:0.001\n",
      "Episode:41, Validation Loss:4.702569171786308e-05,Acc:98.7333%,lr:0.001\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Episode:42, Validation Loss:4.629422848423322e-05,Acc:98.3333%,lr:0.0001\n",
      "Episode:43, Validation Loss:4.2223855853080747e-05,Acc:98.7000%,lr:0.0001\n",
      "Episode:44, Validation Loss:4.0178563445806505e-05,Acc:98.8000%,lr:0.0001\n",
      "Episode:45, Validation Loss:3.6433830857276915e-05,Acc:98.8333%,lr:0.0001\n",
      "Episode:46, Validation Loss:3.5842491934696835e-05,Acc:98.8667%,lr:0.0001\n",
      "Episode:47, Validation Loss:3.5637656847635905e-05,Acc:98.9667%,lr:0.0001\n",
      "Episode:48, Validation Loss:3.520499418179194e-05,Acc:99.0000%,lr:0.0001\n",
      "Episode:49, Validation Loss:3.3912981549898784e-05,Acc:98.9000%,lr:0.0001\n",
      "Episode:50, Validation Loss:3.543676311771075e-05,Acc:98.8333%,lr:0.0001\n",
      "Episode:51, Validation Loss:3.856591507792473e-05,Acc:98.8000%,lr:0.0001\n",
      "Episode:52, Validation Loss:3.6912482231855395e-05,Acc:98.8333%,lr:0.0001\n",
      "Episode:53, Validation Loss:3.735192244251569e-05,Acc:98.8333%,lr:0.0001\n",
      "Episode:54, Validation Loss:3.809662039081256e-05,Acc:98.7667%,lr:0.0001\n",
      "Episode:55, Validation Loss:3.656553973754247e-05,Acc:98.8333%,lr:0.0001\n",
      "Episode:56, Validation Loss:3.994450842340787e-05,Acc:98.7000%,lr:0.0001\n",
      "Episode:57, Validation Loss:3.669275219241778e-05,Acc:98.8000%,lr:0.0001\n",
      "Episode:58, Validation Loss:3.83495216568311e-05,Acc:98.6667%,lr:0.0001\n",
      "Episode:59, Validation Loss:3.8036096841096875e-05,Acc:98.6667%,lr:0.0001\n",
      "Episode:60, Validation Loss:3.484021748105685e-05,Acc:98.8333%,lr:0.0001\n",
      "Episode:61, Validation Loss:3.558997189005216e-05,Acc:98.9000%,lr:0.0001\n",
      "Episode:62, Validation Loss:3.838082278768221e-05,Acc:98.7000%,lr:0.0001\n",
      "Episode:63, Validation Loss:3.853251288334529e-05,Acc:98.7667%,lr:0.0001\n",
      "Episode:64, Validation Loss:3.65827518204848e-05,Acc:98.8000%,lr:0.0001\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Episode:65, Validation Loss:3.402236476540565e-05,Acc:98.9333%,lr:1e-05\n",
      "Episode:66, Validation Loss:3.4179470191399256e-05,Acc:98.9333%,lr:1e-05\n",
      "Episode:67, Validation Loss:3.424560402830442e-05,Acc:98.9000%,lr:1e-05\n",
      "Episode:68, Validation Loss:3.472741569081942e-05,Acc:98.9000%,lr:1e-05\n",
      "Episode:69, Validation Loss:3.521702686945597e-05,Acc:98.9000%,lr:1e-05\n",
      "Episode:70, Validation Loss:3.547798966368039e-05,Acc:98.8333%,lr:1e-05\n",
      "Episode:71, Validation Loss:3.5652590294679006e-05,Acc:98.8667%,lr:1e-05\n",
      "Episode:72, Validation Loss:3.51831279695034e-05,Acc:98.9000%,lr:1e-05\n",
      "Episode:73, Validation Loss:3.466704611976941e-05,Acc:98.9333%,lr:1e-05\n",
      "Episode:74, Validation Loss:3.5322214166323346e-05,Acc:98.9000%,lr:1e-05\n",
      "Episode:75, Validation Loss:3.5331077873706816e-05,Acc:98.8333%,lr:1e-05\n",
      "Episode:76, Validation Loss:3.5368726899226504e-05,Acc:98.8667%,lr:1e-05\n",
      "Episode:77, Validation Loss:3.5823922604322434e-05,Acc:98.9000%,lr:1e-05\n",
      "Episode:78, Validation Loss:3.5872412224610646e-05,Acc:98.8667%,lr:1e-05\n",
      "Episode:79, Validation Loss:3.6209328720966976e-05,Acc:98.8667%,lr:1e-05\n",
      "Episode:80, Validation Loss:3.624334931373596e-05,Acc:98.8667%,lr:1e-05\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-06.\n",
      "===================Best Fold:0 Saved, Acc:0.99==================\n",
      "======================================================\n",
      "Episode:1, Validation Loss:0.0018393983840942383,Acc:10.0000%,lr:0.001\n",
      "Episode:2, Validation Loss:0.00224375057220459,Acc:11.0333%,lr:0.001\n",
      "Episode:3, Validation Loss:0.002462728261947632,Acc:18.4333%,lr:0.001\n",
      "Episode:4, Validation Loss:0.0029439180692036947,Acc:20.5000%,lr:0.001\n",
      "Episode:5, Validation Loss:0.0035673305193583173,Acc:11.0667%,lr:0.001\n",
      "Episode:6, Validation Loss:0.004744276841481526,Acc:11.0000%,lr:0.001\n",
      "Episode:7, Validation Loss:0.004729265054066976,Acc:12.4000%,lr:0.001\n",
      "Episode:8, Validation Loss:0.004569169998168946,Acc:11.4667%,lr:0.001\n",
      "Episode:9, Validation Loss:0.0039839266141255695,Acc:18.2667%,lr:0.001\n",
      "Episode:10, Validation Loss:0.0022364378770192463,Acc:35.4333%,lr:0.001\n",
      "Episode:11, Validation Loss:0.0013578515847524008,Acc:59.0000%,lr:0.001\n",
      "Episode:12, Validation Loss:0.0015195127328236897,Acc:54.3667%,lr:0.001\n",
      "Episode:13, Validation Loss:0.00012625335653622944,Acc:94.3000%,lr:0.001\n",
      "Episode:14, Validation Loss:0.00020293964445590974,Acc:90.6667%,lr:0.001\n",
      "Episode:15, Validation Loss:2.3512547835707664e-05,Acc:98.9000%,lr:0.001\n",
      "Episode:16, Validation Loss:9.101462612549464e-05,Acc:95.3667%,lr:0.001\n",
      "Episode:17, Validation Loss:7.025518516699473e-05,Acc:96.3000%,lr:0.001\n",
      "Episode:18, Validation Loss:1.7715086539586385e-05,Acc:99.3333%,lr:0.001\n",
      "Episode:19, Validation Loss:2.5086142122745515e-05,Acc:99.0333%,lr:0.001\n",
      "Episode:20, Validation Loss:2.000721109410127e-05,Acc:99.2333%,lr:0.001\n",
      "Episode:21, Validation Loss:9.039173057923714e-06,Acc:99.7333%,lr:0.001\n",
      "Episode:22, Validation Loss:1.3375874298314254e-05,Acc:99.5000%,lr:0.001\n",
      "Episode:23, Validation Loss:1.8622564151883126e-05,Acc:99.1000%,lr:0.001\n",
      "Episode:24, Validation Loss:4.986657574772835e-05,Acc:98.0333%,lr:0.001\n",
      "Episode:25, Validation Loss:1.4160941044489542e-05,Acc:99.2667%,lr:0.001\n",
      "Episode:26, Validation Loss:2.7852298070987065e-05,Acc:98.2667%,lr:0.001\n",
      "Episode:27, Validation Loss:1.0180007666349412e-05,Acc:99.6333%,lr:0.001\n",
      "Episode:28, Validation Loss:1.304961834102869e-05,Acc:99.3667%,lr:0.001\n",
      "Episode:29, Validation Loss:8.52982761959235e-06,Acc:99.6667%,lr:0.001\n",
      "Episode:30, Validation Loss:1.724813257654508e-05,Acc:99.4000%,lr:0.001\n",
      "Episode:31, Validation Loss:2.2388036673267684e-05,Acc:99.0667%,lr:0.001\n",
      "Episode:32, Validation Loss:1.4627027635773023e-05,Acc:99.4000%,lr:0.001\n",
      "Episode:33, Validation Loss:1.264769583940506e-05,Acc:99.4333%,lr:0.001\n",
      "Episode:34, Validation Loss:1.1888351912299793e-05,Acc:99.3667%,lr:0.001\n",
      "Episode:35, Validation Loss:7.862547722955544e-06,Acc:99.6333%,lr:0.001\n",
      "Episode:36, Validation Loss:8.964294412483771e-06,Acc:99.5667%,lr:0.001\n",
      "Episode:37, Validation Loss:2.4623986954490344e-05,Acc:98.9000%,lr:0.001\n",
      "Episode:38, Validation Loss:1.2744736547271411e-05,Acc:99.4000%,lr:0.001\n",
      "Episode:39, Validation Loss:1.2132377053300539e-05,Acc:99.4667%,lr:0.001\n",
      "Episode:40, Validation Loss:2.6064968357483545e-05,Acc:98.5667%,lr:0.001\n",
      "Episode:41, Validation Loss:1.044628427674373e-05,Acc:99.4667%,lr:0.001\n",
      "Episode:42, Validation Loss:8.50570946931839e-06,Acc:99.6000%,lr:0.001\n",
      "Episode:43, Validation Loss:1.1858269882698853e-05,Acc:99.4333%,lr:0.001\n",
      "Episode:44, Validation Loss:1.5791527926921846e-05,Acc:99.3333%,lr:0.001\n",
      "Episode:45, Validation Loss:7.456677500158549e-06,Acc:99.7000%,lr:0.001\n",
      "Episode:46, Validation Loss:1.4044835232198238e-05,Acc:99.4000%,lr:0.001\n",
      "Episode:47, Validation Loss:6.2749677648146945e-06,Acc:99.8333%,lr:0.001\n",
      "Episode:48, Validation Loss:1.1489330170055231e-05,Acc:99.5667%,lr:0.001\n",
      "Episode:49, Validation Loss:9.137537640829881e-06,Acc:99.7333%,lr:0.001\n",
      "Episode:50, Validation Loss:1.0063586135705312e-05,Acc:99.4667%,lr:0.001\n",
      "Episode:51, Validation Loss:8.460808855791887e-06,Acc:99.5667%,lr:0.001\n",
      "Episode:52, Validation Loss:1.3180995360016824e-05,Acc:99.4000%,lr:0.001\n",
      "Episode:53, Validation Loss:1.3438315751651922e-05,Acc:99.4000%,lr:0.001\n",
      "Episode:54, Validation Loss:1.0637597957005103e-05,Acc:99.5333%,lr:0.001\n",
      "Episode:55, Validation Loss:9.663402878989776e-06,Acc:99.6333%,lr:0.001\n",
      "Episode:56, Validation Loss:1.993404639263948e-05,Acc:98.8333%,lr:0.001\n",
      "Episode:57, Validation Loss:1.3790124716858069e-05,Acc:99.4333%,lr:0.001\n",
      "Episode:58, Validation Loss:1.652540856351455e-05,Acc:99.4667%,lr:0.001\n",
      "Episode:59, Validation Loss:8.420657832175493e-06,Acc:99.5667%,lr:0.001\n",
      "Episode:60, Validation Loss:1.0732152809699376e-05,Acc:99.4667%,lr:0.001\n",
      "Episode:61, Validation Loss:7.758492603898049e-06,Acc:99.4333%,lr:0.001\n",
      "Episode:62, Validation Loss:6.544183784474929e-06,Acc:99.7000%,lr:0.001\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Episode:63, Validation Loss:1.8409030511975287e-05,Acc:99.1000%,lr:0.0001\n",
      "Episode:64, Validation Loss:9.852285496890545e-06,Acc:99.6667%,lr:0.0001\n",
      "Episode:65, Validation Loss:8.613601637383302e-06,Acc:99.7333%,lr:0.0001\n",
      "Episode:66, Validation Loss:6.983610335737467e-06,Acc:99.7667%,lr:0.0001\n",
      "Episode:67, Validation Loss:6.822611360500256e-06,Acc:99.7667%,lr:0.0001\n",
      "Episode:68, Validation Loss:6.2093430509169895e-06,Acc:99.7333%,lr:0.0001\n",
      "Episode:69, Validation Loss:6.8900459446012976e-06,Acc:99.7000%,lr:0.0001\n",
      "Episode:70, Validation Loss:5.832780928661426e-06,Acc:99.7667%,lr:0.0001\n",
      "Episode:71, Validation Loss:6.806338516374429e-06,Acc:99.7333%,lr:0.0001\n",
      "Episode:72, Validation Loss:7.864164809385936e-06,Acc:99.7000%,lr:0.0001\n",
      "Episode:73, Validation Loss:8.359827101230622e-06,Acc:99.6667%,lr:0.0001\n",
      "Episode:74, Validation Loss:7.81364847595493e-06,Acc:99.7000%,lr:0.0001\n",
      "Episode:75, Validation Loss:7.661821165432532e-06,Acc:99.6667%,lr:0.0001\n",
      "Episode:76, Validation Loss:6.5420661121606825e-06,Acc:99.7333%,lr:0.0001\n",
      "Episode:77, Validation Loss:5.999192440261443e-06,Acc:99.8000%,lr:0.0001\n",
      "Episode:78, Validation Loss:6.782929878681898e-06,Acc:99.7333%,lr:0.0001\n",
      "Episode:79, Validation Loss:8.150645842154821e-06,Acc:99.6333%,lr:0.0001\n",
      "Episode:80, Validation Loss:7.48392924045523e-06,Acc:99.6667%,lr:0.0001\n",
      "Episode:81, Validation Loss:7.39861357336243e-06,Acc:99.7333%,lr:0.0001\n",
      "Episode:82, Validation Loss:7.1645937860012054e-06,Acc:99.7333%,lr:0.0001\n",
      "Episode:83, Validation Loss:7.034015376120806e-06,Acc:99.7333%,lr:0.0001\n",
      "Episode:84, Validation Loss:7.563045248389244e-06,Acc:99.7333%,lr:0.0001\n",
      "Episode:85, Validation Loss:6.536117289215326e-06,Acc:99.7333%,lr:0.0001\n",
      "Epoch    85: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Episode:86, Validation Loss:6.622182670980692e-06,Acc:99.6667%,lr:1e-05\n",
      "Episode:87, Validation Loss:6.2989920067290465e-06,Acc:99.7000%,lr:1e-05\n",
      "Episode:88, Validation Loss:6.201365807404121e-06,Acc:99.7333%,lr:1e-05\n",
      "Episode:89, Validation Loss:6.097002265353997e-06,Acc:99.7333%,lr:1e-05\n",
      "Episode:90, Validation Loss:6.038891151547432e-06,Acc:99.7333%,lr:1e-05\n",
      "Episode:91, Validation Loss:5.967129332323869e-06,Acc:99.7333%,lr:1e-05\n",
      "Episode:92, Validation Loss:6.01236599807938e-06,Acc:99.7333%,lr:1e-05\n",
      "Episode:93, Validation Loss:6.134841125458479e-06,Acc:99.7333%,lr:1e-05\n",
      "Episode:94, Validation Loss:6.05303297440211e-06,Acc:99.7333%,lr:1e-05\n",
      "Episode:95, Validation Loss:6.0495897196233275e-06,Acc:99.7333%,lr:1e-05\n",
      "Episode:96, Validation Loss:6.066972855478525e-06,Acc:99.7333%,lr:1e-05\n",
      "Episode:97, Validation Loss:5.9938454069197176e-06,Acc:99.7333%,lr:1e-05\n",
      "Episode:98, Validation Loss:6.037905346602201e-06,Acc:99.7333%,lr:1e-05\n",
      "Episode:99, Validation Loss:6.060600746423006e-06,Acc:99.7667%,lr:1e-05\n",
      "Episode:100, Validation Loss:5.941694602370262e-06,Acc:99.7667%,lr:1e-05\n",
      "Episode:101, Validation Loss:5.901018933703502e-06,Acc:99.7333%,lr:1e-05\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-06.\n",
      "===================Best Fold:1 Saved, Acc:0.9983333333333333==================\n",
      "======================================================\n",
      "Episode:1, Validation Loss:0.0016251684029897054,Acc:22.6667%,lr:0.001\n",
      "Episode:2, Validation Loss:0.0016315771738688151,Acc:26.4000%,lr:0.001\n",
      "Episode:3, Validation Loss:0.0023376705646514892,Acc:16.7667%,lr:0.001\n",
      "Episode:4, Validation Loss:0.003511980692545573,Acc:20.8000%,lr:0.001\n",
      "Episode:5, Validation Loss:0.004951004505157471,Acc:15.6667%,lr:0.001\n",
      "Episode:6, Validation Loss:0.005175262610117594,Acc:16.8333%,lr:0.001\n",
      "Episode:7, Validation Loss:0.003930915673573812,Acc:21.8333%,lr:0.001\n",
      "Episode:8, Validation Loss:0.002152047316233317,Acc:43.9000%,lr:0.001\n",
      "Episode:9, Validation Loss:0.001973103364308675,Acc:45.5667%,lr:0.001\n",
      "Episode:10, Validation Loss:0.0016045920848846435,Acc:52.6000%,lr:0.001\n",
      "Episode:11, Validation Loss:0.0006952033042907714,Acc:71.8000%,lr:0.001\n",
      "Episode:12, Validation Loss:0.0004878893196582794,Acc:83.3667%,lr:0.001\n",
      "Episode:13, Validation Loss:4.8358742147684094e-05,Acc:98.0667%,lr:0.001\n",
      "Episode:14, Validation Loss:8.233437438805898e-05,Acc:95.9333%,lr:0.001\n",
      "Episode:15, Validation Loss:3.759292016426722e-05,Acc:97.9667%,lr:0.001\n",
      "Episode:16, Validation Loss:1.5269227946798006e-05,Acc:99.4333%,lr:0.001\n",
      "Episode:17, Validation Loss:7.961958025892575e-06,Acc:99.7333%,lr:0.001\n",
      "Episode:18, Validation Loss:6.873591337352991e-06,Acc:99.6333%,lr:0.001\n",
      "Episode:19, Validation Loss:9.270159217218558e-06,Acc:99.6667%,lr:0.001\n",
      "Episode:20, Validation Loss:2.5338177879651388e-05,Acc:98.8333%,lr:0.001\n",
      "Episode:21, Validation Loss:1.2355883916219076e-05,Acc:99.3667%,lr:0.001\n",
      "Episode:22, Validation Loss:1.360733744998773e-05,Acc:99.4667%,lr:0.001\n",
      "Episode:23, Validation Loss:1.2037322856485844e-05,Acc:99.4333%,lr:0.001\n",
      "Episode:24, Validation Loss:1.5270099975168705e-05,Acc:99.4333%,lr:0.001\n",
      "Episode:25, Validation Loss:1.5363766190906366e-05,Acc:99.3333%,lr:0.001\n",
      "Episode:26, Validation Loss:9.523235261440276e-06,Acc:99.6000%,lr:0.001\n",
      "Episode:27, Validation Loss:1.1526501116653284e-05,Acc:99.4333%,lr:0.001\n",
      "Episode:28, Validation Loss:9.958035623033842e-06,Acc:99.5333%,lr:0.001\n",
      "Episode:29, Validation Loss:1.0116256773471832e-05,Acc:99.5667%,lr:0.001\n",
      "Episode:30, Validation Loss:9.09851398319006e-06,Acc:99.6333%,lr:0.001\n",
      "Episode:31, Validation Loss:1.3048368816574414e-05,Acc:99.5000%,lr:0.001\n",
      "Episode:32, Validation Loss:1.2110399082303046e-05,Acc:99.5333%,lr:0.001\n",
      "Episode:33, Validation Loss:1.0848751291632652e-05,Acc:99.4000%,lr:0.001\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Episode:34, Validation Loss:8.277253247797489e-06,Acc:99.6333%,lr:0.0001\n",
      "Episode:35, Validation Loss:7.5108828023076056e-06,Acc:99.6000%,lr:0.0001\n",
      "Episode:36, Validation Loss:6.996157889564832e-06,Acc:99.6000%,lr:0.0001\n",
      "Episode:37, Validation Loss:7.852906671663126e-06,Acc:99.6000%,lr:0.0001\n",
      "Episode:38, Validation Loss:8.321750598649184e-06,Acc:99.6000%,lr:0.0001\n",
      "Episode:39, Validation Loss:8.25246951232354e-06,Acc:99.6000%,lr:0.0001\n",
      "Episode:40, Validation Loss:7.952763698995114e-06,Acc:99.6000%,lr:0.0001\n",
      "Episode:41, Validation Loss:7.996642030775547e-06,Acc:99.6333%,lr:0.0001\n",
      "Episode:42, Validation Loss:8.052270859479904e-06,Acc:99.6000%,lr:0.0001\n",
      "Episode:43, Validation Loss:7.359906410177549e-06,Acc:99.6333%,lr:0.0001\n",
      "Episode:44, Validation Loss:8.228328078985215e-06,Acc:99.6667%,lr:0.0001\n",
      "Episode:45, Validation Loss:7.212980029483636e-06,Acc:99.6667%,lr:0.0001\n",
      "Episode:46, Validation Loss:8.04884017755588e-06,Acc:99.6000%,lr:0.0001\n",
      "Episode:47, Validation Loss:8.064315343896548e-06,Acc:99.6667%,lr:0.0001\n",
      "Episode:48, Validation Loss:7.895883172750472e-06,Acc:99.7333%,lr:0.0001\n",
      "Episode:49, Validation Loss:7.468096911907196e-06,Acc:99.6667%,lr:0.0001\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Episode:50, Validation Loss:7.103295996785164e-06,Acc:99.7667%,lr:1e-05\n",
      "Episode:51, Validation Loss:7.075184335311254e-06,Acc:99.7667%,lr:1e-05\n",
      "Episode:52, Validation Loss:7.152078983684381e-06,Acc:99.7667%,lr:1e-05\n",
      "Episode:53, Validation Loss:7.145517195264498e-06,Acc:99.7667%,lr:1e-05\n",
      "Episode:54, Validation Loss:7.176531789203485e-06,Acc:99.7667%,lr:1e-05\n",
      "Episode:55, Validation Loss:7.162515074014664e-06,Acc:99.7333%,lr:1e-05\n",
      "Episode:56, Validation Loss:7.341773249208927e-06,Acc:99.7333%,lr:1e-05\n",
      "Episode:57, Validation Loss:7.2138470908006035e-06,Acc:99.7667%,lr:1e-05\n",
      "Episode:58, Validation Loss:7.167082900802294e-06,Acc:99.7667%,lr:1e-05\n",
      "Episode:59, Validation Loss:7.207066131134828e-06,Acc:99.7667%,lr:1e-05\n",
      "Episode:60, Validation Loss:7.289136139055093e-06,Acc:99.7667%,lr:1e-05\n",
      "Episode:61, Validation Loss:7.240467394391696e-06,Acc:99.7333%,lr:1e-05\n",
      "Episode:62, Validation Loss:7.2412847851713495e-06,Acc:99.7333%,lr:1e-05\n",
      "Episode:63, Validation Loss:7.335341845949491e-06,Acc:99.7000%,lr:1e-05\n",
      "Episode:64, Validation Loss:7.496236202617486e-06,Acc:99.7000%,lr:1e-05\n",
      "Episode:65, Validation Loss:7.67741600672404e-06,Acc:99.7000%,lr:1e-05\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-06.\n",
      "===================Best Fold:2 Saved, Acc:0.9976666666666667==================\n",
      "======================================================\n",
      "Episode:1, Validation Loss:0.0015617231527964273,Acc:10.0000%,lr:0.001\n",
      "Episode:2, Validation Loss:0.0017129224141438802,Acc:24.3000%,lr:0.001\n",
      "Episode:3, Validation Loss:0.003007134437561035,Acc:16.8333%,lr:0.001\n",
      "Episode:4, Validation Loss:0.004173659165700277,Acc:19.9000%,lr:0.001\n",
      "Episode:5, Validation Loss:0.004744774341583252,Acc:19.9333%,lr:0.001\n",
      "Episode:6, Validation Loss:0.004045472939809163,Acc:18.5000%,lr:0.001\n",
      "Episode:7, Validation Loss:0.003765929063161214,Acc:19.7667%,lr:0.001\n",
      "Episode:8, Validation Loss:0.0036843775113423664,Acc:19.4000%,lr:0.001\n",
      "Episode:9, Validation Loss:0.0030823040008544924,Acc:32.8333%,lr:0.001\n",
      "Episode:10, Validation Loss:0.002909441868464152,Acc:32.6000%,lr:0.001\n",
      "Episode:11, Validation Loss:0.0001987962673107783,Acc:90.4667%,lr:0.001\n",
      "Episode:12, Validation Loss:0.00025888791680335997,Acc:88.4667%,lr:0.001\n",
      "Episode:13, Validation Loss:3.615383555491765e-05,Acc:98.2667%,lr:0.001\n",
      "Episode:14, Validation Loss:1.643823025127252e-05,Acc:99.0667%,lr:0.001\n",
      "Episode:15, Validation Loss:1.479410007596016e-05,Acc:99.2333%,lr:0.001\n",
      "Episode:16, Validation Loss:1.104045348862807e-05,Acc:99.4000%,lr:0.001\n",
      "Episode:17, Validation Loss:1.090530026704073e-05,Acc:99.5333%,lr:0.001\n",
      "Episode:18, Validation Loss:1.3079731104274592e-05,Acc:99.5667%,lr:0.001\n",
      "Episode:19, Validation Loss:1.684082547823588e-05,Acc:99.2333%,lr:0.001\n",
      "Episode:20, Validation Loss:1.7446740840872127e-05,Acc:99.3333%,lr:0.001\n",
      "Episode:21, Validation Loss:1.498946671684583e-05,Acc:99.2333%,lr:0.001\n",
      "Episode:22, Validation Loss:1.2192398309707641e-05,Acc:99.3667%,lr:0.001\n",
      "Episode:23, Validation Loss:3.120677173137665e-05,Acc:98.8333%,lr:0.001\n",
      "Episode:24, Validation Loss:1.1560280496875445e-05,Acc:99.3333%,lr:0.001\n",
      "Episode:25, Validation Loss:1.7929908509055774e-05,Acc:99.2333%,lr:0.001\n",
      "Episode:26, Validation Loss:1.804166194051504e-05,Acc:99.2000%,lr:0.001\n",
      "Episode:27, Validation Loss:1.3768113528688748e-05,Acc:99.2667%,lr:0.001\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    epochs = 200\n",
    "    period = 40\n",
    "    ensemble_models = []\n",
    "    lr = 1e-3\n",
    "    val_period = 1\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "#     criterion_b = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    print(\"Fold:\",len(train_loaders))\n",
    "    \n",
    "    for fold in range(len(train_loaders)):\n",
    "        train_loader = train_loaders[fold]\n",
    "        val_loader = val_loaders[fold]\n",
    "        \n",
    "        model = get_model()\n",
    "            \n",
    "        max_acc = 0\n",
    "        min_loss = 10000\n",
    "        best_model_dict = None\n",
    "        data_num = 0\n",
    "        loss_avg = 0\n",
    "\n",
    "#         optimizer = torch.optim.SGD(model.parameters(),lr=lr)\n",
    "#         optimizer = torch.optim.RMSprop(model.parameters(),lr=lr)\n",
    "        optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "#         optimizer = torch.optim.Adagrad(model.parameters(),lr=lr)\n",
    "#         lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=period,T_mult=1,eta_min=1e-5) #original \n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, patience=15)\n",
    "        \n",
    "        for ep in range(0,epochs+1):\n",
    "            model.train()\n",
    "            for idx, data in enumerate(train_loader):\n",
    "                img, target = data\n",
    "                img, target = img.to(device), target.to(device,dtype=torch.long)\n",
    "                \n",
    "#                 print(np.shape(img),np.shape(target_b),np.shape(target)) #Tensor(4,1,28,28), Tensor(4)\n",
    "#                 print(np.max(img.cpu().numpy()),np.min(img.cpu().numpy())) #1.0 0.0\n",
    "                pred = model(img)\n",
    "#                 print(pred.size())   #(32,10)\n",
    "#                 print(target.size()) #(32,)\n",
    "                  \n",
    "                ###Input shape: input:(batch_num,1), target:(batch_num,a int 0 or 1) for CSE LOSS, target:(batch_num,1) for BCE loss\n",
    "#                 loss_b = criterion_b(pred_b,target_b) \n",
    "\n",
    "                ###Input shape: input:(batch_num,10), target:(batch_num,a int between 0~10)\n",
    "                loss = criterion(pred,target)\n",
    "                \n",
    "                loss_avg += loss.item()\n",
    "                data_num += img.size(0)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            ###Cosine annealing\n",
    "#             lr_scheduler.step()\n",
    "\n",
    "            ###Evaluate Train Loss \n",
    "#             if ep%2 == 0:\n",
    "#                 loss_avg /= data_num\n",
    "#                 print(\"Ep:{}, loss:{}, lr:{}\".format(ep, loss_avg,optimizer.param_groups[0]['lr']))\n",
    "#                 loss_avg = 0\n",
    "#                 data_num = 0\n",
    "\n",
    "            ###Validation\n",
    "            if ep!=0 and ep%val_period == 0:\n",
    "                model.eval()\n",
    "                acc = 0\n",
    "                val_loss = 0\n",
    "                data_num  = 0\n",
    "                with torch.no_grad():\n",
    "                    for idx, data in enumerate(val_loader):\n",
    "                        img, target = data\n",
    "                        img, target = img.to(device), target.to(device,dtype=torch.long)\n",
    "                        pred = model(img)\n",
    "\n",
    "                        val_loss += criterion(pred, target).item()\n",
    "                        \n",
    "                        # print(pred) \n",
    "                        _,pred_class = torch.max(pred.data, 1)\n",
    "    #                     print(pred_class)\n",
    "                        acc += (pred_class == target).sum().item()\n",
    "                        data_num += img.size(0)\n",
    "\n",
    "                acc /= data_num\n",
    "                val_loss /= data_num\n",
    "\n",
    "                ###Plateau\n",
    "                lr_scheduler.step(val_loss)\n",
    "                if optimizer.param_groups[0]['lr'] < 1e-5:\n",
    "                    break                    \n",
    "\n",
    "                if acc >= max_acc:\n",
    "                    max_acc = acc\n",
    "                    best_model_dict = model.state_dict()\n",
    "                \n",
    "                if val_loss <= min_loss:\n",
    "                    min_loss = val_loss\n",
    "#                     best_model_dict = model.state_dict()\n",
    "                \n",
    "                print(\"Episode:{}, Validation Loss:{},Acc:{:.4f}%,lr:{}\"\n",
    "                      .format(ep,val_loss,acc*100,optimizer.param_groups[0]['lr']))\n",
    "            \n",
    "#             if max_acc>0.995 and ep!=0 and ep%10 == 0:\n",
    "#                 torch.save(best_model_dict, \"./Kmnist_saved_model/tmp_Fold{}_acc{:.4f}\".format(fold,max_acc*1e2))\n",
    "    \n",
    "        ###K-Fold ensemble: Saved k best model for k dataloader\n",
    "        print(\"===================Best Fold:{} Saved, Acc:{}==================\".format(fold,max_acc))\n",
    "        torch.save(best_model_dict, \"./Kmnist_saved_model/batch2048_Fold{}_loss{:.4f}_acc{:.3f}\".format(fold,min_loss*1e3,max_acc*1e2))\n",
    "        print(\"======================================================\")\n",
    "        \n",
    "        del model\n",
    "            \n",
    "            ###Snapshot ensemble: saved model\n",
    "#             if ep!=0 and ep%period == 0:\n",
    "# #                 ensemble_models.append(best_model_dict)\n",
    "#                 model_id = ep//period\n",
    "#                 print(\"===================Best Model{} Saved, Acc:{}==================\".format(model_id,max_acc))\n",
    "#                 torch.save(best_model_dict, \"./Kmnist_saved_model/model{}_ep{}_acc{:.4f}\".format(model_id,ep,max_acc))\n",
    "#                 print(\"======================================================\")\n",
    "#                 max_acc = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Native classifier Emsemble inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms.Normalize(mean=[0.08229437],std=[0.23876116]) #train dataset dist\n",
    "# transforms.Normalize(mean=[0.09549136],std=[0.24336776]) #dig_augmented distribution        \n",
    "# transforms.Normalize(mean=[0.08889289],std=[0.24106446])  #train_large dataset distribution\n",
    "\n",
    "\n",
    "ensemble_root = \"./Kmnist_saved_model/emsemble/20_fold_simple_cnn_ensemble\"   #model-> 1 fc(512) + dropout(0.1)\n",
    "ensemble_models = []\n",
    "\n",
    "ensemble_root_dig = \"./Kmnist_saved_model/emsemble/dig_aug_ensemble\"    #model-> 1 fc(256) + dropout(0.2)\n",
    "ensemble_models_dig = []\n",
    "\n",
    "data_num = 0\n",
    "acc = 0\n",
    "\n",
    "mean,std = 0.08229437, 0.23876116\n",
    "mean_dig,std_dig = 0.09549136, 0.24336776\n",
    "mean_large,std_large = 0.08889289, 0.24106446\n",
    "\n",
    "vr = 1\n",
    "# indices = np.arange(60000)\n",
    "# test_dataset = KMnistDataset(data_len=None,is_validate=True, validate_rate=vr,indices=indices)\n",
    "indices = np.arange(120000)\n",
    "test_dataset = KMnistDataset_binary_aid(data_len=None,is_validate=True, validate_rate=vr,indices=indices)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=12)\n",
    "\n",
    "native_model = convNet_native(in_channels=1)\n",
    "native_model.cuda()\n",
    "native_model.load_state_dict(torch.load(\"./Kmnist_saved_model/emsemble/native_classifier/Fold0_loss0.0242_acc_b99.704_without_aug\"))\n",
    "native_model.eval()   \n",
    "\n",
    "\n",
    "for file_name in os.listdir(ensemble_root):\n",
    "    if file_name[4:6] != \"18\":\n",
    "        continue\n",
    "    model = convNet_tmp(in_channels=1)\n",
    "    model.cuda()\n",
    "    model.load_state_dict(torch.load(\"{}/{}\".format(ensemble_root,file_name)))\n",
    "    model.eval()\n",
    "    ensemble_models.append(model)\n",
    "\n",
    "for file_name in os.listdir(ensemble_root_dig):\n",
    "    if file_name[4] != \"0\":\n",
    "        continue\n",
    "    model = convNet(in_channels=1)\n",
    "    model.cuda()\n",
    "    model.load_state_dict(torch.load(\"{}/{}\".format(ensemble_root_dig,file_name)))\n",
    "    model.eval()\n",
    "    ensemble_models_dig.append(model)\n",
    "    \n",
    "### Test Native Classifier\n",
    "with torch.no_grad():\n",
    "    for idx,data in enumerate(test_loader):\n",
    "        ###Classify native or not\n",
    "        img, target_b, target = data\n",
    "        img, target = img.to(device), target.to(device,dtype=torch.long)\n",
    "        _,pred_native = torch.max(native_model(img),dim=1)  #(batch_num,)\n",
    "        \n",
    "        norm_img = (img-mean_large)/std_large\n",
    "        ###Classify by normal model, Average Ensemble\n",
    "        pred_list = torch.Tensor([]).to(device)\n",
    "        model_num = len(ensemble_models)\n",
    "        for i in range(model_num):\n",
    "            pred = ensemble_models[i](norm_img) #(batch_num,10)\n",
    "            pred_list = torch.cat((pred_list,pred.unsqueeze(2)),dim=2) #pred_list: (batch_num,10,model_num)\n",
    "        pred = torch.mean(pred_list,dim=2)   #(batch,10)\n",
    "        _,pred_class = torch.max(pred.data, 1)   #(batch_num,)\n",
    "        \n",
    "        \n",
    "        norm_img = (img-mean_large)/std_large\n",
    "        ###Classify by dig_aug_model, Average Ensemble\n",
    "        pred_list = torch.Tensor([]).to(device)\n",
    "        model_num = len(ensemble_models_dig)\n",
    "        for i in range(model_num):\n",
    "            pred = ensemble_models_dig[i](norm_img) #(batch_num,10)\n",
    "            pred_list = torch.cat((pred_list,pred.unsqueeze(2)),dim=2) #pred_list: (batch_num,10,model_num)\n",
    "        pred = torch.mean(pred_list,dim=2)   #(batch,10)\n",
    "        _,pred_class_dig = torch.max(pred.data, 1)   #(batch_num,)\n",
    "        \n",
    "        ###Make final result tensor\n",
    "        native_mask = pred_native    #(batch_num,)  ex: ([1,0,0,1,0])\n",
    "        nonnative_mask = torch.ones([img.size(0),], dtype=torch.long).to(device) - native_mask  #(batch_num,) ex:([0,1,1,0,1])\n",
    "        \n",
    "        r1 = (pred_class*native_mask)  #a*b = torch.mul(a,b)\n",
    "        r2 = (pred_class_dig*nonnative_mask)\n",
    "        pred_final = (r1+r2).to(torch.long)  #(batch_num,)\n",
    "\n",
    "#         print(\"model1:\",pred_class)\n",
    "#         print(\"model2:\",pred_class_dig)\n",
    "#         print(\"mask:\",native_mask)\n",
    "#         print(\"non_mask:\",nonnative_mask)\n",
    "#         print(\"r1:\",r1)\n",
    "#         print(\"r2:\",r2)\n",
    "#         print(\"result:\",result)\n",
    "#         print(\"target:\",target)\n",
    "#         stop\n",
    "    \n",
    "#         acc += (pred_native).sum().item()\n",
    "        acc += (pred_final == target).sum().item()\n",
    "        data_num += img.size(0)\n",
    "\n",
    "#     val_loss /= data_num\n",
    "    acc /= data_num\n",
    "    print(\"Acc:{:.4f}%\".format(acc*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_root = \"/home/ccchang/localization_net/Kmnist_saved_model/emsemble/5_fold_ep80_lr1e-2\"\n",
    "ensemble_models = []\n",
    "epochs = 500\n",
    "period = 100\n",
    "model_num = epochs//period\n",
    "model = 5\n",
    "data_num = 0\n",
    "acc = 0\n",
    "\n",
    "for file_name in os.listdir(ensemble_root):\n",
    "    model = convNet(in_channels=1)\n",
    "    model.cuda()\n",
    "    model.load_state_dict(torch.load(\"{}/{}\".format(ensemble_root,file_name)))\n",
    "    model.eval()\n",
    "    ensemble_models.append(model)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, data in enumerate(validate_loader):\n",
    "        img, target = data\n",
    "        img, target = img.to(device), target.to(device,dtype=torch.long)\n",
    "\n",
    "        ###Single model\n",
    "#         pred = model(img)\n",
    "#         _,pred_class = torch.max(pred.data, 1)\n",
    "        \n",
    "        ###Average Ensemble\n",
    "#         pred_list = torch.Tensor([]).to(device)\n",
    "#         for i in range(model_num):\n",
    "#             pred = ensemble_models[i](img) #(batch_num,10)\n",
    "#             pred_list = torch.cat((pred_list,pred.unsqueeze(2)),dim=2) #pred_list: (batch_num,10,model_num)\n",
    "#         pred = torch.mean(pred_list,dim=2)   #(batch,10)\n",
    "        \n",
    "#         _,pred_class = torch.max(pred.data, 1)   #(batch_num,)\n",
    "#         val_loss += criterion(pred, target)\n",
    "\n",
    "        ###Voting Ensemble\n",
    "        pred_list = torch.LongTensor([]).to(device)\n",
    "        for i in range(model_num):\n",
    "            pred = ensemble_models[i](img) #(batch_num,10)\n",
    "            _,pred_class = torch.max(pred.data, 1)   #(batch_num,)\n",
    "            pred_list = torch.cat((pred_list,pred_class.unsqueeze(1)),dim=1)\n",
    "            \n",
    "        pred_class_list = torch.LongTensor([]).to(device)\n",
    "        for i in range(img.size(0)):\n",
    "            pred_np = pred_list[i].cpu().numpy()\n",
    "            unique_class,count = np.unique(pred_np,return_counts=True)\n",
    "            unique_class = np.array(unique_class[np.argmax(count)]).reshape(-1)   #unique class shape(1,)\n",
    "            class_voted= torch.from_numpy(unique_class).to(device)    #(1,)\n",
    "            pred_class_list = torch.cat((pred_class_list,class_voted))    \n",
    "    \n",
    "#         acc += (pred_class == target).sum().item()\n",
    "        acc += (pred_class_list == target).sum().item()\n",
    "        data_num += img.size(0)\n",
    "\n",
    "#     val_loss /= data_num\n",
    "    acc /= data_num\n",
    "    print(\"Acc:{:.4f}%\".format(acc*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.data = global_dataimport torch\n",
    "import numpy as np\n",
    "\n",
    "t1 = torch.Tensor([[1,2,3,4],[4,3,2,1],[1,5,3,3]])  #(3,4)\n",
    "t1 = t1.unsqueeze(2)\n",
    "\n",
    "t_list = torch.Tensor([])\n",
    "\n",
    "for i in range(3):\n",
    "    t_list = torch.cat((t_list,t1),dim=2)\n",
    "\n",
    "print(t_list.size())\n",
    "print(t_list)\n",
    "t_list = torch.mean(t_list,dim=2)\n",
    "print(t_list.size())\n",
    "print(t_list)\n",
    "\n",
    "\n",
    "# n1 = t1.cpu().numpy()\n",
    "\n",
    "# n1, count = np.unique(n1,return_counts=True,axis=0)\n",
    "# print(count)\n",
    "# n1 = np.argmax(count)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
