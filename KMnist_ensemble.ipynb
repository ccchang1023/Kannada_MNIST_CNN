{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torchsummary import summary\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Squeeze and Excitation Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Seq_Ex_Block(nn.Module):\n",
    "    def __init__(self, in_ch, r=16):\n",
    "        super(Seq_Ex_Block, self).__init__()\n",
    "        self.se = nn.Sequential(\n",
    "            GlobalAvgPool(),\n",
    "            nn.Linear(in_ch, in_ch//r),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_ch//r, in_ch),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        se_weight = self.se(x).unsqueeze(-1).unsqueeze(-1)\n",
    "#         print(f'x:{x.sum()}, x_se:{x.mul(se_weight).sum()}')\n",
    "        return x.mul(se_weight)\n",
    "\n",
    "class GlobalAvgPool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalAvgPool, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x.view(*(x.shape[:-2]),-1).mean(-1)\n",
    "\n",
    "class SE_Net(nn.Module):\n",
    "    def __init__(self,in_channels):\n",
    "        super(SE_Net,self).__init__()\n",
    "        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, \n",
    "        #                dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "        self.c1 = nn.Conv2d(in_channels=in_channels, out_channels=64,kernel_size=3,stride=1,padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=64,eps=1e-3,momentum=0.01)\n",
    "        self.c2 = nn.Conv2d(64,64,3,1,0)\n",
    "        self.bn2 = nn.BatchNorm2d(64,1e-3,0.01)\n",
    "        self.c3 = nn.Conv2d(64,64,5,1,2)\n",
    "        self.bn3 = nn.BatchNorm2d(64,1e-3,0.01)\n",
    "        self.m1 = nn.MaxPool2d(2)\n",
    "        self.d1 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.c4 = nn.Conv2d(64,128,3,1,0)\n",
    "        self.bn4 = nn.BatchNorm2d(128,1e-3,0.01)\n",
    "        self.c5 = nn.Conv2d(128,128,3,1,0)\n",
    "        self.bn5 = nn.BatchNorm2d(128,1e-3,0.01)\n",
    "        self.c6 = nn.Conv2d(128,128,5,1,2)\n",
    "        self.bn6 = nn.BatchNorm2d(128,1e-3,0.01)        \n",
    "        self.m2 = nn.MaxPool2d(2)\n",
    "        self.d2 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.c7 = nn.Conv2d(128,256,3,1,0)\n",
    "        self.bn7 = nn.BatchNorm2d(256,1e-3,0.01)\n",
    "        self.se1 = Seq_Ex_Block(in_ch=256,r=16)\n",
    "        self.d3 = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc1 = nn.Linear(256*2*2,256)\n",
    "        self.bn8 = nn.BatchNorm1d(256,1e-3,0.01)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256,128)\n",
    "        self.bn9 = nn.BatchNorm1d(128,1e-3,0.01)\n",
    "        \n",
    "        self.out = nn.Linear(128,10)\n",
    "        \n",
    "        self.init_linear_weights()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.bn1(F.relu(self.c1(x)))\n",
    "        x = self.bn2(F.relu(self.c2(x)))\n",
    "        x = self.bn3(F.relu(self.c3(x)))\n",
    "        x = self.m1(x)\n",
    "        x = self.d1(x)\n",
    "        \n",
    "        x = self.bn4(F.relu(self.c4(x)))\n",
    "        x = self.bn5(F.relu(self.c5(x)))\n",
    "        x = self.bn6(F.relu(self.c6(x)))\n",
    "        x = self.m2(x)\n",
    "        x = self.d2(x)\n",
    "        \n",
    "        x = self.bn7(F.relu(self.c7(x)))\n",
    "        x = self.se1(x)\n",
    "        \n",
    "        x = self.d3(x)        \n",
    "        \n",
    "        x = x.view(-1, 256*2*2) #reshape\n",
    "        \n",
    "        x = self.bn8(self.fc1(x))\n",
    "        x = self.bn9(self.fc2(x))\n",
    "        \n",
    "        return self.out(x)\n",
    "    \n",
    "    def init_linear_weights(self):\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in')  #default mode: fan_in\n",
    "        nn.init.kaiming_normal_(self.fc2.weight, mode='fan_in')\n",
    "        nn.init.kaiming_normal_(self.out.weight, mode='fan_in')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class convNet(nn.Module):\n",
    "    def __init__(self,in_channels):\n",
    "        super(convNet,self).__init__()\n",
    "        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, \n",
    "        #                dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "        self.c1 = nn.Conv2d(in_channels=in_channels, out_channels=64,kernel_size=3,stride=1,padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=64,eps=1e-3,momentum=0.01)\n",
    "        self.c2 = nn.Conv2d(64,64,3,1,0)\n",
    "        self.bn2 = nn.BatchNorm2d(64,1e-3,0.01)\n",
    "        self.c3 = nn.Conv2d(64,64,5,1,2)\n",
    "        self.bn3 = nn.BatchNorm2d(64,1e-3,0.01)\n",
    "        self.m1 = nn.MaxPool2d(2)\n",
    "        self.d1 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.c4 = nn.Conv2d(64,128,3,1,0)\n",
    "        self.bn4 = nn.BatchNorm2d(128,1e-3,0.01)\n",
    "        self.c5 = nn.Conv2d(128,128,3,1,0)\n",
    "        self.bn5 = nn.BatchNorm2d(128,1e-3,0.01)\n",
    "        self.c6 = nn.Conv2d(128,128,5,1,2)\n",
    "        self.bn6 = nn.BatchNorm2d(128,1e-3,0.01)        \n",
    "        self.m2 = nn.MaxPool2d(2)\n",
    "        self.d2 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.c7 = nn.Conv2d(128,256,3,1,0)\n",
    "        self.bn7 = nn.BatchNorm2d(256,1e-3,0.01)\n",
    "        self.m3 = nn.MaxPool2d(2)\n",
    "        self.d3 = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc1 = nn.Linear(256*1*1,256)\n",
    "        self.bn8 = nn.BatchNorm1d(256,1e-3,0.01)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256,128)\n",
    "        self.bn9 = nn.BatchNorm1d(128,1e-3,0.01)\n",
    "        \n",
    "        self.out = nn.Linear(128,10)\n",
    "        \n",
    "        self.init_linear_weights()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.bn1(F.leaky_relu(self.c1(x)),0.1)\n",
    "        x = self.bn2(F.relu(self.c2(x)))\n",
    "        x = self.bn3(F.relu(self.c3(x)))\n",
    "        x = self.m1(x)\n",
    "        x = self.d1(x)\n",
    "        \n",
    "        x = self.bn4(F.relu(self.c4(x)))\n",
    "        x = self.bn5(F.relu(self.c5(x)))\n",
    "        x = self.bn6(F.relu(self.c6(x)))\n",
    "        x = self.m2(x)\n",
    "        x = self.d2(x)\n",
    "        \n",
    "        x = self.bn7(F.relu(self.c7(x)))\n",
    "        x = self.m3(x)\n",
    "        x = self.d3(x)        \n",
    "        \n",
    "        x = x.view(-1, 256*1*1) #reshape\n",
    "        \n",
    "        x = self.bn8(self.fc1(x))\n",
    "        x = self.bn9(self.fc2(x))\n",
    "        \n",
    "        return self.out(x)\n",
    "    \n",
    "    def init_linear_weights(self):\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in')  #default mode: fan_in\n",
    "        nn.init.kaiming_normal_(self.fc2.weight, mode='fan_in')\n",
    "        nn.init.kaiming_normal_(self.out.weight, mode='fan_in')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv model by Chris Deotte (replace max pooling with average pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class convNet_avp(nn.Module):\n",
    "    def __init__(self,in_channels):\n",
    "        super(convNet_avp,self).__init__()\n",
    "        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, \n",
    "        #                dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "        self.c1 = nn.Conv2d(in_channels=in_channels, out_channels=64,kernel_size=3,stride=1,padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=64,eps=1e-3,momentum=0.01)\n",
    "        self.c2 = nn.Conv2d(64,64,3,1,0)\n",
    "        self.bn2 = nn.BatchNorm2d(64,1e-3,0.01)\n",
    "        self.c3 = nn.Conv2d(64,64,5,2,2)  #Use strides 2 instead of maxpooling\n",
    "        self.bn3 = nn.BatchNorm2d(64,1e-3,0.01)\n",
    "        self.d1 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.c4 = nn.Conv2d(64,128,3,1,0)\n",
    "        self.bn4 = nn.BatchNorm2d(128,1e-3,0.01)\n",
    "        self.c5 = nn.Conv2d(128,128,3,1,0)\n",
    "        self.bn5 = nn.BatchNorm2d(128,1e-3,0.01)\n",
    "        self.c6 = nn.Conv2d(128,128,5,2,2)\n",
    "        self.bn6 = nn.BatchNorm2d(128,1e-3,0.01)        \n",
    "        self.d2 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.c7 = nn.Conv2d(128,256,4,1,0)\n",
    "        self.bn7 = nn.BatchNorm2d(256,1e-3,0.01)\n",
    "        self.d3 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(256*1*1,256)\n",
    "        self.bn8 = nn.BatchNorm1d(256,1e-3,0.01)\n",
    "        \n",
    "        self.out = nn.Linear(256,10)\n",
    "#         self.init_linear_weights()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.bn1(F.relu(self.c1(x)))\n",
    "        x = self.bn2(F.relu(self.c2(x)))\n",
    "        x = self.bn3(F.relu(self.c3(x)))\n",
    "        x = self.d1(x)\n",
    "        \n",
    "        x = self.bn4(F.relu(self.c4(x)))\n",
    "        x = self.bn5(F.relu(self.c5(x)))\n",
    "        x = self.bn6(F.relu(self.c6(x)))\n",
    "        x = self.d2(x)\n",
    "        \n",
    "        x = self.bn7(F.relu(self.c7(x)))\n",
    "        x = self.d3(x)\n",
    "\n",
    "        x = x.view(-1, 256*1*1) #reshape\n",
    "        x = self.bn8(self.fc1(x))\n",
    "        return self.out(x)\n",
    "    \n",
    "#     def init_linear_weights(self):\n",
    "#         nn.init.kaiming_normal_(self.out.weight, mode='fan_in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 26, 26]             640\n",
      "       BatchNorm2d-2           [-1, 64, 26, 26]             128\n",
      "            Conv2d-3           [-1, 64, 24, 24]          36,928\n",
      "       BatchNorm2d-4           [-1, 64, 24, 24]             128\n",
      "            Conv2d-5           [-1, 64, 12, 12]         102,464\n",
      "       BatchNorm2d-6           [-1, 64, 12, 12]             128\n",
      "           Dropout-7           [-1, 64, 12, 12]               0\n",
      "            Conv2d-8          [-1, 128, 10, 10]          73,856\n",
      "       BatchNorm2d-9          [-1, 128, 10, 10]             256\n",
      "           Conv2d-10            [-1, 128, 8, 8]         147,584\n",
      "      BatchNorm2d-11            [-1, 128, 8, 8]             256\n",
      "           Conv2d-12            [-1, 128, 4, 4]         409,728\n",
      "      BatchNorm2d-13            [-1, 128, 4, 4]             256\n",
      "          Dropout-14            [-1, 128, 4, 4]               0\n",
      "           Conv2d-15            [-1, 256, 1, 1]         524,544\n",
      "      BatchNorm2d-16            [-1, 256, 1, 1]             512\n",
      "          Dropout-17            [-1, 256, 1, 1]               0\n",
      "           Linear-18                  [-1, 256]          65,792\n",
      "      BatchNorm1d-19                  [-1, 256]             512\n",
      "           Linear-20                   [-1, 10]           2,570\n",
      "================================================================\n",
      "Total params: 1,366,282\n",
      "Trainable params: 1,366,282\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.81\n",
      "Params size (MB): 5.21\n",
      "Estimated Total Size (MB): 7.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# model = convNet(in_channels=1)\n",
    "# model = SE_Net(in_channels=1)\n",
    "model = convNet_avp(in_channels=1)\n",
    "model.cuda()\n",
    "summary(model, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from imgaug import augmenters as iaa\n",
    "# from imgaug.augmentables.segmaps import SegmentationMapOnImage\n",
    "\n",
    "class ImgAugTransform:\n",
    "    def __init__(self):\n",
    "        self.aug = iaa.Sequential([\n",
    "#         iaa.Scale((640, 480)),\n",
    "#         iaa.Fliplr(0.5),\n",
    "            \n",
    "#         iaa.Sometimes(0.5, iaa.GaussianBlur(sigma=(0, 0.6))),\n",
    "        iaa.Sometimes(0.1, iaa.AverageBlur(1.2)),\n",
    "        iaa.Sometimes(0.5, iaa.Affine(rotate=(-35, 35),order=[0, 1],translate_px={\"x\":(-3, 3),\"y\":(-4,4)},mode='symmetric')),\n",
    "        iaa.Sometimes(0.5,iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.25))),\n",
    "        iaa.Sometimes(0.1, iaa.SaltAndPepper(0.05,False)),\n",
    "        iaa.Invert(0.5),\n",
    "#         iaa.Add((-5, 5)), # change brightness of images (by -10 to 10 of original value)\n",
    "#         iaa.AdditiveGaussianNoise(-1,1)\n",
    "        iaa.Sometimes(0.2,iaa.GammaContrast(2))\n",
    "            \n",
    "#         iaa.AddToHueAndSaturation(from_colorspace=\"GRAY\",value=(-20, 20))  #Hue-> color, saturation -> saido\n",
    "    ])\n",
    "    def __call__(self, img, mask=None):\n",
    "        img = np.array(img)        \n",
    "        return self.aug.augment_image(image=img)\n",
    "#         return self.aug(image=img, segmentation_maps=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trans and Dataset definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([\n",
    "#         transforms.ColorJitter(0.9,0.2,0.2,0.5),\n",
    "#         transforms.RandomAffine(degrees=10,translate=(0.25,0.25),scale=(0.75,1.25),shear=5),  \n",
    "        transforms.RandomAffine(degrees=10,translate=(0.1,0.1),scale=(0.9,1.1)),    \n",
    "#         transforms.RandomAffine(degrees=10,translate=(0.2,0.2),scale=[0.9,1.1]), #For native distinguisher\n",
    "#         ImgAugTransform(),\n",
    "#         lambda x: Image.fromarray(x),\n",
    "        transforms.ToTensor(),  #Take Image as input and convert to tensor with value from 0 to1  \n",
    "#         transforms.Normalize(mean=[0.08889289],std=[0.24106446])  #train_large dataset distribution\n",
    "#         transforms.Normalize(mean=[0.08229437],std=[0.23876116]) #train dataset dist\n",
    "#         transforms.Normalize(mean=[0.09549136],std=[0.24336776]) #dig_augmented distribution\n",
    "    ])\n",
    "\n",
    "trans_val = transforms.Compose([\n",
    "        transforms.ToTensor(),  #Take Image as input and convert to tensor with value from 0 to1\n",
    "#         transforms.Normalize(mean=[0.08889289],std=[0.24106446])  #train_large dataset distribution\n",
    "#         transforms.Normalize(mean=[0.08229437],std=[0.23876116]) #train dataset dist\n",
    "#         transforms.Normalize(mean=[0.09549136],std=[0.24336776]) #dig_augmented distribution\n",
    "    ])\n",
    "\n",
    "trans_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.08889289],std=[0.24106446])  #train_large dataset distribution\n",
    "#         transforms.Normalize(mean=[0.08229437],std=[0.23876116]) #train dataset dist\n",
    "#         transforms.Normalize(mean=[0.09549136],std=[0.24336776]) #dig_augmented distribution\n",
    "])\n",
    "\n",
    "global_data = pd.read_csv(\"./dataset/train.csv\")\n",
    "global_dig_aug_data = pd.read_csv(\"./dataset/Dig-Mnist-Augmented.csv\")\n",
    "# global_dig_data = pd.read_csv(\"./dataset/Dig-MNIST.csv\")\n",
    "# global_data_large = pd.read_csv(\"./dataset/train_large.csv\")\n",
    "\n",
    "class KMnistDataset(Dataset):\n",
    "    def __init__(self,data_len=None, is_validate=False,validate_rate=None,indices=None):\n",
    "        self.is_validate = is_validate\n",
    "        self.data = global_data\n",
    "#         self.data = global_dig_aug_data    ################Temp Revised Caution##############\n",
    "#         print(\"data shape:\", np.shape(self.data))\n",
    "        if data_len == None:\n",
    "            data_len = len(self.data)\n",
    "        \n",
    "        self.indices = indices\n",
    "        if self.is_validate:\n",
    "            self.len = int(data_len*validate_rate)\n",
    "            self.offset = int(data_len*(1-validate_rate))\n",
    "            self.transform = trans_val\n",
    "        else:\n",
    "            self.len = int(data_len*(1-validate_rate))\n",
    "            self.offset = 0\n",
    "            self.transform = trans\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        idx += self.offset\n",
    "        idx = self.indices[idx]\n",
    "#         print(idx)\n",
    "        img = self.data.iloc[idx, 1:].values.astype(np.uint8).reshape((28, 28))  #value: 0~255\n",
    "        label = self.data.iloc[idx, 0]  #(num,)\n",
    "        img = Image.fromarray(img)\n",
    "        img = self.transform(img)     #value: 0~1, shape:(1,28,28)\n",
    "        label = torch.as_tensor(label, dtype=torch.uint8)    #value: 0~9, shape(1)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "class KMnistDataset_binary_aid(Dataset):\n",
    "    def __init__(self,data_len=None, is_validate=False,validate_rate=None,indices=None):\n",
    "        self.is_validate = is_validate\n",
    "        self.data = global_data_large\n",
    "        \n",
    "        if data_len == None:\n",
    "            data_len = len(self.data)\n",
    "        \n",
    "        self.indices = indices\n",
    "        if self.is_validate:\n",
    "            self.len = int(data_len*validate_rate)\n",
    "            self.offset = int(data_len*(1-validate_rate))\n",
    "            self.transform = trans_val\n",
    "        else:\n",
    "            self.len = int(data_len*(1-validate_rate))\n",
    "            self.offset = 0\n",
    "            self.transform = trans\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        idx += self.offset\n",
    "        idx = self.indices[idx]\n",
    "#         print(idx)\n",
    "        img = self.data.iloc[idx, 2:].values.astype(np.uint8).reshape((28, 28))  #value: 0~255\n",
    "        native_label = self.data.iloc[idx, 0]  #(num,)\n",
    "        label = self.data.iloc[idx, 1]  #(num,)\n",
    "        img = Image.fromarray(img)\n",
    "        img = self.transform(img)     #value: 0~1, shape:(1,28,28)\n",
    "#         native_label = torch.as_tensor(native_label, dtype=torch.uint8).unsqueeze(0)    #value: 0~9, shape(1,1) for BCE loss\n",
    "        native_label = torch.as_tensor(native_label, dtype=torch.uint8)    #value: 0~9, shape(1) for CSE loss\n",
    "        label = torch.as_tensor(label, dtype=torch.uint8)    #value: 0~9, shape(1)\n",
    "        return img, native_label, label\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    \n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,data_len=None):\n",
    "        self.data = pd.read_csv(\"./dataset/test.csv\")\n",
    "        print(\"data shape:\", np.shape(self.data))\n",
    "        self.transform = trans_test\n",
    "        if data_len == None:\n",
    "            self.len = len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.data.iloc[idx, 1:].values.astype(np.uint8).reshape((28, 28))  #value: 0~255\n",
    "        img = Image.fromarray(img)\n",
    "        img = self.transform(img)     #value: 0~1, shape:(1,28,28)\n",
    "        return img, torch.Tensor([])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get kfold dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kfold_dataset_loader(k=5,val_rate=0.1,indices_len=None, batch_size=None,num_workers=None, binary_aid=False):\n",
    "    ###Return [list of train dataset_loader, list of val dataset_loader]\n",
    "    train_loader_list = []\n",
    "    val_loader_list = []\n",
    "    indices = np.arange(indices_len)\n",
    "    val_len = indices_len//k\n",
    "    idx = 0\n",
    "    \n",
    "    for i in range(k):\n",
    "#         np.random.shuffle(indices)  #Random cross validation\n",
    "        ind = np.concatenate([indices[:idx],indices[idx+val_len:],indices[idx:idx+val_len]])\n",
    "        idx += val_len\n",
    "#         print(ind)\n",
    "        \n",
    "        if binary_aid == True:\n",
    "            train_dataset = KMnistDataset_binary_aid(data_len=None,is_validate=False, validate_rate=val_rate,indices=ind)\n",
    "            val_dataset = KMnistDataset_binary_aid(data_len=None,is_validate=True, validate_rate=val_rate, indices=ind)\n",
    "        else:\n",
    "            train_dataset = KMnistDataset(data_len=None,is_validate=False, validate_rate=val_rate,indices=ind)\n",
    "            val_dataset = KMnistDataset(data_len=None,is_validate=True, validate_rate=val_rate, indices=ind)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "        \n",
    "        train_loader_list.append(train_loader)\n",
    "        val_loader_list.append(val_loader)\n",
    "        \n",
    "    return train_loader_list, val_loader_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(native_net=False):\n",
    "    #Advance model\n",
    "    # model_name = 'efficientnet-b0'\n",
    "    # image_size = EfficientNet.get_image_size(model_name)\n",
    "    # model = EfficientNet.from_pretrained(model_name, num_classes=10)\n",
    "    # model = model.to(device)\n",
    "\n",
    "    #Basic cnn\n",
    "    if native_net == True:\n",
    "        model = convNet_native(in_channels=1)\n",
    "    else:\n",
    "#         model = SE_Net(in_channels=1)\n",
    "#         model = convNet(in_channels=1)\n",
    "        model = convNet_avp(in_channels=1)\n",
    "    \n",
    "    # model.load_state_dict(torch.load(\"./Kmnist_saved_model/ep20_acc0.9910\"))\n",
    "\n",
    "    #pretrained model\n",
    "#     model = models.resnet18()\n",
    "#     model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,bias=False)\n",
    "# #     summary(model, input_size=(1, 28, 28))\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        model.cuda()\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get dataset distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# train distribution: mean=[0.08229437],std=[0.23876116]\n",
    "# dig augmented distribution: mean=[0.09549136],std=[0.24336776]\n",
    "# train large distribution: mean=[0.08889286],std=[0.24106438]\n",
    "\n",
    "def get_dataset_mean_std(dataloader):\n",
    "    print(\"Calculate distribution:\")\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    nb_samples = 0.\n",
    "    for data in dataloader:\n",
    "        img = data[0].to(device)\n",
    "        batch_samples = img.size(0)\n",
    "        img = img.contiguous().view(batch_samples, img.size(1), -1)\n",
    "        mean += img.mean(2).sum(0)\n",
    "        std += img.std(2).sum(0)\n",
    "        nb_samples += batch_samples\n",
    "        if nb_samples%5120 == 0:\n",
    "            print(\"Finished:\", nb_samples)\n",
    "            \n",
    "    print(\"num of samples:\",nb_samples)\n",
    "    mean /= nb_samples\n",
    "    std /= nb_samples\n",
    "#     print(\"Average mean:\",mean)\n",
    "#     print(\"Average std:\", std)\n",
    "    return mean.cpu().numpy(), std.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get train and val loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation rate: 0.06666666666666667\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "num_workers = 0\n",
    "k = 15\n",
    "indices_len = 60000\n",
    "vr = (indices_len//k)/indices_len\n",
    "print(\"validation rate:\",vr)\n",
    "\n",
    "# indices_len = 10240  ################Temp Revised Caution##############\n",
    "# indices_len = 120000\n",
    "\n",
    "###Single dataset\n",
    "# indices = np.arange(indices_len)\n",
    "# train_dataset = KMnistDataset(data_len=None,is_validate=False,validate_rate=vr,indices=indices)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "# mean, std = get_dataset_mean_std(train_loader)\n",
    "# print(\"train distribution: mean={},std={}\".format(mean, std))\n",
    "\n",
    "# indices = np.arange(10240)\n",
    "# dig_val_dataset = DigValDataset(data_len=None,indices=indices)\n",
    "# dig_val_loader = DataLoader(dig_val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "# mean, std = get_dataset_mean_std(dig_val_loader)\n",
    "# print(\"validate distribution:\",mean, std)\n",
    "\n",
    "# test_dataset = TestDataset(data_len=None)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "# mean, std = get_dataset_mean_std(test_loader)\n",
    "# print(\"test distribution:\",mean, std)\n",
    "\n",
    "###K-fold dataset\n",
    "train_loaders, val_loaders = get_kfold_dataset_loader(k, vr, indices_len, batch_size, num_workers, binary_aid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train native classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    epochs = 120\n",
    "    period = 40\n",
    "    ensemble_models = []\n",
    "    lr = 1e-3\n",
    "    val_period = 1\n",
    "    \n",
    "    criterion_b = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"Fold:\",len(train_loaders))\n",
    "    \n",
    "    for fold in range(len(train_loaders)):\n",
    "        train_loader = train_loaders[fold]\n",
    "        val_loader = val_loaders[fold]\n",
    "        \n",
    "        model = get_model(native_net=True)\n",
    "        torch.cuda.empty_cache()    #Need further check\n",
    "            \n",
    "        max_acc_b = 0\n",
    "        min_loss_b = 10000\n",
    "        best_model_dict = None\n",
    "        data_num = 0\n",
    "        loss_avg_b = 0\n",
    "\n",
    "#         optimizer = torch.optim.SGD(model.parameters(),lr=lr)\n",
    "#         optimizer = torch.optim.RMSprop(model.parameters(),lr=lr,alpha=0.9)\n",
    "        optimizer = torch.optim.Adam(model.parameters(),lr=lr,betas=(0.9,0.99))\n",
    "#         optimizer = torch.optim.Adagrad(model.parameters(),lr=lr)\n",
    "#         lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=period,T_mult=1,eta_min=1e-5) #original \n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, patience=15)\n",
    "        \n",
    "        tmp_count = 0\n",
    "        for ep in range(0,epochs+1):\n",
    "            model.train()\n",
    "            for idx, data in enumerate(train_loader):\n",
    "                img, target_b, target = data\n",
    "                img, target_b, target = img.to(device), target_b.to(device,dtype=torch.long), target.to(device,dtype=torch.long)\n",
    "                pred_b = model(img)\n",
    "                  \n",
    "                loss_b = criterion_b(pred_b,target_b) \n",
    "                loss_avg_b += loss_b.item()\n",
    "                \n",
    "                data_num += img.size(0)\n",
    "                optimizer.zero_grad()\n",
    "                loss_b.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            ###Cosine annealing\n",
    "#             lr_scheduler.step()\n",
    "\n",
    "            ###Evaluate Train Loss \n",
    "#             if ep%2 == 0:\n",
    "#                 loss_avg /= data_num\n",
    "#                 print(\"Ep:{}, loss:{}, lr:{}\".format(ep, loss_avg,optimizer.param_groups[0]['lr']))\n",
    "#                 loss_avg = 0\n",
    "#                 data_num = 0\n",
    "\n",
    "            ###Validation\n",
    "            if ep!=0 and ep%val_period == 0:\n",
    "                model.eval()\n",
    "                acc_b = 0\n",
    "                val_loss_b = 0\n",
    "                data_num  = 0\n",
    "                with torch.no_grad():\n",
    "                    for idx, data in enumerate(val_loader):\n",
    "                        img, target_b, target = data\n",
    "                        img, target_b, target = img.to(device), target_b.to(device,dtype=torch.long), target.to(device,dtype=torch.long)\n",
    "                        pred_b = model(img)\n",
    "\n",
    "                        val_loss_b += criterion_b(pred_b,target_b).item()\n",
    "                        \n",
    "                        # print(pred) \n",
    "                        ########\n",
    "                        _,pred_native_class = torch.max(pred_b.data, 1)\n",
    "                        \n",
    "                        acc_b += (pred_native_class == target_b).sum().item()\n",
    "                        data_num += img.size(0)\n",
    "\n",
    "                acc_b /= data_num\n",
    "                val_loss_b /= data_num\n",
    "\n",
    "                ###Plateau\n",
    "                lr_scheduler.step(val_loss_b)\n",
    "                if optimizer.param_groups[0]['lr'] < 1e-4:\n",
    "                    break                    \n",
    "\n",
    "                if acc_b >= max_acc_b:\n",
    "                    max_acc_b = acc_b\n",
    "                    best_model_dict = model.state_dict()\n",
    "                    \n",
    "                if val_loss_b <= min_loss:\n",
    "                    min_loss_b = val_loss_b\n",
    "#                     best_model_dict = model.state_dict()\n",
    "                \n",
    "                print(\"Episode:{}, Validation Loss:{},Acc_b:{:.3f}%,lr:{}\"\n",
    "                      .format(ep,val_loss_b,acc_b*100,optimizer.param_groups[0]['lr']))\n",
    "            \n",
    "            if ep!=0 and ep%10 == 0:\n",
    "                torch.save(best_model_dict, \"./Kmnist_saved_model/tmp_Fold{}_acc_b{:.3f}\".format(fold,max_acc_b*1e2))\n",
    "            \n",
    "        ###K-Fold ensemble: Saved k best model for k dataloader\n",
    "        print(\"===================Best Fold:{} Saved, Acc:{}==================\".format(fold,max_acc_b))\n",
    "        torch.save(best_model_dict, \"./Kmnist_saved_model/Fold{}_loss{:.4f}_acc_b{:.3f}\".format(fold,min_loss_b*1e3,max_acc_b*1e2))\n",
    "        print(\"======================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train digit classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 15\n",
      "Episode:1, Validation Loss:0.0036366967558860777,Acc:20.9250%,lr:0.001\n",
      "Episode:2, Validation Loss:0.004017080128192902,Acc:22.3250%,lr:0.001\n",
      "Episode:3, Validation Loss:0.004571078419685364,Acc:21.4000%,lr:0.001\n",
      "Episode:4, Validation Loss:0.0014447613656520843,Acc:68.3750%,lr:0.001\n",
      "Episode:5, Validation Loss:0.0015801115334033967,Acc:68.9750%,lr:0.001\n",
      "Episode:6, Validation Loss:0.0011174093037843705,Acc:77.8750%,lr:0.001\n",
      "Episode:7, Validation Loss:0.0006554327011108398,Acc:87.6000%,lr:0.001\n",
      "Episode:8, Validation Loss:0.00040168851613998414,Acc:91.1000%,lr:0.001\n",
      "Episode:9, Validation Loss:0.00011330749001353979,Acc:97.0000%,lr:0.001\n",
      "Episode:10, Validation Loss:6.730526289902627e-05,Acc:98.3500%,lr:0.001\n",
      "Episode:11, Validation Loss:5.867774644866586e-05,Acc:98.6500%,lr:0.001\n",
      "Episode:12, Validation Loss:5.5472910986281934e-05,Acc:98.8750%,lr:0.001\n",
      "Episode:13, Validation Loss:7.729627098888159e-05,Acc:98.4750%,lr:0.001\n",
      "Episode:14, Validation Loss:8.165460609598085e-05,Acc:98.6750%,lr:0.001\n",
      "Episode:15, Validation Loss:7.222920475760475e-05,Acc:98.5500%,lr:0.001\n",
      "Episode:16, Validation Loss:7.908329303609207e-05,Acc:98.1000%,lr:0.001\n",
      "Episode:17, Validation Loss:8.579441532492637e-05,Acc:97.8250%,lr:0.001\n",
      "Episode:18, Validation Loss:8.437210024567321e-05,Acc:98.4750%,lr:0.001\n",
      "Episode:19, Validation Loss:8.325715886894613e-05,Acc:98.4750%,lr:0.001\n",
      "Episode:20, Validation Loss:6.955837982241064e-05,Acc:98.6250%,lr:0.001\n",
      "Episode:21, Validation Loss:7.608413696289063e-05,Acc:98.3250%,lr:0.001\n",
      "Episode:22, Validation Loss:8.983650803565979e-05,Acc:97.8750%,lr:0.001\n",
      "Episode:23, Validation Loss:8.6117243161425e-05,Acc:97.9750%,lr:0.001\n",
      "Episode:24, Validation Loss:6.163969601038844e-05,Acc:98.6500%,lr:0.001\n",
      "Episode:25, Validation Loss:7.171832647873088e-05,Acc:98.6500%,lr:0.001\n",
      "Episode:26, Validation Loss:9.281363501213491e-05,Acc:98.1250%,lr:0.001\n",
      "Episode:27, Validation Loss:5.857244192156941e-05,Acc:98.8000%,lr:0.001\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Episode:28, Validation Loss:6.957113486714661e-05,Acc:98.5500%,lr:0.0001\n",
      "Episode:29, Validation Loss:6.162320583825931e-05,Acc:98.9000%,lr:0.0001\n",
      "Episode:30, Validation Loss:6.20497404015623e-05,Acc:98.8500%,lr:0.0001\n",
      "Episode:31, Validation Loss:5.9910816024057565e-05,Acc:98.8500%,lr:0.0001\n",
      "Episode:32, Validation Loss:6.129413191229105e-05,Acc:98.8500%,lr:0.0001\n",
      "Episode:33, Validation Loss:5.972696957178414e-05,Acc:98.9500%,lr:0.0001\n",
      "Episode:34, Validation Loss:5.992535850964486e-05,Acc:98.9250%,lr:0.0001\n",
      "Episode:35, Validation Loss:5.907248641597107e-05,Acc:98.9500%,lr:0.0001\n",
      "Episode:36, Validation Loss:6.318200324312783e-05,Acc:98.8750%,lr:0.0001\n",
      "Episode:37, Validation Loss:6.274221825879067e-05,Acc:98.8750%,lr:0.0001\n",
      "Episode:38, Validation Loss:6.162010031403042e-05,Acc:98.9500%,lr:0.0001\n",
      "Episode:39, Validation Loss:6.346773327095434e-05,Acc:98.7750%,lr:0.0001\n",
      "Episode:40, Validation Loss:6.108266045339405e-05,Acc:98.9000%,lr:0.0001\n",
      "Episode:41, Validation Loss:6.296037850552238e-05,Acc:98.9750%,lr:0.0001\n",
      "Episode:42, Validation Loss:6.20668011251837e-05,Acc:98.8250%,lr:0.0001\n",
      "Episode:43, Validation Loss:6.306048898841254e-05,Acc:98.8750%,lr:0.0001\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Episode:44, Validation Loss:6.488776078913361e-05,Acc:98.6750%,lr:1e-05\n",
      "Episode:45, Validation Loss:6.411492312327028e-05,Acc:98.7250%,lr:1e-05\n",
      "Episode:46, Validation Loss:6.391143705695868e-05,Acc:98.7250%,lr:1e-05\n",
      "Episode:47, Validation Loss:6.358318999991752e-05,Acc:98.8000%,lr:1e-05\n",
      "Episode:48, Validation Loss:6.282225894392468e-05,Acc:98.8250%,lr:1e-05\n",
      "Episode:49, Validation Loss:6.287338273250498e-05,Acc:98.7500%,lr:1e-05\n",
      "Episode:50, Validation Loss:6.400706109707244e-05,Acc:98.8250%,lr:1e-05\n",
      "Episode:51, Validation Loss:6.398764712503179e-05,Acc:98.8000%,lr:1e-05\n",
      "Episode:52, Validation Loss:6.387403351254761e-05,Acc:98.7500%,lr:1e-05\n",
      "Episode:53, Validation Loss:6.378334172768518e-05,Acc:98.8000%,lr:1e-05\n",
      "Episode:54, Validation Loss:6.384892572532408e-05,Acc:98.7750%,lr:1e-05\n",
      "Episode:55, Validation Loss:6.346885199309327e-05,Acc:98.8750%,lr:1e-05\n",
      "Episode:56, Validation Loss:6.333487379015423e-05,Acc:98.7750%,lr:1e-05\n",
      "Episode:57, Validation Loss:6.35675905214157e-05,Acc:98.8000%,lr:1e-05\n",
      "Episode:58, Validation Loss:6.288444297388196e-05,Acc:98.7750%,lr:1e-05\n",
      "Episode:59, Validation Loss:6.270237025455572e-05,Acc:98.8250%,lr:1e-05\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-06.\n",
      "===================Best Fold:0 Saved, Acc:0.98975==================\n",
      "======================================================\n",
      "Episode:1, Validation Loss:0.00962274670600891,Acc:10.0000%,lr:0.001\n",
      "Episode:2, Validation Loss:0.011687200784683228,Acc:10.0000%,lr:0.001\n",
      "Episode:3, Validation Loss:0.016656743049621583,Acc:10.0000%,lr:0.001\n",
      "Episode:4, Validation Loss:0.01442086124420166,Acc:10.0000%,lr:0.001\n",
      "Episode:5, Validation Loss:0.01918835687637329,Acc:10.0000%,lr:0.001\n",
      "Episode:6, Validation Loss:0.010633460521697998,Acc:19.5500%,lr:0.001\n",
      "Episode:7, Validation Loss:0.0008711128234863281,Acc:80.4250%,lr:0.001\n",
      "Episode:8, Validation Loss:0.0011736498028039931,Acc:72.8750%,lr:0.001\n",
      "Episode:9, Validation Loss:2.1622205851599574e-05,Acc:99.1500%,lr:0.001\n",
      "Episode:10, Validation Loss:5.122906481847167e-05,Acc:98.6250%,lr:0.001\n",
      "Episode:11, Validation Loss:2.2185578127391637e-05,Acc:99.4250%,lr:0.001\n",
      "Episode:12, Validation Loss:1.7974089249037205e-05,Acc:99.4500%,lr:0.001\n",
      "Episode:13, Validation Loss:1.3962351600639523e-05,Acc:99.6250%,lr:0.001\n",
      "Episode:14, Validation Loss:8.418775303289294e-05,Acc:97.0000%,lr:0.001\n",
      "Episode:15, Validation Loss:1.7248893156647682e-05,Acc:99.5500%,lr:0.001\n",
      "Episode:16, Validation Loss:2.606303815264255e-05,Acc:99.3000%,lr:0.001\n",
      "Episode:17, Validation Loss:1.870325766503811e-05,Acc:99.4500%,lr:0.001\n",
      "Episode:18, Validation Loss:2.4872401845641433e-05,Acc:99.3250%,lr:0.001\n",
      "Episode:19, Validation Loss:1.7452346975915134e-05,Acc:99.4500%,lr:0.001\n",
      "Episode:20, Validation Loss:1.4551551663316785e-05,Acc:99.5750%,lr:0.001\n",
      "Episode:21, Validation Loss:2.687523397617042e-05,Acc:99.2250%,lr:0.001\n",
      "Episode:22, Validation Loss:1.8768033012747765e-05,Acc:99.5250%,lr:0.001\n",
      "Episode:23, Validation Loss:1.3745572476182132e-05,Acc:99.5750%,lr:0.001\n",
      "Episode:24, Validation Loss:2.5138198165223003e-05,Acc:99.3750%,lr:0.001\n",
      "Episode:25, Validation Loss:2.052494534291327e-05,Acc:99.3500%,lr:0.001\n",
      "Episode:26, Validation Loss:1.5576935606077314e-05,Acc:99.6250%,lr:0.001\n",
      "Episode:27, Validation Loss:2.0740117877721786e-05,Acc:99.4250%,lr:0.001\n",
      "Episode:28, Validation Loss:1.913168968167156e-05,Acc:99.4250%,lr:0.001\n",
      "Episode:29, Validation Loss:1.3202810427173973e-05,Acc:99.6250%,lr:0.001\n",
      "Episode:30, Validation Loss:2.03796197893098e-05,Acc:99.5750%,lr:0.001\n",
      "Episode:31, Validation Loss:2.383564889896661e-05,Acc:99.4500%,lr:0.001\n",
      "Episode:32, Validation Loss:2.198713819961995e-05,Acc:99.3000%,lr:0.001\n",
      "Episode:33, Validation Loss:1.822509407065809e-05,Acc:99.5000%,lr:0.001\n",
      "Episode:34, Validation Loss:2.149545797146857e-05,Acc:99.2250%,lr:0.001\n",
      "Episode:35, Validation Loss:2.6985321193933487e-05,Acc:99.3000%,lr:0.001\n",
      "Episode:36, Validation Loss:1.4926200965419411e-05,Acc:99.5000%,lr:0.001\n",
      "Episode:37, Validation Loss:1.2723434716463088e-05,Acc:99.6500%,lr:0.001\n",
      "Episode:38, Validation Loss:1.4790604473091662e-05,Acc:99.5250%,lr:0.001\n",
      "Episode:39, Validation Loss:1.729448209516704e-05,Acc:99.4750%,lr:0.001\n",
      "Episode:40, Validation Loss:1.7236683401279152e-05,Acc:99.5250%,lr:0.001\n",
      "Episode:41, Validation Loss:1.4529933920130134e-05,Acc:99.6000%,lr:0.001\n",
      "Episode:42, Validation Loss:3.3518707263283435e-05,Acc:98.9250%,lr:0.001\n",
      "Episode:43, Validation Loss:1.604282995685935e-05,Acc:99.5750%,lr:0.001\n",
      "Episode:44, Validation Loss:1.5874171163886786e-05,Acc:99.6500%,lr:0.001\n",
      "Episode:45, Validation Loss:1.4689298230223358e-05,Acc:99.5750%,lr:0.001\n",
      "Episode:46, Validation Loss:1.3247886090539396e-05,Acc:99.6500%,lr:0.001\n",
      "Episode:47, Validation Loss:1.2424448505043983e-05,Acc:99.6250%,lr:0.001\n",
      "Episode:48, Validation Loss:2.8684958815574647e-05,Acc:99.1750%,lr:0.001\n",
      "Episode:49, Validation Loss:2.2111985366791487e-05,Acc:99.5250%,lr:0.001\n",
      "Episode:50, Validation Loss:1.5687374863773585e-05,Acc:99.6500%,lr:0.001\n",
      "Episode:51, Validation Loss:2.0794048439711333e-05,Acc:99.4000%,lr:0.001\n",
      "Episode:52, Validation Loss:3.440553415566683e-05,Acc:99.0500%,lr:0.001\n",
      "Episode:53, Validation Loss:1.5609779162332414e-05,Acc:99.6250%,lr:0.001\n",
      "Episode:54, Validation Loss:1.7934360541403293e-05,Acc:99.4750%,lr:0.001\n",
      "Episode:55, Validation Loss:2.123646973632276e-05,Acc:99.5000%,lr:0.001\n",
      "Episode:56, Validation Loss:2.047213492915034e-05,Acc:99.5750%,lr:0.001\n",
      "Episode:57, Validation Loss:1.687360124196857e-05,Acc:99.6000%,lr:0.001\n",
      "Episode:58, Validation Loss:1.8625105847604572e-05,Acc:99.4500%,lr:0.001\n",
      "Episode:59, Validation Loss:2.105725067667663e-05,Acc:99.5500%,lr:0.001\n",
      "Episode:60, Validation Loss:1.772480399813503e-05,Acc:99.6000%,lr:0.001\n",
      "Episode:61, Validation Loss:2.4071114021353425e-05,Acc:99.4250%,lr:0.001\n",
      "Episode:62, Validation Loss:2.1149218780919908e-05,Acc:99.4500%,lr:0.001\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Episode:63, Validation Loss:1.4669704833067953e-05,Acc:99.5750%,lr:0.0001\n",
      "Episode:64, Validation Loss:1.2682663975283504e-05,Acc:99.6750%,lr:0.0001\n",
      "Episode:65, Validation Loss:1.1738693690858782e-05,Acc:99.6500%,lr:0.0001\n",
      "Episode:66, Validation Loss:1.241503469645977e-05,Acc:99.6750%,lr:0.0001\n",
      "Episode:67, Validation Loss:1.1258302256464958e-05,Acc:99.7000%,lr:0.0001\n",
      "Episode:68, Validation Loss:1.1802909662947058e-05,Acc:99.7000%,lr:0.0001\n",
      "Episode:69, Validation Loss:1.3251906260848046e-05,Acc:99.7250%,lr:0.0001\n",
      "Episode:70, Validation Loss:1.2352052959613503e-05,Acc:99.6750%,lr:0.0001\n",
      "Episode:71, Validation Loss:1.152793993242085e-05,Acc:99.7750%,lr:0.0001\n",
      "Episode:72, Validation Loss:1.1734451400116086e-05,Acc:99.6750%,lr:0.0001\n",
      "Episode:73, Validation Loss:1.2246707803569733e-05,Acc:99.7000%,lr:0.0001\n",
      "Episode:74, Validation Loss:1.2178808334283531e-05,Acc:99.7000%,lr:0.0001\n",
      "Episode:75, Validation Loss:1.199896726757288e-05,Acc:99.7250%,lr:0.0001\n",
      "Episode:76, Validation Loss:1.2750390684232115e-05,Acc:99.7000%,lr:0.0001\n",
      "Episode:77, Validation Loss:1.2794926180504262e-05,Acc:99.6750%,lr:0.0001\n",
      "Episode:78, Validation Loss:1.249838573858142e-05,Acc:99.7000%,lr:0.0001\n",
      "Episode:79, Validation Loss:1.2728148140013219e-05,Acc:99.7000%,lr:0.0001\n",
      "Episode:80, Validation Loss:1.1813443154096603e-05,Acc:99.6750%,lr:0.0001\n",
      "Episode:81, Validation Loss:1.3644217047840357e-05,Acc:99.7000%,lr:0.0001\n",
      "Episode:82, Validation Loss:1.3262001564726233e-05,Acc:99.7000%,lr:0.0001\n",
      "Epoch    82: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Episode:83, Validation Loss:1.2626547133550049e-05,Acc:99.7000%,lr:1e-05\n",
      "Episode:84, Validation Loss:1.2468837900087237e-05,Acc:99.7250%,lr:1e-05\n",
      "Episode:85, Validation Loss:1.2566674849949776e-05,Acc:99.7250%,lr:1e-05\n",
      "Episode:86, Validation Loss:1.2668388430029153e-05,Acc:99.7250%,lr:1e-05\n",
      "Episode:87, Validation Loss:1.262272393796593e-05,Acc:99.7250%,lr:1e-05\n",
      "Episode:88, Validation Loss:1.2808141647838056e-05,Acc:99.7250%,lr:1e-05\n",
      "Episode:89, Validation Loss:1.2615321087650954e-05,Acc:99.7250%,lr:1e-05\n",
      "Episode:90, Validation Loss:1.2619208078831435e-05,Acc:99.7250%,lr:1e-05\n",
      "Episode:91, Validation Loss:1.2803058256395162e-05,Acc:99.7250%,lr:1e-05\n",
      "Episode:92, Validation Loss:1.2909614946693183e-05,Acc:99.7250%,lr:1e-05\n",
      "Episode:93, Validation Loss:1.2762275291606784e-05,Acc:99.7250%,lr:1e-05\n",
      "Episode:94, Validation Loss:1.2784864637069404e-05,Acc:99.7250%,lr:1e-05\n",
      "Episode:95, Validation Loss:1.2972254306077958e-05,Acc:99.7250%,lr:1e-05\n",
      "Episode:96, Validation Loss:1.2933432240970433e-05,Acc:99.7250%,lr:1e-05\n",
      "Episode:97, Validation Loss:1.3053573900833727e-05,Acc:99.7250%,lr:1e-05\n",
      "Episode:98, Validation Loss:1.3135639834217727e-05,Acc:99.7500%,lr:1e-05\n",
      "Epoch    98: reducing learning rate of group 0 to 1.0000e-06.\n",
      "===================Best Fold:1 Saved, Acc:0.99775==================\n",
      "======================================================\n",
      "Episode:1, Validation Loss:0.0028075583577156066,Acc:31.2000%,lr:0.001\n",
      "Episode:2, Validation Loss:0.001992114543914795,Acc:53.7000%,lr:0.001\n",
      "Episode:3, Validation Loss:0.002387955665588379,Acc:49.2750%,lr:0.001\n",
      "Episode:4, Validation Loss:0.000729532852768898,Acc:77.0250%,lr:0.001\n",
      "Episode:5, Validation Loss:0.0003335915803909302,Acc:90.0000%,lr:0.001\n",
      "Episode:6, Validation Loss:0.00013193598203361034,Acc:95.4250%,lr:0.001\n",
      "Episode:7, Validation Loss:2.6248167268931867e-05,Acc:99.1500%,lr:0.001\n",
      "Episode:8, Validation Loss:2.394664566963911e-05,Acc:99.2500%,lr:0.001\n",
      "Episode:9, Validation Loss:2.149349870160222e-05,Acc:99.2250%,lr:0.001\n",
      "Episode:10, Validation Loss:4.324095277115703e-05,Acc:98.7250%,lr:0.001\n",
      "Episode:11, Validation Loss:3.533029765821993e-05,Acc:99.1500%,lr:0.001\n",
      "Episode:12, Validation Loss:2.6516307145357133e-05,Acc:99.0500%,lr:0.001\n",
      "Episode:13, Validation Loss:2.8770228498615324e-05,Acc:99.1500%,lr:0.001\n",
      "Episode:14, Validation Loss:3.616734547540545e-05,Acc:98.9000%,lr:0.001\n",
      "Episode:15, Validation Loss:2.248634328134358e-05,Acc:99.2750%,lr:0.001\n",
      "Episode:16, Validation Loss:2.2733298595994712e-05,Acc:99.3750%,lr:0.001\n",
      "Episode:17, Validation Loss:1.7324815271422267e-05,Acc:99.4750%,lr:0.001\n",
      "Episode:18, Validation Loss:4.809404630213976e-05,Acc:98.6500%,lr:0.001\n",
      "Episode:19, Validation Loss:1.581821765284985e-05,Acc:99.5500%,lr:0.001\n",
      "Episode:20, Validation Loss:2.7808719081804157e-05,Acc:99.2250%,lr:0.001\n",
      "Episode:21, Validation Loss:1.7354818526655435e-05,Acc:99.4250%,lr:0.001\n",
      "Episode:22, Validation Loss:2.0457216072827576e-05,Acc:99.4250%,lr:0.001\n",
      "Episode:23, Validation Loss:2.1288590040057896e-05,Acc:99.4750%,lr:0.001\n",
      "Episode:24, Validation Loss:1.8059527268633246e-05,Acc:99.5750%,lr:0.001\n",
      "Episode:25, Validation Loss:2.57906352635473e-05,Acc:99.2000%,lr:0.001\n",
      "Episode:26, Validation Loss:2.2637875052168966e-05,Acc:99.4250%,lr:0.001\n",
      "Episode:27, Validation Loss:1.9396763294935228e-05,Acc:99.3500%,lr:0.001\n",
      "Episode:28, Validation Loss:4.079711018130183e-05,Acc:99.1000%,lr:0.001\n",
      "Episode:29, Validation Loss:3.301674942485988e-05,Acc:99.1750%,lr:0.001\n",
      "Episode:30, Validation Loss:3.375196363776922e-05,Acc:99.2500%,lr:0.001\n",
      "Episode:31, Validation Loss:1.951219839975238e-05,Acc:99.5000%,lr:0.001\n",
      "Episode:32, Validation Loss:1.1622543446719647e-05,Acc:99.6500%,lr:0.001\n",
      "Episode:33, Validation Loss:3.817246202379465e-05,Acc:99.3250%,lr:0.001\n",
      "Episode:34, Validation Loss:2.9642781242728233e-05,Acc:99.2500%,lr:0.001\n",
      "Episode:35, Validation Loss:2.0080590853467584e-05,Acc:99.3750%,lr:0.001\n",
      "Episode:36, Validation Loss:1.5658298740163444e-05,Acc:99.6750%,lr:0.001\n",
      "Episode:37, Validation Loss:2.33667129650712e-05,Acc:99.4750%,lr:0.001\n",
      "Episode:38, Validation Loss:1.416577585041523e-05,Acc:99.5500%,lr:0.001\n",
      "Episode:39, Validation Loss:1.9471765030175447e-05,Acc:99.4000%,lr:0.001\n",
      "Episode:40, Validation Loss:2.924317098222673e-05,Acc:99.1250%,lr:0.001\n",
      "Episode:41, Validation Loss:1.8288575345650315e-05,Acc:99.4750%,lr:0.001\n",
      "Episode:42, Validation Loss:1.6268185921944677e-05,Acc:99.6500%,lr:0.001\n",
      "Episode:43, Validation Loss:1.5354645089246332e-05,Acc:99.5500%,lr:0.001\n",
      "Episode:44, Validation Loss:2.9783422825857998e-05,Acc:99.3000%,lr:0.001\n",
      "Episode:45, Validation Loss:3.3758572535589337e-05,Acc:99.2750%,lr:0.001\n",
      "Episode:46, Validation Loss:2.019873238168657e-05,Acc:99.5000%,lr:0.001\n",
      "Episode:47, Validation Loss:2.3420490557327865e-05,Acc:99.4250%,lr:0.001\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Episode:48, Validation Loss:1.5563382301479577e-05,Acc:99.6750%,lr:0.0001\n",
      "Episode:49, Validation Loss:1.519513048697263e-05,Acc:99.7000%,lr:0.0001\n",
      "Episode:50, Validation Loss:1.2910788762383164e-05,Acc:99.7500%,lr:0.0001\n",
      "Episode:51, Validation Loss:1.4920038636773825e-05,Acc:99.6500%,lr:0.0001\n",
      "Episode:52, Validation Loss:1.3879304402507841e-05,Acc:99.7000%,lr:0.0001\n",
      "Episode:53, Validation Loss:1.3535319711081683e-05,Acc:99.7000%,lr:0.0001\n",
      "Episode:54, Validation Loss:1.2479457189328969e-05,Acc:99.7000%,lr:0.0001\n",
      "Episode:55, Validation Loss:1.372168364468962e-05,Acc:99.6750%,lr:0.0001\n",
      "Episode:56, Validation Loss:1.364116184413433e-05,Acc:99.6250%,lr:0.0001\n",
      "Episode:57, Validation Loss:1.4481804566457868e-05,Acc:99.6250%,lr:0.0001\n",
      "Episode:58, Validation Loss:1.4036642503924668e-05,Acc:99.6500%,lr:0.0001\n",
      "Episode:59, Validation Loss:1.4741374063305557e-05,Acc:99.6500%,lr:0.0001\n",
      "Episode:60, Validation Loss:1.484376541338861e-05,Acc:99.6500%,lr:0.0001\n",
      "Episode:61, Validation Loss:1.5969543717801572e-05,Acc:99.6250%,lr:0.0001\n",
      "Episode:62, Validation Loss:1.805081218481064e-05,Acc:99.6000%,lr:0.0001\n",
      "Episode:63, Validation Loss:1.3616697979159653e-05,Acc:99.7000%,lr:0.0001\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Episode:64, Validation Loss:1.3368567219004035e-05,Acc:99.7000%,lr:1e-05\n",
      "Episode:65, Validation Loss:1.3883325387723744e-05,Acc:99.7000%,lr:1e-05\n",
      "Episode:66, Validation Loss:1.4176459168083966e-05,Acc:99.7000%,lr:1e-05\n",
      "Episode:67, Validation Loss:1.4452098403126002e-05,Acc:99.7000%,lr:1e-05\n",
      "Episode:68, Validation Loss:1.4641654910519718e-05,Acc:99.7000%,lr:1e-05\n",
      "Episode:69, Validation Loss:1.4789181295782328e-05,Acc:99.6750%,lr:1e-05\n",
      "Episode:70, Validation Loss:1.4886586694046855e-05,Acc:99.6750%,lr:1e-05\n",
      "Episode:71, Validation Loss:1.484686927869916e-05,Acc:99.6750%,lr:1e-05\n",
      "Episode:72, Validation Loss:1.474403333850205e-05,Acc:99.6750%,lr:1e-05\n",
      "Episode:73, Validation Loss:1.4727840316481888e-05,Acc:99.6750%,lr:1e-05\n",
      "Episode:74, Validation Loss:1.4873451320454478e-05,Acc:99.6750%,lr:1e-05\n",
      "Episode:75, Validation Loss:1.4554131892509758e-05,Acc:99.6750%,lr:1e-05\n",
      "Episode:76, Validation Loss:1.4152473653666676e-05,Acc:99.7000%,lr:1e-05\n",
      "Episode:77, Validation Loss:1.415003149304539e-05,Acc:99.7000%,lr:1e-05\n",
      "Episode:78, Validation Loss:1.3993507251143456e-05,Acc:99.7000%,lr:1e-05\n",
      "Episode:79, Validation Loss:1.4536287635564803e-05,Acc:99.7000%,lr:1e-05\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-06.\n",
      "===================Best Fold:2 Saved, Acc:0.9975==================\n",
      "======================================================\n",
      "Episode:1, Validation Loss:0.004956766486167908,Acc:29.1000%,lr:0.001\n",
      "Episode:2, Validation Loss:0.0053138648271560665,Acc:17.2500%,lr:0.001\n",
      "Episode:3, Validation Loss:0.005218489170074463,Acc:21.8000%,lr:0.001\n",
      "Episode:4, Validation Loss:0.001270065814256668,Acc:67.6500%,lr:0.001\n",
      "Episode:5, Validation Loss:0.0007308550626039505,Acc:78.0500%,lr:0.001\n",
      "Episode:6, Validation Loss:0.00018299131654202937,Acc:94.2000%,lr:0.001\n",
      "Episode:7, Validation Loss:5.596767039969564e-05,Acc:98.2500%,lr:0.001\n",
      "Episode:8, Validation Loss:4.0366246830672024e-05,Acc:98.8500%,lr:0.001\n",
      "Episode:9, Validation Loss:2.9588839039206506e-05,Acc:99.2250%,lr:0.001\n",
      "Episode:10, Validation Loss:4.2659936705604195e-05,Acc:98.7250%,lr:0.001\n",
      "Episode:11, Validation Loss:2.7898526284843684e-05,Acc:99.2250%,lr:0.001\n",
      "Episode:12, Validation Loss:5.2089394768700004e-05,Acc:98.5000%,lr:0.001\n",
      "Episode:13, Validation Loss:3.533675579819828e-05,Acc:99.0250%,lr:0.001\n",
      "Episode:14, Validation Loss:5.405161436647177e-05,Acc:98.8250%,lr:0.001\n",
      "Episode:15, Validation Loss:4.0976051706820725e-05,Acc:98.8750%,lr:0.001\n",
      "Episode:16, Validation Loss:1.4982095919549465e-05,Acc:99.6000%,lr:0.001\n",
      "Episode:17, Validation Loss:6.82470902102068e-05,Acc:98.5500%,lr:0.001\n",
      "Episode:18, Validation Loss:3.82673692656681e-05,Acc:99.0500%,lr:0.001\n",
      "Episode:19, Validation Loss:6.998389877844602e-05,Acc:98.5250%,lr:0.001\n",
      "Episode:20, Validation Loss:1.9657433847896754e-05,Acc:99.5250%,lr:0.001\n",
      "Episode:21, Validation Loss:2.653239923529327e-05,Acc:99.2750%,lr:0.001\n",
      "Episode:22, Validation Loss:1.2562106596305966e-05,Acc:99.5250%,lr:0.001\n",
      "Episode:23, Validation Loss:2.3193871602416038e-05,Acc:99.3250%,lr:0.001\n",
      "Episode:24, Validation Loss:3.498712938744575e-05,Acc:99.1000%,lr:0.001\n",
      "Episode:25, Validation Loss:3.0260186875239016e-05,Acc:99.1750%,lr:0.001\n",
      "Episode:26, Validation Loss:4.0038063772954045e-05,Acc:99.0750%,lr:0.001\n",
      "Episode:27, Validation Loss:4.0444020880386234e-05,Acc:99.0500%,lr:0.001\n",
      "Episode:28, Validation Loss:6.123684853082523e-05,Acc:98.9250%,lr:0.001\n",
      "Episode:29, Validation Loss:2.1396536496467887e-05,Acc:99.4000%,lr:0.001\n",
      "Episode:30, Validation Loss:2.6936533278785647e-05,Acc:99.3500%,lr:0.001\n",
      "Episode:31, Validation Loss:2.5507794227451088e-05,Acc:99.1750%,lr:0.001\n",
      "Episode:32, Validation Loss:5.9904215857386586e-05,Acc:98.6750%,lr:0.001\n",
      "Episode:33, Validation Loss:3.8103354279883205e-05,Acc:99.0750%,lr:0.001\n",
      "Episode:34, Validation Loss:3.5339648369699714e-05,Acc:99.0000%,lr:0.001\n",
      "Episode:35, Validation Loss:2.4789404589682818e-05,Acc:99.2500%,lr:0.001\n",
      "Episode:36, Validation Loss:1.9948092522099614e-05,Acc:99.5000%,lr:0.001\n",
      "Episode:37, Validation Loss:2.3822866729460656e-05,Acc:99.2750%,lr:0.001\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Episode:38, Validation Loss:2.3717538802884518e-05,Acc:99.4250%,lr:0.0001\n",
      "Episode:39, Validation Loss:2.0625147444661708e-05,Acc:99.4000%,lr:0.0001\n",
      "Episode:40, Validation Loss:2.52762560849078e-05,Acc:99.3250%,lr:0.0001\n",
      "Episode:41, Validation Loss:2.6361997646745293e-05,Acc:99.4250%,lr:0.0001\n",
      "Episode:42, Validation Loss:2.9977870755828916e-05,Acc:99.2250%,lr:0.0001\n",
      "Episode:43, Validation Loss:2.996131149120629e-05,Acc:99.2500%,lr:0.0001\n",
      "Episode:44, Validation Loss:2.7012990321964026e-05,Acc:99.3000%,lr:0.0001\n",
      "Episode:45, Validation Loss:2.847859391476959e-05,Acc:99.2250%,lr:0.0001\n",
      "Episode:46, Validation Loss:2.8090561390854417e-05,Acc:99.3000%,lr:0.0001\n",
      "Episode:47, Validation Loss:3.286876331549138e-05,Acc:99.1500%,lr:0.0001\n",
      "Episode:48, Validation Loss:3.720094554591924e-05,Acc:99.0750%,lr:0.0001\n",
      "Episode:49, Validation Loss:2.852812525816262e-05,Acc:99.2250%,lr:0.0001\n",
      "Episode:50, Validation Loss:2.6918865158222615e-05,Acc:99.2750%,lr:0.0001\n",
      "Episode:51, Validation Loss:2.4003069847822188e-05,Acc:99.4000%,lr:0.0001\n",
      "Episode:52, Validation Loss:2.732904569711536e-05,Acc:99.3250%,lr:0.0001\n",
      "Episode:53, Validation Loss:2.38733688602224e-05,Acc:99.3250%,lr:0.0001\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Episode:54, Validation Loss:2.9593760380521415e-05,Acc:99.3000%,lr:1e-05\n",
      "Episode:55, Validation Loss:2.83313924446702e-05,Acc:99.3500%,lr:1e-05\n",
      "Episode:56, Validation Loss:2.909900574013591e-05,Acc:99.3250%,lr:1e-05\n",
      "Episode:57, Validation Loss:2.903738466557115e-05,Acc:99.3000%,lr:1e-05\n",
      "Episode:58, Validation Loss:2.9127818066626787e-05,Acc:99.2750%,lr:1e-05\n",
      "Episode:59, Validation Loss:2.8693223604932426e-05,Acc:99.3250%,lr:1e-05\n",
      "Episode:60, Validation Loss:2.7701533515937627e-05,Acc:99.3250%,lr:1e-05\n",
      "Episode:61, Validation Loss:2.900436462368816e-05,Acc:99.3000%,lr:1e-05\n",
      "Episode:62, Validation Loss:2.8455540537834166e-05,Acc:99.2750%,lr:1e-05\n",
      "Episode:63, Validation Loss:2.9209862230345608e-05,Acc:99.3000%,lr:1e-05\n",
      "Episode:64, Validation Loss:2.9439954203553496e-05,Acc:99.3000%,lr:1e-05\n",
      "Episode:65, Validation Loss:2.8937433031387628e-05,Acc:99.3000%,lr:1e-05\n",
      "Episode:66, Validation Loss:2.733626146800816e-05,Acc:99.3000%,lr:1e-05\n",
      "Episode:67, Validation Loss:2.8181510511785745e-05,Acc:99.3000%,lr:1e-05\n",
      "Episode:68, Validation Loss:2.9338828404434025e-05,Acc:99.2750%,lr:1e-05\n",
      "Episode:69, Validation Loss:2.880762959830463e-05,Acc:99.3000%,lr:1e-05\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-06.\n",
      "===================Best Fold:3 Saved, Acc:0.996==================\n",
      "======================================================\n",
      "Episode:1, Validation Loss:0.003072905421257019,Acc:10.1250%,lr:0.001\n",
      "Episode:2, Validation Loss:0.00408799946308136,Acc:14.0000%,lr:0.001\n",
      "Episode:3, Validation Loss:0.003367672383785248,Acc:28.4000%,lr:0.001\n",
      "Episode:4, Validation Loss:0.00205020996928215,Acc:40.8750%,lr:0.001\n",
      "Episode:5, Validation Loss:0.000937761515378952,Acc:73.6250%,lr:0.001\n",
      "Episode:6, Validation Loss:0.00038127292320132256,Acc:88.1000%,lr:0.001\n",
      "Episode:7, Validation Loss:6.846125237643719e-05,Acc:97.5000%,lr:0.001\n",
      "Episode:8, Validation Loss:3.7867768201977015e-05,Acc:98.8500%,lr:0.001\n",
      "Episode:9, Validation Loss:4.356541857123375e-05,Acc:98.6000%,lr:0.001\n",
      "Episode:10, Validation Loss:3.513112384825945e-05,Acc:99.0000%,lr:0.001\n",
      "Episode:11, Validation Loss:4.9360780976712704e-05,Acc:98.5500%,lr:0.001\n",
      "Episode:12, Validation Loss:4.747139802202583e-05,Acc:98.6250%,lr:0.001\n",
      "Episode:13, Validation Loss:3.4946071449667215e-05,Acc:99.0000%,lr:0.001\n",
      "Episode:14, Validation Loss:3.8162870798259975e-05,Acc:98.8750%,lr:0.001\n",
      "Episode:15, Validation Loss:4.2034853249788286e-05,Acc:98.7500%,lr:0.001\n",
      "Episode:16, Validation Loss:5.5178701877593994e-05,Acc:98.5750%,lr:0.001\n",
      "Episode:17, Validation Loss:5.6291060987859965e-05,Acc:98.3750%,lr:0.001\n",
      "Episode:18, Validation Loss:5.11497613042593e-05,Acc:98.7000%,lr:0.001\n",
      "Episode:19, Validation Loss:3.578003402799368e-05,Acc:98.9250%,lr:0.001\n",
      "Episode:20, Validation Loss:4.460106696933508e-05,Acc:98.7500%,lr:0.001\n",
      "Episode:21, Validation Loss:3.6184535827487704e-05,Acc:98.8500%,lr:0.001\n",
      "Episode:22, Validation Loss:3.884337842464447e-05,Acc:98.8000%,lr:0.001\n",
      "Episode:23, Validation Loss:3.996933437883854e-05,Acc:98.9000%,lr:0.001\n",
      "Episode:24, Validation Loss:5.237227212637663e-05,Acc:98.6500%,lr:0.001\n",
      "Episode:25, Validation Loss:3.6799415946006775e-05,Acc:99.0000%,lr:0.001\n",
      "Episode:26, Validation Loss:3.951425431296229e-05,Acc:99.1000%,lr:0.001\n",
      "Episode:27, Validation Loss:5.119253462180495e-05,Acc:98.9000%,lr:0.001\n",
      "Episode:28, Validation Loss:5.192896071821451e-05,Acc:98.5750%,lr:0.001\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Episode:29, Validation Loss:3.525358904153108e-05,Acc:98.8750%,lr:0.0001\n",
      "Episode:30, Validation Loss:4.125536046922207e-05,Acc:98.8250%,lr:0.0001\n",
      "Episode:31, Validation Loss:4.3873438611626624e-05,Acc:98.7250%,lr:0.0001\n",
      "Episode:32, Validation Loss:4.63059414178133e-05,Acc:98.7500%,lr:0.0001\n",
      "Episode:33, Validation Loss:4.4359576888382435e-05,Acc:98.8000%,lr:0.0001\n",
      "Episode:34, Validation Loss:3.9269722066819666e-05,Acc:98.8750%,lr:0.0001\n",
      "Episode:35, Validation Loss:3.8292068988084795e-05,Acc:98.9000%,lr:0.0001\n",
      "Episode:36, Validation Loss:3.871939331293106e-05,Acc:98.9500%,lr:0.0001\n",
      "Episode:37, Validation Loss:3.665742091834545e-05,Acc:98.9750%,lr:0.0001\n",
      "Episode:38, Validation Loss:3.7678079679608344e-05,Acc:99.0000%,lr:0.0001\n",
      "Episode:39, Validation Loss:3.8088289089500905e-05,Acc:99.0750%,lr:0.0001\n",
      "Episode:40, Validation Loss:3.712113108485937e-05,Acc:98.9750%,lr:0.0001\n",
      "Episode:41, Validation Loss:3.6304967012256386e-05,Acc:99.0500%,lr:0.0001\n",
      "Episode:42, Validation Loss:3.908745944499969e-05,Acc:99.0500%,lr:0.0001\n",
      "Episode:43, Validation Loss:4.0389669127762315e-05,Acc:98.9750%,lr:0.0001\n",
      "Episode:44, Validation Loss:3.738790098577738e-05,Acc:99.0250%,lr:0.0001\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Episode:45, Validation Loss:4.262098576873541e-05,Acc:98.9000%,lr:1e-05\n",
      "Episode:46, Validation Loss:4.172027669847011e-05,Acc:98.9500%,lr:1e-05\n",
      "Episode:47, Validation Loss:4.159403825178743e-05,Acc:98.9500%,lr:1e-05\n",
      "Episode:48, Validation Loss:4.120031837373972e-05,Acc:98.9750%,lr:1e-05\n",
      "Episode:49, Validation Loss:4.1321069933474064e-05,Acc:98.9750%,lr:1e-05\n",
      "Episode:50, Validation Loss:4.087954480201006e-05,Acc:99.0000%,lr:1e-05\n",
      "Episode:51, Validation Loss:4.0643238462507726e-05,Acc:99.0000%,lr:1e-05\n",
      "Episode:52, Validation Loss:4.028779407963157e-05,Acc:99.0500%,lr:1e-05\n",
      "Episode:53, Validation Loss:4.075869824737311e-05,Acc:99.0500%,lr:1e-05\n",
      "Episode:54, Validation Loss:4.02483306825161e-05,Acc:99.0500%,lr:1e-05\n",
      "Episode:55, Validation Loss:4.019431583583355e-05,Acc:99.0500%,lr:1e-05\n",
      "Episode:56, Validation Loss:3.9891756139695644e-05,Acc:99.0500%,lr:1e-05\n",
      "Episode:57, Validation Loss:3.982674796134233e-05,Acc:99.0500%,lr:1e-05\n",
      "Episode:58, Validation Loss:3.865076694637537e-05,Acc:99.0750%,lr:1e-05\n",
      "Episode:59, Validation Loss:3.934398340061307e-05,Acc:99.0750%,lr:1e-05\n",
      "Episode:60, Validation Loss:3.9500840939581396e-05,Acc:99.0750%,lr:1e-05\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-06.\n",
      "===================Best Fold:4 Saved, Acc:0.991==================\n",
      "======================================================\n",
      "Episode:1, Validation Loss:0.007048369407653809,Acc:10.0000%,lr:0.001\n",
      "Episode:2, Validation Loss:0.009709117650985718,Acc:10.0000%,lr:0.001\n",
      "Episode:3, Validation Loss:0.005145869255065918,Acc:19.5250%,lr:0.001\n",
      "Episode:4, Validation Loss:0.0014726961404085159,Acc:59.2750%,lr:0.001\n",
      "Episode:5, Validation Loss:0.0017689547538757323,Acc:62.6250%,lr:0.001\n",
      "Episode:6, Validation Loss:0.0008108009696006775,Acc:77.5000%,lr:0.001\n",
      "Episode:7, Validation Loss:6.682082079350948e-05,Acc:97.9500%,lr:0.001\n",
      "Episode:8, Validation Loss:3.9323968696407976e-05,Acc:98.7250%,lr:0.001\n",
      "Episode:9, Validation Loss:5.5906005029100925e-05,Acc:98.5750%,lr:0.001\n",
      "Episode:10, Validation Loss:3.1646552728489045e-05,Acc:99.0250%,lr:0.001\n",
      "Episode:11, Validation Loss:1.0236862057354301e-05,Acc:99.6750%,lr:0.001\n",
      "Episode:12, Validation Loss:2.442837976559531e-05,Acc:99.2250%,lr:0.001\n",
      "Episode:13, Validation Loss:4.661916758050211e-05,Acc:98.7750%,lr:0.001\n",
      "Episode:14, Validation Loss:4.586291022133082e-05,Acc:98.6500%,lr:0.001\n",
      "Episode:15, Validation Loss:5.481491901446134e-05,Acc:98.5500%,lr:0.001\n",
      "Episode:16, Validation Loss:4.7434436157345775e-05,Acc:98.3000%,lr:0.001\n",
      "Episode:17, Validation Loss:3.0872518196702005e-05,Acc:99.1500%,lr:0.001\n",
      "Episode:18, Validation Loss:2.0240676880348473e-05,Acc:99.3000%,lr:0.001\n",
      "Episode:19, Validation Loss:1.7840315238572657e-05,Acc:99.4250%,lr:0.001\n",
      "Episode:20, Validation Loss:6.342792825307697e-05,Acc:98.5000%,lr:0.001\n",
      "Episode:21, Validation Loss:3.825083214906044e-05,Acc:98.8500%,lr:0.001\n",
      "Episode:22, Validation Loss:2.7784818434156476e-05,Acc:99.3000%,lr:0.001\n",
      "Episode:23, Validation Loss:2.0989940960134844e-05,Acc:99.5000%,lr:0.001\n",
      "Episode:24, Validation Loss:1.689540717052296e-05,Acc:99.6000%,lr:0.001\n",
      "Episode:25, Validation Loss:2.081388446094934e-05,Acc:99.4500%,lr:0.001\n",
      "Episode:26, Validation Loss:3.3814575232099745e-05,Acc:99.0750%,lr:0.001\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Episode:27, Validation Loss:4.114483512239531e-05,Acc:98.8500%,lr:0.0001\n",
      "Episode:28, Validation Loss:2.4041541008045897e-05,Acc:99.3000%,lr:0.0001\n",
      "Episode:29, Validation Loss:2.446903081727214e-05,Acc:99.4000%,lr:0.0001\n",
      "Episode:30, Validation Loss:1.9107306172372772e-05,Acc:99.4750%,lr:0.0001\n",
      "Episode:31, Validation Loss:2.236146006907802e-05,Acc:99.3750%,lr:0.0001\n",
      "Episode:32, Validation Loss:1.7798914093873465e-05,Acc:99.5250%,lr:0.0001\n",
      "Episode:33, Validation Loss:1.8557786825112997e-05,Acc:99.5250%,lr:0.0001\n",
      "Episode:34, Validation Loss:2.0651355473091825e-05,Acc:99.4750%,lr:0.0001\n",
      "Episode:35, Validation Loss:1.7727918282616882e-05,Acc:99.5250%,lr:0.0001\n",
      "Episode:36, Validation Loss:1.9909650116460396e-05,Acc:99.5000%,lr:0.0001\n",
      "Episode:37, Validation Loss:2.155216118262615e-05,Acc:99.5000%,lr:0.0001\n",
      "Episode:38, Validation Loss:2.0303366603911855e-05,Acc:99.5000%,lr:0.0001\n",
      "Episode:39, Validation Loss:1.950411526922835e-05,Acc:99.4750%,lr:0.0001\n",
      "Episode:40, Validation Loss:1.775766859645955e-05,Acc:99.4750%,lr:0.0001\n",
      "Episode:41, Validation Loss:2.418593887705356e-05,Acc:99.4500%,lr:0.0001\n",
      "Episode:42, Validation Loss:2.3347507129074074e-05,Acc:99.4250%,lr:0.0001\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Episode:43, Validation Loss:2.226210152730346e-05,Acc:99.4250%,lr:1e-05\n",
      "Episode:44, Validation Loss:2.201089799928013e-05,Acc:99.4750%,lr:1e-05\n",
      "Episode:45, Validation Loss:2.145284434664063e-05,Acc:99.5250%,lr:1e-05\n",
      "Episode:46, Validation Loss:2.1671154783689415e-05,Acc:99.5000%,lr:1e-05\n",
      "Episode:47, Validation Loss:2.164511469891295e-05,Acc:99.5000%,lr:1e-05\n",
      "Episode:48, Validation Loss:2.142598784121219e-05,Acc:99.5000%,lr:1e-05\n",
      "Episode:49, Validation Loss:2.0498619982390665e-05,Acc:99.5000%,lr:1e-05\n",
      "Episode:50, Validation Loss:2.087676561495755e-05,Acc:99.5000%,lr:1e-05\n",
      "Episode:51, Validation Loss:2.0207400651997888e-05,Acc:99.5000%,lr:1e-05\n",
      "Episode:52, Validation Loss:2.068491285899654e-05,Acc:99.5000%,lr:1e-05\n",
      "Episode:53, Validation Loss:1.988703399547376e-05,Acc:99.5000%,lr:1e-05\n",
      "Episode:54, Validation Loss:2.0735347381560133e-05,Acc:99.5000%,lr:1e-05\n",
      "Episode:55, Validation Loss:1.9925084270653315e-05,Acc:99.5000%,lr:1e-05\n",
      "Episode:56, Validation Loss:2.1094455500133334e-05,Acc:99.5000%,lr:1e-05\n",
      "Episode:57, Validation Loss:2.1024279107223265e-05,Acc:99.5000%,lr:1e-05\n",
      "Episode:58, Validation Loss:2.0057485016877763e-05,Acc:99.5000%,lr:1e-05\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-06.\n",
      "===================Best Fold:5 Saved, Acc:0.99675==================\n",
      "======================================================\n",
      "Episode:1, Validation Loss:0.0032958496212959288,Acc:21.0250%,lr:0.001\n",
      "Episode:2, Validation Loss:0.0029306052327156065,Acc:33.7500%,lr:0.001\n",
      "Episode:3, Validation Loss:0.004214224815368653,Acc:24.3000%,lr:0.001\n",
      "Episode:4, Validation Loss:0.0010316717326641083,Acc:74.0500%,lr:0.001\n",
      "Episode:5, Validation Loss:0.001629708707332611,Acc:72.4250%,lr:0.001\n",
      "Episode:6, Validation Loss:0.0013913720250129699,Acc:82.2750%,lr:0.001\n",
      "Episode:7, Validation Loss:7.7694458886981e-05,Acc:97.2000%,lr:0.001\n",
      "Episode:8, Validation Loss:2.2659705486148597e-05,Acc:99.2000%,lr:0.001\n",
      "Episode:9, Validation Loss:2.357395924627781e-05,Acc:99.2500%,lr:0.001\n",
      "Episode:10, Validation Loss:3.979716077446937e-05,Acc:98.8250%,lr:0.001\n",
      "Episode:11, Validation Loss:2.090296521782875e-05,Acc:99.2500%,lr:0.001\n",
      "Episode:12, Validation Loss:9.191791294142604e-06,Acc:99.6250%,lr:0.001\n",
      "Episode:13, Validation Loss:2.5477140909060834e-05,Acc:99.1250%,lr:0.001\n",
      "Episode:14, Validation Loss:1.3259143917821348e-05,Acc:99.5250%,lr:0.001\n",
      "Episode:15, Validation Loss:1.3199714012444019e-05,Acc:99.5250%,lr:0.001\n",
      "Episode:16, Validation Loss:9.051040979102255e-06,Acc:99.6250%,lr:0.001\n",
      "Episode:17, Validation Loss:1.2540852883830667e-05,Acc:99.6250%,lr:0.001\n",
      "Episode:18, Validation Loss:9.973720530979336e-06,Acc:99.7250%,lr:0.001\n",
      "Episode:19, Validation Loss:9.708344121463597e-06,Acc:99.7500%,lr:0.001\n",
      "Episode:20, Validation Loss:9.178139618597925e-06,Acc:99.7000%,lr:0.001\n",
      "Episode:21, Validation Loss:1.9983286503702403e-05,Acc:99.3250%,lr:0.001\n",
      "Episode:22, Validation Loss:1.0189586435444652e-05,Acc:99.6500%,lr:0.001\n",
      "Episode:23, Validation Loss:1.3719712500460446e-05,Acc:99.5500%,lr:0.001\n",
      "Episode:24, Validation Loss:1.3169537065550685e-05,Acc:99.6750%,lr:0.001\n",
      "Episode:25, Validation Loss:9.07612347509712e-06,Acc:99.7000%,lr:0.001\n",
      "Episode:26, Validation Loss:1.384050422348082e-05,Acc:99.4750%,lr:0.001\n",
      "Episode:27, Validation Loss:0.0006245911791920662,Acc:87.2250%,lr:0.001\n",
      "Episode:28, Validation Loss:3.0208315525669604e-05,Acc:99.0250%,lr:0.001\n",
      "Episode:29, Validation Loss:9.134300518780946e-06,Acc:99.7250%,lr:0.001\n",
      "Episode:30, Validation Loss:1.8724442459642888e-05,Acc:99.2750%,lr:0.001\n",
      "Episode:31, Validation Loss:7.655157358385623e-06,Acc:99.8000%,lr:0.001\n",
      "Episode:32, Validation Loss:9.717420442029833e-06,Acc:99.7000%,lr:0.001\n",
      "Episode:33, Validation Loss:1.2697807047516107e-05,Acc:99.7000%,lr:0.001\n",
      "Episode:34, Validation Loss:9.76845750119537e-06,Acc:99.6250%,lr:0.001\n",
      "Episode:35, Validation Loss:1.8091104691848157e-05,Acc:99.5500%,lr:0.001\n",
      "Episode:36, Validation Loss:9.949893108569085e-06,Acc:99.6500%,lr:0.001\n",
      "Episode:37, Validation Loss:1.767166785430163e-05,Acc:99.5250%,lr:0.001\n",
      "Episode:38, Validation Loss:1.3577526435256004e-05,Acc:99.5250%,lr:0.001\n",
      "Episode:39, Validation Loss:2.2914176108315588e-05,Acc:99.2250%,lr:0.001\n",
      "Episode:40, Validation Loss:1.399416325148195e-05,Acc:99.5500%,lr:0.001\n",
      "Episode:41, Validation Loss:2.0408367970958352e-05,Acc:99.4000%,lr:0.001\n",
      "Episode:42, Validation Loss:4.381583142094314e-05,Acc:98.6250%,lr:0.001\n",
      "Episode:43, Validation Loss:1.6032621962949633e-05,Acc:99.6750%,lr:0.001\n",
      "Episode:44, Validation Loss:1.512577326502651e-05,Acc:99.5000%,lr:0.001\n",
      "Episode:45, Validation Loss:1.3667203718796372e-05,Acc:99.5500%,lr:0.001\n",
      "Episode:46, Validation Loss:7.946718716993927e-06,Acc:99.7750%,lr:0.001\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Episode:47, Validation Loss:9.40078473649919e-06,Acc:99.7750%,lr:0.0001\n",
      "Episode:48, Validation Loss:7.04930501524359e-06,Acc:99.8750%,lr:0.0001\n",
      "Episode:49, Validation Loss:6.455529946833849e-06,Acc:99.8250%,lr:0.0001\n",
      "Episode:50, Validation Loss:7.920308853499591e-06,Acc:99.7750%,lr:0.0001\n",
      "Episode:51, Validation Loss:7.097227848134935e-06,Acc:99.8000%,lr:0.0001\n",
      "Episode:52, Validation Loss:6.599595188163221e-06,Acc:99.8750%,lr:0.0001\n",
      "Episode:53, Validation Loss:6.4862880390137435e-06,Acc:99.8750%,lr:0.0001\n",
      "Episode:54, Validation Loss:6.473300512880087e-06,Acc:99.8500%,lr:0.0001\n",
      "Episode:55, Validation Loss:6.729704793542624e-06,Acc:99.8250%,lr:0.0001\n",
      "Episode:56, Validation Loss:6.171510089188814e-06,Acc:99.8250%,lr:0.0001\n",
      "Episode:57, Validation Loss:6.58612628467381e-06,Acc:99.8000%,lr:0.0001\n",
      "Episode:58, Validation Loss:7.594637805595994e-06,Acc:99.8000%,lr:0.0001\n",
      "Episode:59, Validation Loss:5.914111970923841e-06,Acc:99.8000%,lr:0.0001\n",
      "Episode:60, Validation Loss:5.878768279217184e-06,Acc:99.8000%,lr:0.0001\n",
      "Episode:61, Validation Loss:6.769363535568118e-06,Acc:99.8250%,lr:0.0001\n",
      "Episode:62, Validation Loss:6.691806251183152e-06,Acc:99.7750%,lr:0.0001\n",
      "Episode:63, Validation Loss:6.345866830088198e-06,Acc:99.8000%,lr:0.0001\n",
      "Episode:64, Validation Loss:5.744974594563246e-06,Acc:99.8000%,lr:0.0001\n",
      "Episode:65, Validation Loss:5.534733878448606e-06,Acc:99.8500%,lr:0.0001\n",
      "Episode:66, Validation Loss:6.191705353558063e-06,Acc:99.8250%,lr:0.0001\n",
      "Episode:67, Validation Loss:6.3165833707898856e-06,Acc:99.7750%,lr:0.0001\n",
      "Episode:68, Validation Loss:8.403490646742284e-06,Acc:99.7750%,lr:0.0001\n",
      "Episode:69, Validation Loss:7.971587241627276e-06,Acc:99.7500%,lr:0.0001\n",
      "Episode:70, Validation Loss:8.053598401602357e-06,Acc:99.7500%,lr:0.0001\n",
      "Episode:71, Validation Loss:7.956363260746002e-06,Acc:99.7750%,lr:0.0001\n",
      "Episode:72, Validation Loss:7.731580000836402e-06,Acc:99.7000%,lr:0.0001\n",
      "Episode:73, Validation Loss:7.954993867315351e-06,Acc:99.6750%,lr:0.0001\n",
      "Episode:74, Validation Loss:8.81710764952004e-06,Acc:99.6750%,lr:0.0001\n",
      "Episode:75, Validation Loss:9.173228987492622e-06,Acc:99.7000%,lr:0.0001\n",
      "Episode:76, Validation Loss:8.655023644678295e-06,Acc:99.7250%,lr:0.0001\n",
      "Episode:77, Validation Loss:8.795455796644092e-06,Acc:99.7250%,lr:0.0001\n",
      "Episode:78, Validation Loss:7.033257163129747e-06,Acc:99.8000%,lr:0.0001\n",
      "Episode:79, Validation Loss:7.1309278719127176e-06,Acc:99.7750%,lr:0.0001\n",
      "Episode:80, Validation Loss:6.9281276082620025e-06,Acc:99.8250%,lr:0.0001\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Episode:81, Validation Loss:1.0083114728331566e-05,Acc:99.7000%,lr:1e-05\n",
      "Episode:82, Validation Loss:8.447379455901683e-06,Acc:99.7500%,lr:1e-05\n",
      "Episode:83, Validation Loss:8.126933360472322e-06,Acc:99.7500%,lr:1e-05\n",
      "Episode:84, Validation Loss:7.958891103044153e-06,Acc:99.7500%,lr:1e-05\n",
      "Episode:85, Validation Loss:7.464907597750425e-06,Acc:99.7500%,lr:1e-05\n",
      "Episode:86, Validation Loss:7.493194774724543e-06,Acc:99.7500%,lr:1e-05\n",
      "Episode:87, Validation Loss:7.708508754149079e-06,Acc:99.7500%,lr:1e-05\n",
      "Episode:88, Validation Loss:7.381798233836889e-06,Acc:99.7500%,lr:1e-05\n",
      "Episode:89, Validation Loss:7.605903781950474e-06,Acc:99.7500%,lr:1e-05\n",
      "Episode:90, Validation Loss:7.773623103275896e-06,Acc:99.7500%,lr:1e-05\n",
      "Episode:91, Validation Loss:7.928701583296061e-06,Acc:99.7500%,lr:1e-05\n",
      "Episode:92, Validation Loss:7.593027199618519e-06,Acc:99.7750%,lr:1e-05\n",
      "Episode:93, Validation Loss:7.631392567418516e-06,Acc:99.7500%,lr:1e-05\n",
      "Episode:94, Validation Loss:7.620356045663357e-06,Acc:99.7500%,lr:1e-05\n",
      "Episode:95, Validation Loss:7.498521823436022e-06,Acc:99.7500%,lr:1e-05\n",
      "Episode:96, Validation Loss:7.389145321212709e-06,Acc:99.7750%,lr:1e-05\n",
      "Epoch    96: reducing learning rate of group 0 to 1.0000e-06.\n",
      "===================Best Fold:6 Saved, Acc:0.99875==================\n",
      "======================================================\n",
      "Episode:1, Validation Loss:0.005565694689750671,Acc:10.0000%,lr:0.001\n",
      "Episode:2, Validation Loss:0.008563279867172242,Acc:10.0000%,lr:0.001\n",
      "Episode:3, Validation Loss:0.0041771942377090455,Acc:24.1000%,lr:0.001\n",
      "Episode:4, Validation Loss:0.002927993595600128,Acc:41.8000%,lr:0.001\n",
      "Episode:5, Validation Loss:0.0005318072438240052,Acc:85.8500%,lr:0.001\n",
      "Episode:6, Validation Loss:9.121175250038505e-05,Acc:97.7000%,lr:0.001\n",
      "Episode:7, Validation Loss:0.00026158642768859863,Acc:93.2250%,lr:0.001\n",
      "Episode:8, Validation Loss:5.7936370489187535e-05,Acc:98.6750%,lr:0.001\n",
      "Episode:9, Validation Loss:4.80257763992995e-05,Acc:98.6250%,lr:0.001\n",
      "Episode:10, Validation Loss:3.5164197208359835e-05,Acc:99.1500%,lr:0.001\n",
      "Episode:11, Validation Loss:3.412705229129642e-05,Acc:99.0750%,lr:0.001\n",
      "Episode:12, Validation Loss:3.1097962171770633e-05,Acc:99.2000%,lr:0.001\n",
      "Episode:13, Validation Loss:2.680683194193989e-05,Acc:99.2750%,lr:0.001\n",
      "Episode:14, Validation Loss:2.773991972208023e-05,Acc:99.2750%,lr:0.001\n",
      "Episode:15, Validation Loss:4.733077075798064e-05,Acc:98.7750%,lr:0.001\n",
      "Episode:16, Validation Loss:2.9386611655354498e-05,Acc:99.1000%,lr:0.001\n",
      "Episode:17, Validation Loss:3.430431918241084e-05,Acc:99.1750%,lr:0.001\n",
      "Episode:18, Validation Loss:3.737620660103857e-05,Acc:98.9750%,lr:0.001\n",
      "Episode:19, Validation Loss:4.5666101388633254e-05,Acc:98.6750%,lr:0.001\n",
      "Episode:20, Validation Loss:3.601820138283074e-05,Acc:99.0000%,lr:0.001\n",
      "Episode:21, Validation Loss:3.1256278161890806e-05,Acc:99.1000%,lr:0.001\n",
      "Episode:22, Validation Loss:3.59408640069887e-05,Acc:99.1750%,lr:0.001\n",
      "Episode:23, Validation Loss:4.576049116440117e-05,Acc:98.7250%,lr:0.001\n",
      "Episode:24, Validation Loss:3.63211240619421e-05,Acc:99.1000%,lr:0.001\n",
      "Episode:25, Validation Loss:3.1347265117801725e-05,Acc:99.2250%,lr:0.001\n",
      "Episode:26, Validation Loss:3.1947664683684704e-05,Acc:99.2500%,lr:0.001\n",
      "Episode:27, Validation Loss:4.265620850492269e-05,Acc:98.8500%,lr:0.001\n",
      "Episode:28, Validation Loss:0.0002490428686141968,Acc:93.8000%,lr:0.001\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Episode:29, Validation Loss:2.8513258555904032e-05,Acc:99.2250%,lr:0.0001\n",
      "Episode:30, Validation Loss:2.2254161187447606e-05,Acc:99.3000%,lr:0.0001\n",
      "Episode:31, Validation Loss:2.152347832452506e-05,Acc:99.3500%,lr:0.0001\n",
      "Episode:32, Validation Loss:2.2167362505570053e-05,Acc:99.3750%,lr:0.0001\n",
      "Episode:33, Validation Loss:2.245339751243591e-05,Acc:99.3750%,lr:0.0001\n",
      "Episode:34, Validation Loss:2.2468088543973862e-05,Acc:99.4500%,lr:0.0001\n",
      "Episode:35, Validation Loss:2.5232642190530898e-05,Acc:99.3000%,lr:0.0001\n",
      "Episode:36, Validation Loss:2.1889183903113007e-05,Acc:99.3500%,lr:0.0001\n",
      "Episode:37, Validation Loss:2.2588469320908187e-05,Acc:99.2750%,lr:0.0001\n",
      "Episode:38, Validation Loss:2.0997789571993052e-05,Acc:99.3750%,lr:0.0001\n",
      "Episode:39, Validation Loss:2.2749378287699073e-05,Acc:99.3500%,lr:0.0001\n",
      "Episode:40, Validation Loss:2.0417287189047782e-05,Acc:99.4500%,lr:0.0001\n",
      "Episode:41, Validation Loss:2.5501830037683248e-05,Acc:99.2000%,lr:0.0001\n",
      "Episode:42, Validation Loss:2.28185651358217e-05,Acc:99.3250%,lr:0.0001\n",
      "Episode:43, Validation Loss:2.4448309559375048e-05,Acc:99.2250%,lr:0.0001\n",
      "Episode:44, Validation Loss:2.223269909154624e-05,Acc:99.3500%,lr:0.0001\n",
      "Episode:45, Validation Loss:2.123711199965328e-05,Acc:99.4250%,lr:0.0001\n",
      "Episode:46, Validation Loss:2.7322749490849673e-05,Acc:99.2750%,lr:0.0001\n",
      "Episode:47, Validation Loss:2.34917321940884e-05,Acc:99.3250%,lr:0.0001\n",
      "Episode:48, Validation Loss:2.072381984908134e-05,Acc:99.4250%,lr:0.0001\n",
      "Episode:49, Validation Loss:2.2732380777597426e-05,Acc:99.2750%,lr:0.0001\n",
      "Episode:50, Validation Loss:2.1324352826923132e-05,Acc:99.3750%,lr:0.0001\n",
      "Episode:51, Validation Loss:2.018577081616968e-05,Acc:99.4500%,lr:0.0001\n",
      "Episode:52, Validation Loss:2.2162247565574945e-05,Acc:99.4000%,lr:0.0001\n",
      "Episode:53, Validation Loss:1.9975712173618375e-05,Acc:99.4250%,lr:0.0001\n",
      "Episode:54, Validation Loss:2.3232887382619084e-05,Acc:99.5250%,lr:0.0001\n",
      "Episode:55, Validation Loss:2.2010261425748468e-05,Acc:99.4250%,lr:0.0001\n",
      "Episode:56, Validation Loss:2.224811154883355e-05,Acc:99.3750%,lr:0.0001\n",
      "Episode:57, Validation Loss:2.5290885474532842e-05,Acc:99.3500%,lr:0.0001\n",
      "Episode:58, Validation Loss:2.3504144279286266e-05,Acc:99.3500%,lr:0.0001\n",
      "Episode:59, Validation Loss:2.7202412369661033e-05,Acc:99.2750%,lr:0.0001\n",
      "Episode:60, Validation Loss:2.2045001504011454e-05,Acc:99.3500%,lr:0.0001\n",
      "Episode:61, Validation Loss:1.9292919198051094e-05,Acc:99.5250%,lr:0.0001\n",
      "Episode:62, Validation Loss:2.3149291286244987e-05,Acc:99.2500%,lr:0.0001\n",
      "Episode:63, Validation Loss:1.7662154277786613e-05,Acc:99.5000%,lr:0.0001\n",
      "Episode:64, Validation Loss:2.1493580657988786e-05,Acc:99.4000%,lr:0.0001\n",
      "Episode:65, Validation Loss:2.4255657801404595e-05,Acc:99.4000%,lr:0.0001\n",
      "Episode:66, Validation Loss:1.863586576655507e-05,Acc:99.5250%,lr:0.0001\n",
      "Episode:67, Validation Loss:2.2719722357578576e-05,Acc:99.4000%,lr:0.0001\n",
      "Episode:68, Validation Loss:2.3785525583662092e-05,Acc:99.3750%,lr:0.0001\n",
      "Episode:69, Validation Loss:2.0610430045053364e-05,Acc:99.4250%,lr:0.0001\n",
      "Episode:70, Validation Loss:2.353617816697806e-05,Acc:99.3500%,lr:0.0001\n",
      "Episode:71, Validation Loss:2.7564156451262534e-05,Acc:99.2500%,lr:0.0001\n",
      "Episode:72, Validation Loss:2.5087745510973036e-05,Acc:99.2750%,lr:0.0001\n",
      "Episode:73, Validation Loss:2.447890071198344e-05,Acc:99.2500%,lr:0.0001\n",
      "Episode:74, Validation Loss:2.3632367490790785e-05,Acc:99.3500%,lr:0.0001\n",
      "Episode:75, Validation Loss:2.229064970742911e-05,Acc:99.3250%,lr:0.0001\n",
      "Episode:76, Validation Loss:2.6417668559588493e-05,Acc:99.2750%,lr:0.0001\n",
      "Episode:77, Validation Loss:2.326210227329284e-05,Acc:99.3750%,lr:0.0001\n",
      "Episode:78, Validation Loss:2.456855121999979e-05,Acc:99.3500%,lr:0.0001\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Episode:79, Validation Loss:2.4350393912754953e-05,Acc:99.3000%,lr:1e-05\n",
      "Episode:80, Validation Loss:2.3886823328211904e-05,Acc:99.3000%,lr:1e-05\n",
      "Episode:81, Validation Loss:2.3512197309173643e-05,Acc:99.3000%,lr:1e-05\n",
      "Episode:82, Validation Loss:2.2635171422734856e-05,Acc:99.3500%,lr:1e-05\n",
      "Episode:83, Validation Loss:2.3927429923787712e-05,Acc:99.3250%,lr:1e-05\n",
      "Episode:84, Validation Loss:2.3263024631887673e-05,Acc:99.3500%,lr:1e-05\n",
      "Episode:85, Validation Loss:2.20020585693419e-05,Acc:99.4000%,lr:1e-05\n",
      "Episode:86, Validation Loss:2.3121382342651488e-05,Acc:99.3500%,lr:1e-05\n",
      "Episode:87, Validation Loss:2.3688420886173845e-05,Acc:99.3500%,lr:1e-05\n",
      "Episode:88, Validation Loss:2.4854101706296206e-05,Acc:99.3000%,lr:1e-05\n",
      "Episode:89, Validation Loss:2.4320458411239087e-05,Acc:99.3000%,lr:1e-05\n",
      "Episode:90, Validation Loss:2.4307445972226562e-05,Acc:99.3250%,lr:1e-05\n",
      "Episode:91, Validation Loss:2.4683273863047363e-05,Acc:99.3250%,lr:1e-05\n",
      "Episode:92, Validation Loss:2.5168519583530725e-05,Acc:99.3500%,lr:1e-05\n",
      "Episode:93, Validation Loss:2.4355458095669745e-05,Acc:99.3500%,lr:1e-05\n",
      "Episode:94, Validation Loss:2.1961386082693934e-05,Acc:99.4250%,lr:1e-05\n",
      "Epoch    94: reducing learning rate of group 0 to 1.0000e-06.\n",
      "===================Best Fold:7 Saved, Acc:0.99525==================\n",
      "======================================================\n",
      "Episode:1, Validation Loss:0.006041943073272705,Acc:10.1000%,lr:0.001\n",
      "Episode:2, Validation Loss:0.007438629746437073,Acc:15.2500%,lr:0.001\n",
      "Episode:3, Validation Loss:0.009934624433517456,Acc:10.1250%,lr:0.001\n",
      "Episode:4, Validation Loss:0.003955308377742767,Acc:23.3750%,lr:0.001\n",
      "Episode:5, Validation Loss:0.003269365072250366,Acc:47.4750%,lr:0.001\n",
      "Episode:6, Validation Loss:0.0009031224250793457,Acc:83.2250%,lr:0.001\n",
      "Episode:7, Validation Loss:9.864146588370204e-06,Acc:99.5750%,lr:0.001\n",
      "Episode:8, Validation Loss:5.986195406876505e-06,Acc:99.7750%,lr:0.001\n",
      "Episode:9, Validation Loss:8.354844525456428e-06,Acc:99.7750%,lr:0.001\n",
      "Episode:10, Validation Loss:1.1896147159859539e-05,Acc:99.5750%,lr:0.001\n",
      "Episode:11, Validation Loss:9.344716556370258e-06,Acc:99.8000%,lr:0.001\n",
      "Episode:12, Validation Loss:5.4859681986272335e-06,Acc:99.8500%,lr:0.001\n",
      "Episode:13, Validation Loss:1.0548226302489638e-05,Acc:99.6750%,lr:0.001\n",
      "Episode:14, Validation Loss:6.411277106963098e-06,Acc:99.8250%,lr:0.001\n",
      "Episode:15, Validation Loss:8.783967467024923e-06,Acc:99.8000%,lr:0.001\n",
      "Episode:16, Validation Loss:6.7273228196427225e-06,Acc:99.7750%,lr:0.001\n",
      "Episode:17, Validation Loss:6.354707293212413e-06,Acc:99.8250%,lr:0.001\n",
      "Episode:18, Validation Loss:6.483905133791268e-06,Acc:99.7750%,lr:0.001\n",
      "Episode:19, Validation Loss:8.523461874574422e-06,Acc:99.7500%,lr:0.001\n",
      "Episode:20, Validation Loss:5.31283050077036e-06,Acc:99.8000%,lr:0.001\n",
      "Episode:21, Validation Loss:6.48300105240196e-06,Acc:99.7750%,lr:0.001\n",
      "Episode:22, Validation Loss:5.7799331843853e-06,Acc:99.8500%,lr:0.001\n",
      "Episode:23, Validation Loss:4.38178435433656e-06,Acc:99.9000%,lr:0.001\n",
      "Episode:24, Validation Loss:8.215012960135937e-06,Acc:99.8000%,lr:0.001\n",
      "Episode:25, Validation Loss:3.86174488812685e-06,Acc:99.9000%,lr:0.001\n",
      "Episode:26, Validation Loss:8.547609788365662e-06,Acc:99.7750%,lr:0.001\n",
      "Episode:27, Validation Loss:1.7835377249866723e-05,Acc:99.2000%,lr:0.001\n",
      "Episode:28, Validation Loss:4.835812491364777e-06,Acc:99.9000%,lr:0.001\n",
      "Episode:29, Validation Loss:6.171465385705232e-06,Acc:99.8250%,lr:0.001\n",
      "Episode:30, Validation Loss:4.841345828026533e-06,Acc:99.8250%,lr:0.001\n",
      "Episode:31, Validation Loss:3.556854324415326e-06,Acc:99.8500%,lr:0.001\n",
      "Episode:32, Validation Loss:9.192304452881216e-06,Acc:99.7250%,lr:0.001\n",
      "Episode:33, Validation Loss:1.2472723610699177e-05,Acc:99.5750%,lr:0.001\n",
      "Episode:34, Validation Loss:6.023125257343054e-06,Acc:99.8000%,lr:0.001\n",
      "Episode:35, Validation Loss:4.5348518760874865e-06,Acc:99.8750%,lr:0.001\n",
      "Episode:36, Validation Loss:8.798835333436727e-06,Acc:99.7500%,lr:0.001\n",
      "Episode:37, Validation Loss:8.25311685912311e-06,Acc:99.7250%,lr:0.001\n",
      "Episode:38, Validation Loss:1.2217404320836067e-05,Acc:99.6750%,lr:0.001\n",
      "Episode:39, Validation Loss:4.225741373375058e-06,Acc:99.8750%,lr:0.001\n",
      "Episode:40, Validation Loss:4.756389302201569e-06,Acc:99.8750%,lr:0.001\n",
      "Episode:41, Validation Loss:1.0568563244305552e-05,Acc:99.7750%,lr:0.001\n",
      "Episode:42, Validation Loss:3.33669877727516e-06,Acc:99.9500%,lr:0.001\n",
      "Episode:43, Validation Loss:7.050834392430261e-06,Acc:99.7000%,lr:0.001\n",
      "Episode:44, Validation Loss:5.4500987753272055e-06,Acc:99.8250%,lr:0.001\n",
      "Episode:45, Validation Loss:7.867168518714607e-06,Acc:99.7000%,lr:0.001\n",
      "Episode:46, Validation Loss:3.5876333713531494e-06,Acc:99.8750%,lr:0.001\n",
      "Episode:47, Validation Loss:6.229316466487944e-06,Acc:99.8500%,lr:0.001\n",
      "Episode:48, Validation Loss:1.5123369405046105e-05,Acc:99.4250%,lr:0.001\n",
      "Episode:49, Validation Loss:6.068462855182588e-06,Acc:99.7500%,lr:0.001\n",
      "Episode:50, Validation Loss:4.0365870809182525e-06,Acc:99.8250%,lr:0.001\n",
      "Episode:51, Validation Loss:1.2930831173434854e-05,Acc:99.5000%,lr:0.001\n",
      "Episode:52, Validation Loss:4.339787759818137e-06,Acc:99.8500%,lr:0.001\n",
      "Episode:53, Validation Loss:1.4574808068573475e-05,Acc:99.4500%,lr:0.001\n",
      "Episode:54, Validation Loss:7.003898732364178e-06,Acc:99.8750%,lr:0.001\n",
      "Episode:55, Validation Loss:5.055814632214606e-06,Acc:99.8500%,lr:0.001\n",
      "Episode:56, Validation Loss:4.341951804235578e-06,Acc:99.8750%,lr:0.001\n",
      "Episode:57, Validation Loss:7.99128960352391e-06,Acc:99.6750%,lr:0.001\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Episode:58, Validation Loss:6.491485983133316e-06,Acc:99.8500%,lr:0.0001\n",
      "Episode:59, Validation Loss:4.2414385825395584e-06,Acc:99.8750%,lr:0.0001\n",
      "Episode:60, Validation Loss:4.028887604363263e-06,Acc:99.9000%,lr:0.0001\n",
      "Episode:61, Validation Loss:4.3749157339334486e-06,Acc:99.9000%,lr:0.0001\n",
      "Episode:62, Validation Loss:3.5752389230765404e-06,Acc:99.9250%,lr:0.0001\n",
      "Episode:63, Validation Loss:3.4341595019213855e-06,Acc:99.9500%,lr:0.0001\n",
      "Episode:64, Validation Loss:3.4526276867836715e-06,Acc:99.9500%,lr:0.0001\n",
      "Episode:65, Validation Loss:3.877395414747297e-06,Acc:99.9250%,lr:0.0001\n",
      "Episode:66, Validation Loss:3.4864524495787918e-06,Acc:99.9000%,lr:0.0001\n",
      "Episode:67, Validation Loss:3.2330116373486815e-06,Acc:99.9500%,lr:0.0001\n",
      "Episode:68, Validation Loss:3.6274779704399408e-06,Acc:99.9000%,lr:0.0001\n",
      "Episode:69, Validation Loss:3.6791774909943342e-06,Acc:99.9250%,lr:0.0001\n",
      "Episode:70, Validation Loss:3.058065311051905e-06,Acc:99.9250%,lr:0.0001\n",
      "Episode:71, Validation Loss:3.2498022774234414e-06,Acc:99.8750%,lr:0.0001\n",
      "Episode:72, Validation Loss:3.1989216222427786e-06,Acc:99.9000%,lr:0.0001\n",
      "Episode:73, Validation Loss:4.001452587544918e-06,Acc:99.8750%,lr:0.0001\n",
      "Episode:74, Validation Loss:3.8438088959082964e-06,Acc:99.8750%,lr:0.0001\n",
      "Episode:75, Validation Loss:3.3006712328642606e-06,Acc:99.8750%,lr:0.0001\n",
      "Episode:76, Validation Loss:3.6211832193657755e-06,Acc:99.8750%,lr:0.0001\n",
      "Episode:77, Validation Loss:3.9961336879059675e-06,Acc:99.8500%,lr:0.0001\n",
      "Episode:78, Validation Loss:3.878600778989494e-06,Acc:99.8500%,lr:0.0001\n",
      "Episode:79, Validation Loss:3.549827932147309e-06,Acc:99.9000%,lr:0.0001\n",
      "Episode:80, Validation Loss:3.2676564005669207e-06,Acc:99.9250%,lr:0.0001\n",
      "Episode:81, Validation Loss:3.3603617921471596e-06,Acc:99.9250%,lr:0.0001\n",
      "Episode:82, Validation Loss:3.6090830690227447e-06,Acc:99.8750%,lr:0.0001\n",
      "Episode:83, Validation Loss:3.603622841183096e-06,Acc:99.8750%,lr:0.0001\n",
      "Episode:84, Validation Loss:3.333978063892573e-06,Acc:99.9250%,lr:0.0001\n",
      "Episode:85, Validation Loss:3.2696634880267086e-06,Acc:99.9000%,lr:0.0001\n",
      "Epoch    85: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Episode:86, Validation Loss:3.519072022754699e-06,Acc:99.8750%,lr:1e-05\n",
      "Episode:87, Validation Loss:3.4816324478015302e-06,Acc:99.9000%,lr:1e-05\n",
      "Episode:88, Validation Loss:3.52196313906461e-06,Acc:99.9250%,lr:1e-05\n",
      "Episode:89, Validation Loss:3.5588942118920386e-06,Acc:99.9250%,lr:1e-05\n",
      "Episode:90, Validation Loss:3.5002001095563175e-06,Acc:99.9250%,lr:1e-05\n",
      "Episode:91, Validation Loss:3.5024016397073867e-06,Acc:99.9250%,lr:1e-05\n",
      "Episode:92, Validation Loss:3.4926399239338935e-06,Acc:99.9250%,lr:1e-05\n",
      "Episode:93, Validation Loss:3.4654989140108227e-06,Acc:99.9250%,lr:1e-05\n",
      "Episode:94, Validation Loss:3.4215977066196503e-06,Acc:99.9250%,lr:1e-05\n",
      "Episode:95, Validation Loss:3.4045284846797585e-06,Acc:99.9250%,lr:1e-05\n",
      "Episode:96, Validation Loss:3.5193359362892806e-06,Acc:99.9250%,lr:1e-05\n",
      "Episode:97, Validation Loss:3.515807562507689e-06,Acc:99.9250%,lr:1e-05\n",
      "Episode:98, Validation Loss:3.4954219590872526e-06,Acc:99.9250%,lr:1e-05\n",
      "Episode:99, Validation Loss:3.4938816097564993e-06,Acc:99.9250%,lr:1e-05\n",
      "Episode:100, Validation Loss:3.5202312865294515e-06,Acc:99.9250%,lr:1e-05\n",
      "Episode:101, Validation Loss:3.5722493776120247e-06,Acc:99.9250%,lr:1e-05\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-06.\n",
      "===================Best Fold:8 Saved, Acc:0.9995==================\n",
      "======================================================\n",
      "Episode:1, Validation Loss:0.007857948899269104,Acc:10.0000%,lr:0.001\n",
      "Episode:2, Validation Loss:0.013089083433151245,Acc:10.0000%,lr:0.001\n",
      "Episode:3, Validation Loss:0.01373197865486145,Acc:10.0000%,lr:0.001\n",
      "Episode:4, Validation Loss:0.008360159754753113,Acc:12.2000%,lr:0.001\n",
      "Episode:5, Validation Loss:0.0022619975209236145,Acc:48.3500%,lr:0.001\n",
      "Episode:6, Validation Loss:0.00036421078816056254,Acc:87.1000%,lr:0.001\n",
      "Episode:7, Validation Loss:0.00027056828513741493,Acc:91.9750%,lr:0.001\n",
      "Episode:8, Validation Loss:5.31801322940737e-05,Acc:98.3750%,lr:0.001\n",
      "Episode:9, Validation Loss:1.744289160706103e-05,Acc:99.4250%,lr:0.001\n",
      "Episode:10, Validation Loss:2.3983438964933156e-05,Acc:99.3000%,lr:0.001\n",
      "Episode:11, Validation Loss:1.6707659931853414e-05,Acc:99.5500%,lr:0.001\n",
      "Episode:12, Validation Loss:2.3939976701512933e-05,Acc:99.4250%,lr:0.001\n",
      "Episode:13, Validation Loss:1.2549918377771974e-05,Acc:99.7000%,lr:0.001\n",
      "Episode:14, Validation Loss:1.4254174893721937e-05,Acc:99.6750%,lr:0.001\n",
      "Episode:15, Validation Loss:1.4764032559469341e-05,Acc:99.6250%,lr:0.001\n",
      "Episode:16, Validation Loss:2.066042250953615e-05,Acc:99.5500%,lr:0.001\n",
      "Episode:17, Validation Loss:1.4072189223952591e-05,Acc:99.6750%,lr:0.001\n",
      "Episode:18, Validation Loss:1.5370005392469464e-05,Acc:99.5250%,lr:0.001\n",
      "Episode:19, Validation Loss:1.6265475773252546e-05,Acc:99.5500%,lr:0.001\n",
      "Episode:20, Validation Loss:1.9956921343691648e-05,Acc:99.4750%,lr:0.001\n",
      "Episode:21, Validation Loss:1.5339518897235394e-05,Acc:99.6750%,lr:0.001\n",
      "Episode:22, Validation Loss:2.4128796998411417e-05,Acc:99.3250%,lr:0.001\n",
      "Episode:23, Validation Loss:1.2025728123262524e-05,Acc:99.7250%,lr:0.001\n",
      "Episode:24, Validation Loss:1.4786183019168675e-05,Acc:99.6750%,lr:0.001\n",
      "Episode:25, Validation Loss:1.3862641877494753e-05,Acc:99.6250%,lr:0.001\n",
      "Episode:26, Validation Loss:1.1207825504243374e-05,Acc:99.7750%,lr:0.001\n",
      "Episode:27, Validation Loss:1.1229508207179605e-05,Acc:99.6750%,lr:0.001\n",
      "Episode:28, Validation Loss:1.1743089882656931e-05,Acc:99.6750%,lr:0.001\n",
      "Episode:29, Validation Loss:2.2791889612562953e-05,Acc:99.4250%,lr:0.001\n",
      "Episode:30, Validation Loss:1.4373491983860731e-05,Acc:99.6000%,lr:0.001\n",
      "Episode:31, Validation Loss:1.9462337251752615e-05,Acc:99.6250%,lr:0.001\n",
      "Episode:32, Validation Loss:1.741193071939051e-05,Acc:99.5750%,lr:0.001\n",
      "Episode:33, Validation Loss:1.6249357839114964e-05,Acc:99.6500%,lr:0.001\n",
      "Episode:34, Validation Loss:1.609320833813399e-05,Acc:99.5500%,lr:0.001\n",
      "Episode:35, Validation Loss:1.7205259762704374e-05,Acc:99.5750%,lr:0.001\n",
      "Episode:36, Validation Loss:1.0462512727826834e-05,Acc:99.7750%,lr:0.001\n",
      "Episode:37, Validation Loss:1.6826158156618476e-05,Acc:99.5750%,lr:0.001\n",
      "Episode:38, Validation Loss:1.78695737849921e-05,Acc:99.4250%,lr:0.001\n",
      "Episode:39, Validation Loss:2.5895075872540474e-05,Acc:99.3750%,lr:0.001\n",
      "Episode:40, Validation Loss:1.8465657951310276e-05,Acc:99.6500%,lr:0.001\n",
      "Episode:41, Validation Loss:2.6375967892818154e-05,Acc:99.4500%,lr:0.001\n",
      "Episode:42, Validation Loss:1.7964245285838844e-05,Acc:99.5750%,lr:0.001\n",
      "Episode:43, Validation Loss:1.7929639201611282e-05,Acc:99.5750%,lr:0.001\n",
      "Episode:44, Validation Loss:1.643629453610629e-05,Acc:99.5750%,lr:0.001\n",
      "Episode:45, Validation Loss:1.4559938572347164e-05,Acc:99.7000%,lr:0.001\n",
      "Episode:46, Validation Loss:1.2330256868153811e-05,Acc:99.7500%,lr:0.001\n",
      "Episode:47, Validation Loss:1.2398936436511576e-05,Acc:99.7000%,lr:0.001\n",
      "Episode:48, Validation Loss:1.8016872520092873e-05,Acc:99.6000%,lr:0.001\n",
      "Episode:49, Validation Loss:2.1562367444857955e-05,Acc:99.5000%,lr:0.001\n",
      "Episode:50, Validation Loss:1.7888871603645384e-05,Acc:99.5750%,lr:0.001\n",
      "Episode:51, Validation Loss:1.554728823248297e-05,Acc:99.6250%,lr:0.001\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Episode:52, Validation Loss:1.2814194429665804e-05,Acc:99.6750%,lr:0.0001\n",
      "Episode:53, Validation Loss:9.788508934434504e-06,Acc:99.6750%,lr:0.0001\n",
      "Episode:54, Validation Loss:9.542505082208663e-06,Acc:99.7750%,lr:0.0001\n",
      "Episode:55, Validation Loss:9.950674138963222e-06,Acc:99.7750%,lr:0.0001\n",
      "Episode:56, Validation Loss:9.70590824726969e-06,Acc:99.7750%,lr:0.0001\n",
      "Episode:57, Validation Loss:9.842766914516688e-06,Acc:99.7750%,lr:0.0001\n",
      "Episode:58, Validation Loss:9.345138096250594e-06,Acc:99.7750%,lr:0.0001\n",
      "Episode:59, Validation Loss:1.0072401142679156e-05,Acc:99.7750%,lr:0.0001\n",
      "Episode:60, Validation Loss:9.904555219691246e-06,Acc:99.7250%,lr:0.0001\n",
      "Episode:61, Validation Loss:9.74801107076928e-06,Acc:99.7750%,lr:0.0001\n",
      "Episode:62, Validation Loss:1.0350840340834112e-05,Acc:99.7500%,lr:0.0001\n",
      "Episode:63, Validation Loss:9.972818079404532e-06,Acc:99.7750%,lr:0.0001\n",
      "Episode:64, Validation Loss:9.91607562173158e-06,Acc:99.8000%,lr:0.0001\n",
      "Episode:65, Validation Loss:8.888201846275478e-06,Acc:99.8000%,lr:0.0001\n",
      "Episode:66, Validation Loss:9.121860086452217e-06,Acc:99.7750%,lr:0.0001\n",
      "Episode:67, Validation Loss:9.02094878256321e-06,Acc:99.7750%,lr:0.0001\n",
      "Episode:68, Validation Loss:9.375943511258811e-06,Acc:99.7250%,lr:0.0001\n",
      "Episode:69, Validation Loss:8.96212091902271e-06,Acc:99.7500%,lr:0.0001\n",
      "Episode:70, Validation Loss:8.854772022459657e-06,Acc:99.7500%,lr:0.0001\n",
      "Episode:71, Validation Loss:9.006931446492673e-06,Acc:99.7750%,lr:0.0001\n",
      "Episode:72, Validation Loss:9.038821328431369e-06,Acc:99.7500%,lr:0.0001\n",
      "Episode:73, Validation Loss:1.000574103090912e-05,Acc:99.7500%,lr:0.0001\n",
      "Episode:74, Validation Loss:1.0317547537852079e-05,Acc:99.7750%,lr:0.0001\n",
      "Episode:75, Validation Loss:1.0895855200942606e-05,Acc:99.7000%,lr:0.0001\n",
      "Episode:76, Validation Loss:1.006966782733798e-05,Acc:99.7000%,lr:0.0001\n",
      "Episode:77, Validation Loss:1.0069911426398903e-05,Acc:99.7500%,lr:0.0001\n",
      "Episode:78, Validation Loss:9.294052520999685e-06,Acc:99.7750%,lr:0.0001\n",
      "Episode:79, Validation Loss:8.954642806202173e-06,Acc:99.7750%,lr:0.0001\n",
      "Episode:80, Validation Loss:1.018883177312091e-05,Acc:99.7500%,lr:0.0001\n",
      "Episode:81, Validation Loss:9.975577355362475e-06,Acc:99.8000%,lr:0.0001\n",
      "Episode:82, Validation Loss:1.0061703622341156e-05,Acc:99.7750%,lr:0.0001\n",
      "Episode:83, Validation Loss:1.0333259881008417e-05,Acc:99.7500%,lr:0.0001\n",
      "Episode:84, Validation Loss:1.0377982864156366e-05,Acc:99.7750%,lr:0.0001\n",
      "Episode:85, Validation Loss:9.611334186047316e-06,Acc:99.8000%,lr:0.0001\n",
      "Epoch    85: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Episode:86, Validation Loss:9.700169146526604e-06,Acc:99.7750%,lr:1e-05\n",
      "Episode:87, Validation Loss:1.01776416413486e-05,Acc:99.7750%,lr:1e-05\n",
      "Episode:88, Validation Loss:1.0320190456695855e-05,Acc:99.7750%,lr:1e-05\n",
      "Episode:89, Validation Loss:1.0184444778133184e-05,Acc:99.7750%,lr:1e-05\n",
      "Episode:90, Validation Loss:1.007515931269154e-05,Acc:99.7750%,lr:1e-05\n",
      "Episode:91, Validation Loss:1.0007920034695417e-05,Acc:99.7750%,lr:1e-05\n",
      "Episode:92, Validation Loss:1.0098040278535336e-05,Acc:99.7750%,lr:1e-05\n",
      "Episode:93, Validation Loss:1.0288188932463526e-05,Acc:99.7750%,lr:1e-05\n",
      "Episode:94, Validation Loss:1.0249486018437892e-05,Acc:99.7750%,lr:1e-05\n",
      "Episode:95, Validation Loss:1.02553375181742e-05,Acc:99.7750%,lr:1e-05\n",
      "Episode:96, Validation Loss:1.0023193259257824e-05,Acc:99.7750%,lr:1e-05\n",
      "Episode:97, Validation Loss:1.0033913189545273e-05,Acc:99.7750%,lr:1e-05\n",
      "Episode:98, Validation Loss:1.0137432487681508e-05,Acc:99.7750%,lr:1e-05\n",
      "Episode:99, Validation Loss:1.0214203386567534e-05,Acc:99.7750%,lr:1e-05\n",
      "Episode:100, Validation Loss:1.0079582687467337e-05,Acc:99.7750%,lr:1e-05\n",
      "Episode:101, Validation Loss:1.0223672434221954e-05,Acc:99.7750%,lr:1e-05\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-06.\n",
      "===================Best Fold:9 Saved, Acc:0.998==================\n",
      "======================================================\n",
      "Episode:1, Validation Loss:0.0024038628935813904,Acc:34.4500%,lr:0.001\n",
      "Episode:2, Validation Loss:0.0034710950255393983,Acc:21.5500%,lr:0.001\n",
      "Episode:3, Validation Loss:0.0016220207214355468,Acc:62.9500%,lr:0.001\n",
      "Episode:4, Validation Loss:0.002232177734375,Acc:52.2250%,lr:0.001\n",
      "Episode:5, Validation Loss:0.0007599192783236503,Acc:76.4250%,lr:0.001\n",
      "Episode:6, Validation Loss:0.00020817375555634497,Acc:93.5750%,lr:0.001\n",
      "Episode:7, Validation Loss:4.9587991088628766e-05,Acc:98.3750%,lr:0.001\n",
      "Episode:8, Validation Loss:1.7739803995937108e-05,Acc:99.4500%,lr:0.001\n",
      "Episode:9, Validation Loss:8.616070030257106e-06,Acc:99.7250%,lr:0.001\n",
      "Episode:10, Validation Loss:1.92466014996171e-05,Acc:99.4500%,lr:0.001\n",
      "Episode:11, Validation Loss:1.2016414431855083e-05,Acc:99.6250%,lr:0.001\n",
      "Episode:12, Validation Loss:1.6416801605373622e-05,Acc:99.5000%,lr:0.001\n",
      "Episode:13, Validation Loss:9.257275960408152e-06,Acc:99.7000%,lr:0.001\n",
      "Episode:14, Validation Loss:1.1792002711445094e-05,Acc:99.5500%,lr:0.001\n",
      "Episode:15, Validation Loss:7.841240032576025e-06,Acc:99.6500%,lr:0.001\n",
      "Episode:16, Validation Loss:1.4531699591316283e-05,Acc:99.4750%,lr:0.001\n",
      "Episode:17, Validation Loss:1.0778017807751893e-05,Acc:99.6250%,lr:0.001\n",
      "Episode:18, Validation Loss:1.625420874916017e-05,Acc:99.5250%,lr:0.001\n",
      "Episode:19, Validation Loss:2.0877060014754532e-05,Acc:99.4500%,lr:0.001\n",
      "Episode:20, Validation Loss:1.3318921672180295e-05,Acc:99.6250%,lr:0.001\n",
      "Episode:21, Validation Loss:1.4028184115886688e-05,Acc:99.6500%,lr:0.001\n",
      "Episode:22, Validation Loss:7.286415901035071e-06,Acc:99.7000%,lr:0.001\n",
      "Episode:23, Validation Loss:8.14498751424253e-06,Acc:99.7250%,lr:0.001\n",
      "Episode:24, Validation Loss:7.66098138410598e-06,Acc:99.7000%,lr:0.001\n",
      "Episode:25, Validation Loss:1.0258012800477445e-05,Acc:99.6750%,lr:0.001\n",
      "Episode:26, Validation Loss:9.288631728850305e-06,Acc:99.7000%,lr:0.001\n",
      "Episode:27, Validation Loss:1.8721033935435116e-05,Acc:99.5000%,lr:0.001\n",
      "Episode:28, Validation Loss:1.0666451416909694e-05,Acc:99.7000%,lr:0.001\n",
      "Episode:29, Validation Loss:1.6685708542354404e-05,Acc:99.5250%,lr:0.001\n",
      "Episode:30, Validation Loss:1.0208771331235766e-05,Acc:99.7250%,lr:0.001\n",
      "Episode:31, Validation Loss:1.2431190814822912e-05,Acc:99.6500%,lr:0.001\n",
      "Episode:32, Validation Loss:1.1964714736677706e-05,Acc:99.6250%,lr:0.001\n",
      "Episode:33, Validation Loss:1.4180931961163879e-05,Acc:99.6000%,lr:0.001\n",
      "Episode:34, Validation Loss:1.964504143688828e-05,Acc:99.3250%,lr:0.001\n",
      "Episode:35, Validation Loss:1.4395070495083929e-05,Acc:99.7500%,lr:0.001\n",
      "Episode:36, Validation Loss:1.1286931112408638e-05,Acc:99.7000%,lr:0.001\n",
      "Episode:37, Validation Loss:1.1086447979323566e-05,Acc:99.6000%,lr:0.001\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Episode:38, Validation Loss:8.43301648274064e-06,Acc:99.6500%,lr:0.0001\n",
      "Episode:39, Validation Loss:9.813168901018799e-06,Acc:99.7000%,lr:0.0001\n",
      "Episode:40, Validation Loss:9.356517111882567e-06,Acc:99.7250%,lr:0.0001\n",
      "Episode:41, Validation Loss:9.84295317903161e-06,Acc:99.7000%,lr:0.0001\n",
      "Episode:42, Validation Loss:9.018443059176206e-06,Acc:99.7250%,lr:0.0001\n",
      "Episode:43, Validation Loss:8.794061373919248e-06,Acc:99.7000%,lr:0.0001\n",
      "Episode:44, Validation Loss:9.570191148668528e-06,Acc:99.6750%,lr:0.0001\n",
      "Episode:45, Validation Loss:9.953548898920417e-06,Acc:99.7000%,lr:0.0001\n",
      "Episode:46, Validation Loss:9.3253031373024e-06,Acc:99.7000%,lr:0.0001\n",
      "Episode:47, Validation Loss:1.057389285415411e-05,Acc:99.6750%,lr:0.0001\n",
      "Episode:48, Validation Loss:1.0128831258043648e-05,Acc:99.6750%,lr:0.0001\n",
      "Episode:49, Validation Loss:9.316637879237532e-06,Acc:99.6750%,lr:0.0001\n",
      "Episode:50, Validation Loss:9.028764092363417e-06,Acc:99.7000%,lr:0.0001\n",
      "Episode:51, Validation Loss:9.877911303192376e-06,Acc:99.6750%,lr:0.0001\n",
      "Episode:52, Validation Loss:1.0185835068114103e-05,Acc:99.7000%,lr:0.0001\n",
      "Episode:53, Validation Loss:1.0110657196491957e-05,Acc:99.7250%,lr:0.0001\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Episode:54, Validation Loss:9.494481491856278e-06,Acc:99.7500%,lr:1e-05\n",
      "Episode:55, Validation Loss:9.558801189996302e-06,Acc:99.7250%,lr:1e-05\n",
      "Episode:56, Validation Loss:9.64531651698053e-06,Acc:99.7000%,lr:1e-05\n",
      "Episode:57, Validation Loss:9.804214932955802e-06,Acc:99.7000%,lr:1e-05\n",
      "Episode:58, Validation Loss:9.73646284546703e-06,Acc:99.7000%,lr:1e-05\n",
      "Episode:59, Validation Loss:9.720273897983135e-06,Acc:99.7000%,lr:1e-05\n",
      "Episode:60, Validation Loss:9.891452384181321e-06,Acc:99.7000%,lr:1e-05\n",
      "Episode:61, Validation Loss:9.713274310342968e-06,Acc:99.7000%,lr:1e-05\n",
      "Episode:62, Validation Loss:9.513365454040468e-06,Acc:99.7000%,lr:1e-05\n",
      "Episode:63, Validation Loss:9.494319092482329e-06,Acc:99.7000%,lr:1e-05\n",
      "Episode:64, Validation Loss:9.4494050135836e-06,Acc:99.7000%,lr:1e-05\n",
      "Episode:65, Validation Loss:9.574637515470385e-06,Acc:99.7000%,lr:1e-05\n",
      "Episode:66, Validation Loss:9.474503924138845e-06,Acc:99.7000%,lr:1e-05\n",
      "Episode:67, Validation Loss:9.489602060057223e-06,Acc:99.7000%,lr:1e-05\n",
      "Episode:68, Validation Loss:9.647791739553212e-06,Acc:99.7000%,lr:1e-05\n",
      "Episode:69, Validation Loss:9.80219105258584e-06,Acc:99.7000%,lr:1e-05\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-06.\n",
      "===================Best Fold:10 Saved, Acc:0.9975==================\n",
      "======================================================\n",
      "Episode:1, Validation Loss:0.0018637419939041137,Acc:30.6500%,lr:0.001\n",
      "Episode:2, Validation Loss:0.0021552808880805968,Acc:34.8250%,lr:0.001\n",
      "Episode:3, Validation Loss:0.0008380444496870041,Acc:75.2500%,lr:0.001\n",
      "Episode:4, Validation Loss:0.00020933052711188794,Acc:92.3000%,lr:0.001\n",
      "Episode:5, Validation Loss:0.00011902032606303692,Acc:95.6500%,lr:0.001\n",
      "Episode:6, Validation Loss:1.4715458964928985e-05,Acc:99.5000%,lr:0.001\n",
      "Episode:7, Validation Loss:2.795095555484295e-05,Acc:98.9250%,lr:0.001\n",
      "Episode:8, Validation Loss:1.2302922317758202e-05,Acc:99.6250%,lr:0.001\n",
      "Episode:9, Validation Loss:8.023525355383754e-06,Acc:99.6500%,lr:0.001\n",
      "Episode:10, Validation Loss:2.4073131557088345e-05,Acc:99.1000%,lr:0.001\n",
      "Episode:11, Validation Loss:5.607769009657204e-06,Acc:99.8000%,lr:0.001\n",
      "Episode:12, Validation Loss:6.036611273884773e-06,Acc:99.7750%,lr:0.001\n",
      "Episode:13, Validation Loss:8.491267915815115e-06,Acc:99.8250%,lr:0.001\n",
      "Episode:14, Validation Loss:7.4756805552169685e-06,Acc:99.7250%,lr:0.001\n",
      "Episode:15, Validation Loss:2.0678892731666566e-05,Acc:99.4000%,lr:0.001\n",
      "Episode:16, Validation Loss:1.9301958847790958e-05,Acc:99.6000%,lr:0.001\n",
      "Episode:17, Validation Loss:6.874773418530822e-06,Acc:99.7250%,lr:0.001\n",
      "Episode:18, Validation Loss:8.758854120969772e-06,Acc:99.6250%,lr:0.001\n",
      "Episode:19, Validation Loss:8.793853223323822e-06,Acc:99.6000%,lr:0.001\n",
      "Episode:20, Validation Loss:8.448877022601665e-06,Acc:99.6500%,lr:0.001\n",
      "Episode:21, Validation Loss:3.923982963897288e-06,Acc:99.8750%,lr:0.001\n",
      "Episode:22, Validation Loss:1.5070473309606314e-05,Acc:99.5750%,lr:0.001\n",
      "Episode:23, Validation Loss:8.421645383350551e-06,Acc:99.6500%,lr:0.001\n",
      "Episode:24, Validation Loss:4.057459358591586e-06,Acc:99.8000%,lr:0.001\n",
      "Episode:25, Validation Loss:8.890201686881483e-06,Acc:99.6000%,lr:0.001\n",
      "Episode:26, Validation Loss:1.9330177688971162e-05,Acc:99.5000%,lr:0.001\n",
      "Episode:27, Validation Loss:1.0901925037615002e-05,Acc:99.6250%,lr:0.001\n",
      "Episode:28, Validation Loss:6.69122370891273e-06,Acc:99.8000%,lr:0.001\n",
      "Episode:29, Validation Loss:7.526458939537406e-06,Acc:99.6500%,lr:0.001\n",
      "Episode:30, Validation Loss:1.2748443870805204e-05,Acc:99.6500%,lr:0.001\n",
      "Episode:31, Validation Loss:1.3648443389683962e-05,Acc:99.5250%,lr:0.001\n",
      "Episode:32, Validation Loss:6.599111016839743e-06,Acc:99.7750%,lr:0.001\n",
      "Episode:33, Validation Loss:9.586709085851907e-06,Acc:99.6750%,lr:0.001\n",
      "Episode:34, Validation Loss:9.96396888513118e-06,Acc:99.6500%,lr:0.001\n",
      "Episode:35, Validation Loss:8.616152103058994e-06,Acc:99.6750%,lr:0.001\n",
      "Episode:36, Validation Loss:1.6018553636968137e-05,Acc:99.6250%,lr:0.001\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Episode:37, Validation Loss:1.2249690247699618e-05,Acc:99.5750%,lr:0.0001\n",
      "Episode:38, Validation Loss:8.164588245563208e-06,Acc:99.6750%,lr:0.0001\n",
      "Episode:39, Validation Loss:9.52148379292339e-06,Acc:99.6250%,lr:0.0001\n",
      "Episode:40, Validation Loss:7.8611820936203e-06,Acc:99.6250%,lr:0.0001\n",
      "Episode:41, Validation Loss:6.0537414392456415e-06,Acc:99.7250%,lr:0.0001\n",
      "Episode:42, Validation Loss:5.517012090422213e-06,Acc:99.7000%,lr:0.0001\n",
      "Episode:43, Validation Loss:8.678369224071502e-06,Acc:99.6750%,lr:0.0001\n",
      "Episode:44, Validation Loss:6.904651643708348e-06,Acc:99.6750%,lr:0.0001\n",
      "Episode:45, Validation Loss:6.323039066046476e-06,Acc:99.7500%,lr:0.0001\n",
      "Episode:46, Validation Loss:7.27265328168869e-06,Acc:99.6750%,lr:0.0001\n",
      "Episode:47, Validation Loss:8.631871081888676e-06,Acc:99.7250%,lr:0.0001\n",
      "Episode:48, Validation Loss:6.322477711364627e-06,Acc:99.7250%,lr:0.0001\n",
      "Episode:49, Validation Loss:9.151832200586796e-06,Acc:99.7000%,lr:0.0001\n",
      "Episode:50, Validation Loss:7.058143964968622e-06,Acc:99.7250%,lr:0.0001\n",
      "Episode:51, Validation Loss:5.7931797346100215e-06,Acc:99.7500%,lr:0.0001\n",
      "Episode:52, Validation Loss:7.790108094923198e-06,Acc:99.6750%,lr:0.0001\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Episode:53, Validation Loss:6.17517123464495e-06,Acc:99.6750%,lr:1e-05\n",
      "Episode:54, Validation Loss:6.880804663524031e-06,Acc:99.6750%,lr:1e-05\n",
      "Episode:55, Validation Loss:7.66364752780646e-06,Acc:99.6750%,lr:1e-05\n",
      "Episode:56, Validation Loss:7.886275532655418e-06,Acc:99.7000%,lr:1e-05\n",
      "Episode:57, Validation Loss:8.123615523800254e-06,Acc:99.7000%,lr:1e-05\n",
      "Episode:58, Validation Loss:8.367755450308322e-06,Acc:99.7000%,lr:1e-05\n",
      "Episode:59, Validation Loss:8.283805684186518e-06,Acc:99.7000%,lr:1e-05\n",
      "Episode:60, Validation Loss:8.333804435096682e-06,Acc:99.7000%,lr:1e-05\n",
      "Episode:61, Validation Loss:8.537963614799082e-06,Acc:99.7000%,lr:1e-05\n",
      "Episode:62, Validation Loss:8.431110880337656e-06,Acc:99.7000%,lr:1e-05\n",
      "Episode:63, Validation Loss:8.255160879343748e-06,Acc:99.7000%,lr:1e-05\n",
      "Episode:64, Validation Loss:8.17388331051916e-06,Acc:99.7000%,lr:1e-05\n",
      "Episode:65, Validation Loss:8.14999989233911e-06,Acc:99.7000%,lr:1e-05\n",
      "Episode:66, Validation Loss:8.35209246724844e-06,Acc:99.7000%,lr:1e-05\n",
      "Episode:67, Validation Loss:8.365828660316766e-06,Acc:99.6750%,lr:1e-05\n",
      "Episode:68, Validation Loss:8.36997537408024e-06,Acc:99.6750%,lr:1e-05\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-06.\n",
      "===================Best Fold:11 Saved, Acc:0.99875==================\n",
      "======================================================\n",
      "Episode:1, Validation Loss:0.0040180465579032895,Acc:10.3000%,lr:0.001\n",
      "Episode:2, Validation Loss:0.003652876853942871,Acc:31.8000%,lr:0.001\n",
      "Episode:3, Validation Loss:0.0032916496992111207,Acc:34.9500%,lr:0.001\n",
      "Episode:4, Validation Loss:0.0014512334764003754,Acc:55.7000%,lr:0.001\n",
      "Episode:5, Validation Loss:0.00022161080129444598,Acc:92.4750%,lr:0.001\n",
      "Episode:6, Validation Loss:6.284000724554062e-05,Acc:97.8250%,lr:0.001\n",
      "Episode:7, Validation Loss:9.593355818651616e-06,Acc:99.7000%,lr:0.001\n",
      "Episode:8, Validation Loss:1.1790437041781843e-05,Acc:99.5750%,lr:0.001\n",
      "Episode:9, Validation Loss:7.118627632735297e-06,Acc:99.7750%,lr:0.001\n",
      "Episode:10, Validation Loss:1.754932862240821e-05,Acc:99.3000%,lr:0.001\n",
      "Episode:11, Validation Loss:2.0950098405592144e-05,Acc:99.3500%,lr:0.001\n",
      "Episode:12, Validation Loss:1.1612236849032342e-05,Acc:99.6750%,lr:0.001\n",
      "Episode:13, Validation Loss:1.1296710814349353e-05,Acc:99.8000%,lr:0.001\n",
      "Episode:14, Validation Loss:2.9069080483168365e-06,Acc:99.9250%,lr:0.001\n",
      "Episode:15, Validation Loss:9.069144609384238e-06,Acc:99.7000%,lr:0.001\n",
      "Episode:16, Validation Loss:8.93965782597661e-06,Acc:99.7750%,lr:0.001\n",
      "Episode:17, Validation Loss:7.650667801499367e-06,Acc:99.8000%,lr:0.001\n",
      "Episode:18, Validation Loss:6.931222043931484e-06,Acc:99.8000%,lr:0.001\n",
      "Episode:19, Validation Loss:8.298416039906442e-06,Acc:99.7500%,lr:0.001\n",
      "Episode:20, Validation Loss:2.499032416380942e-06,Acc:99.9500%,lr:0.001\n",
      "Episode:21, Validation Loss:8.22309055365622e-06,Acc:99.8250%,lr:0.001\n",
      "Episode:22, Validation Loss:1.1633397662080824e-05,Acc:99.6500%,lr:0.001\n",
      "Episode:23, Validation Loss:4.092127899639308e-06,Acc:99.8750%,lr:0.001\n",
      "Episode:24, Validation Loss:1.236663747113198e-05,Acc:99.6750%,lr:0.001\n",
      "Episode:25, Validation Loss:6.390335795003921e-06,Acc:99.8000%,lr:0.001\n",
      "Episode:26, Validation Loss:1.5145419049076736e-05,Acc:99.4000%,lr:0.001\n",
      "Episode:27, Validation Loss:5.4430210730060935e-06,Acc:99.8250%,lr:0.001\n",
      "Episode:28, Validation Loss:9.143733332166447e-06,Acc:99.7000%,lr:0.001\n",
      "Episode:29, Validation Loss:1.0858182038646191e-05,Acc:99.6250%,lr:0.001\n",
      "Episode:30, Validation Loss:3.786871166084893e-06,Acc:99.8750%,lr:0.001\n",
      "Episode:31, Validation Loss:4.933635485940613e-06,Acc:99.8500%,lr:0.001\n",
      "Episode:32, Validation Loss:2.3050191579386593e-05,Acc:99.3000%,lr:0.001\n",
      "Episode:33, Validation Loss:7.740762084722519e-06,Acc:99.7500%,lr:0.001\n",
      "Episode:34, Validation Loss:1.2539796764031053e-05,Acc:99.6250%,lr:0.001\n",
      "Episode:35, Validation Loss:6.230510829482228e-06,Acc:99.8500%,lr:0.001\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Episode:36, Validation Loss:7.273778726812452e-06,Acc:99.8250%,lr:0.0001\n",
      "Episode:37, Validation Loss:4.920346196740865e-06,Acc:99.8750%,lr:0.0001\n",
      "Episode:38, Validation Loss:5.643204873194918e-06,Acc:99.8500%,lr:0.0001\n",
      "Episode:39, Validation Loss:5.990749894408509e-06,Acc:99.8000%,lr:0.0001\n",
      "Episode:40, Validation Loss:5.578143638558686e-06,Acc:99.7750%,lr:0.0001\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    epochs = 200\n",
    "    ensemble_models = []\n",
    "    lr = 1e-3\n",
    "    val_period = 1\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "#     criterion_b = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    print(\"Fold:\",len(train_loaders))\n",
    "    \n",
    "    for fold in range(len(train_loaders)):\n",
    "        train_loader = train_loaders[fold]\n",
    "        val_loader = val_loaders[fold]\n",
    "        \n",
    "        model = get_model()\n",
    "            \n",
    "        max_acc = 0\n",
    "        min_loss = 10000\n",
    "        best_model_dict = None\n",
    "        data_num = 0\n",
    "        loss_avg = 0\n",
    "\n",
    "#         optimizer = torch.optim.SGD(model.parameters(),lr=lr)\n",
    "#         optimizer = torch.optim.RMSprop(model.parameters(),lr=lr)\n",
    "        optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "#         optimizer = torch.optim.Adagrad(model.parameters(),lr=lr)\n",
    "#         lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=period,T_mult=1,eta_min=1e-5) #original \n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, patience=15,factor=0.1)\n",
    "        \n",
    "        for ep in range(0,epochs+1):\n",
    "            model.train()\n",
    "            for idx, data in enumerate(train_loader):\n",
    "                img, target = data\n",
    "                img, target = img.to(device), target.to(device,dtype=torch.long)\n",
    "                \n",
    "#                 print(np.shape(img),np.shape(target_b),np.shape(target)) #Tensor(4,1,28,28), Tensor(4)\n",
    "#                 print(np.max(img.cpu().numpy()),np.min(img.cpu().numpy())) #1.0 0.0\n",
    "                pred = model(img)\n",
    "#                 print(pred.size())   #(32,10)\n",
    "#                 print(target.size()) #(32,)\n",
    "                  \n",
    "                ###Input shape: input:(batch_num,1), target:(batch_num,a int 0 or 1) for CSE LOSS, target:(batch_num,1) for BCE loss\n",
    "#                 loss_b = criterion_b(pred_b,target_b) \n",
    "\n",
    "                ###Input shape: input:(batch_num,10), target:(batch_num,a int between 0~10)\n",
    "                loss = criterion(pred,target)\n",
    "                \n",
    "                loss_avg += loss.item()\n",
    "                data_num += img.size(0)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            ###Cosine annealing\n",
    "#             lr_scheduler.step()\n",
    "\n",
    "            ###Evaluate Train Loss \n",
    "#             if ep%2 == 0:\n",
    "#                 loss_avg /= data_num\n",
    "#                 print(\"Ep:{}, loss:{}, lr:{}\".format(ep, loss_avg,optimizer.param_groups[0]['lr']))\n",
    "#                 loss_avg = 0\n",
    "#                 data_num = 0\n",
    "\n",
    "            ###Validation\n",
    "            if ep!=0 and ep%val_period == 0:\n",
    "                model.eval()\n",
    "                acc = 0\n",
    "                val_loss = 0\n",
    "                data_num  = 0\n",
    "                with torch.no_grad():\n",
    "                    for idx, data in enumerate(val_loader):\n",
    "                        img, target = data\n",
    "                        img, target = img.to(device), target.to(device,dtype=torch.long)\n",
    "                        pred = model(img)\n",
    "\n",
    "                        val_loss += criterion(pred, target).item()\n",
    "                        \n",
    "                        # print(pred) \n",
    "                        _,pred_class = torch.max(pred.data, 1)\n",
    "    #                     print(pred_class)\n",
    "                        acc += (pred_class == target).sum().item()\n",
    "                        data_num += img.size(0)\n",
    "\n",
    "                acc /= data_num\n",
    "                val_loss /= data_num\n",
    "\n",
    "                ###Plateau\n",
    "                lr_scheduler.step(val_loss)\n",
    "                if optimizer.param_groups[0]['lr'] < 1e-5:\n",
    "                    break                    \n",
    "\n",
    "                if acc >= max_acc:\n",
    "                    max_acc = acc\n",
    "                    best_model_dict = model.state_dict()\n",
    "                \n",
    "                if val_loss <= min_loss:\n",
    "                    min_loss = val_loss\n",
    "#                     best_model_dict = model.state_dict()\n",
    "                \n",
    "                print(\"Episode:{}, Validation Loss:{},Acc:{:.4f}%,lr:{}\"\n",
    "                      .format(ep,val_loss,acc*100,optimizer.param_groups[0]['lr']))\n",
    "            \n",
    "#             if max_acc>0.995 and ep!=0 and ep%10 == 0:\n",
    "#                 torch.save(best_model_dict, \"./Kmnist_saved_model/tmp_Fold{}_acc{:.4f}\".format(fold,max_acc*1e2))\n",
    "    \n",
    "        ###K-Fold ensemble: Saved k best model for k dataloader\n",
    "        print(\"===================Best Fold:{} Saved, Acc:{}==================\".format(fold,max_acc))\n",
    "        torch.save(best_model_dict, \"./Kmnist_saved_model/Fold{}_loss{:.4f}_acc{:.3f}\".format(fold,min_loss*1e3,max_acc*1e2))\n",
    "        print(\"======================================================\")\n",
    "        \n",
    "        del model\n",
    "            \n",
    "            ###Snapshot ensemble: saved model\n",
    "#             if ep!=0 and ep%period == 0:\n",
    "# #                 ensemble_models.append(best_model_dict)\n",
    "#                 model_id = ep//period\n",
    "#                 print(\"===================Best Model{} Saved, Acc:{}==================\".format(model_id,max_acc))\n",
    "#                 torch.save(best_model_dict, \"./Kmnist_saved_model/model{}_ep{}_acc{:.4f}\".format(model_id,ep,max_acc))\n",
    "#                 print(\"======================================================\")\n",
    "#                 max_acc = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Native classifier Emsemble inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms.Normalize(mean=[0.08229437],std=[0.23876116]) #train dataset dist\n",
    "# transforms.Normalize(mean=[0.09549136],std=[0.24336776]) #dig_augmented distribution        \n",
    "# transforms.Normalize(mean=[0.08889289],std=[0.24106446])  #train_large dataset distribution\n",
    "\n",
    "\n",
    "ensemble_root = \"./Kmnist_saved_model/emsemble/20_fold_simple_cnn_ensemble\"   #model-> 1 fc(512) + dropout(0.1)\n",
    "ensemble_models = []\n",
    "\n",
    "ensemble_root_dig = \"./Kmnist_saved_model/emsemble/dig_aug_ensemble\"    #model-> 1 fc(256) + dropout(0.2)\n",
    "ensemble_models_dig = []\n",
    "\n",
    "data_num = 0\n",
    "acc = 0\n",
    "\n",
    "mean,std = 0.08229437, 0.23876116\n",
    "mean_dig,std_dig = 0.09549136, 0.24336776\n",
    "mean_large,std_large = 0.08889289, 0.24106446\n",
    "\n",
    "vr = 1\n",
    "# indices = np.arange(60000)\n",
    "# test_dataset = KMnistDataset(data_len=None,is_validate=True, validate_rate=vr,indices=indices)\n",
    "indices = np.arange(120000)\n",
    "test_dataset = KMnistDataset_binary_aid(data_len=None,is_validate=True, validate_rate=vr,indices=indices)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=12)\n",
    "\n",
    "native_model = convNet_native(in_channels=1)\n",
    "native_model.cuda()\n",
    "native_model.load_state_dict(torch.load(\"./Kmnist_saved_model/emsemble/native_classifier/Fold0_loss0.0242_acc_b99.704_without_aug\"))\n",
    "native_model.eval()   \n",
    "\n",
    "\n",
    "for file_name in os.listdir(ensemble_root):\n",
    "    if file_name[4:6] != \"18\":\n",
    "        continue\n",
    "    model = convNet_tmp(in_channels=1)\n",
    "    model.cuda()\n",
    "    model.load_state_dict(torch.load(\"{}/{}\".format(ensemble_root,file_name)))\n",
    "    model.eval()\n",
    "    ensemble_models.append(model)\n",
    "\n",
    "for file_name in os.listdir(ensemble_root_dig):\n",
    "    if file_name[4] != \"0\":\n",
    "        continue\n",
    "    model = convNet(in_channels=1)\n",
    "    model.cuda()\n",
    "    model.load_state_dict(torch.load(\"{}/{}\".format(ensemble_root_dig,file_name)))\n",
    "    model.eval()\n",
    "    ensemble_models_dig.append(model)\n",
    "    \n",
    "### Test Native Classifier\n",
    "with torch.no_grad():\n",
    "    for idx,data in enumerate(test_loader):\n",
    "        ###Classify native or not\n",
    "        img, target_b, target = data\n",
    "        img, target = img.to(device), target.to(device,dtype=torch.long)\n",
    "        _,pred_native = torch.max(native_model(img),dim=1)  #(batch_num,)\n",
    "        \n",
    "        norm_img = (img-mean_large)/std_large\n",
    "        ###Classify by normal model, Average Ensemble\n",
    "        pred_list = torch.Tensor([]).to(device)\n",
    "        model_num = len(ensemble_models)\n",
    "        for i in range(model_num):\n",
    "            pred = ensemble_models[i](norm_img) #(batch_num,10)\n",
    "            pred_list = torch.cat((pred_list,pred.unsqueeze(2)),dim=2) #pred_list: (batch_num,10,model_num)\n",
    "        pred = torch.mean(pred_list,dim=2)   #(batch,10)\n",
    "        _,pred_class = torch.max(pred.data, 1)   #(batch_num,)\n",
    "        \n",
    "        \n",
    "        norm_img = (img-mean_large)/std_large\n",
    "        ###Classify by dig_aug_model, Average Ensemble\n",
    "        pred_list = torch.Tensor([]).to(device)\n",
    "        model_num = len(ensemble_models_dig)\n",
    "        for i in range(model_num):\n",
    "            pred = ensemble_models_dig[i](norm_img) #(batch_num,10)\n",
    "            pred_list = torch.cat((pred_list,pred.unsqueeze(2)),dim=2) #pred_list: (batch_num,10,model_num)\n",
    "        pred = torch.mean(pred_list,dim=2)   #(batch,10)\n",
    "        _,pred_class_dig = torch.max(pred.data, 1)   #(batch_num,)\n",
    "        \n",
    "        ###Make final result tensor\n",
    "        native_mask = pred_native    #(batch_num,)  ex: ([1,0,0,1,0])\n",
    "        nonnative_mask = torch.ones([img.size(0),], dtype=torch.long).to(device) - native_mask  #(batch_num,) ex:([0,1,1,0,1])\n",
    "        \n",
    "        r1 = (pred_class*native_mask)  #a*b = torch.mul(a,b)\n",
    "        r2 = (pred_class_dig*nonnative_mask)\n",
    "        pred_final = (r1+r2).to(torch.long)  #(batch_num,)\n",
    "\n",
    "#         print(\"model1:\",pred_class)\n",
    "#         print(\"model2:\",pred_class_dig)\n",
    "#         print(\"mask:\",native_mask)\n",
    "#         print(\"non_mask:\",nonnative_mask)\n",
    "#         print(\"r1:\",r1)\n",
    "#         print(\"r2:\",r2)\n",
    "#         print(\"result:\",result)\n",
    "#         print(\"target:\",target)\n",
    "#         stop\n",
    "    \n",
    "#         acc += (pred_native).sum().item()\n",
    "        acc += (pred_final == target).sum().item()\n",
    "        data_num += img.size(0)\n",
    "\n",
    "#     val_loss /= data_num\n",
    "    acc /= data_num\n",
    "    print(\"Acc:{:.4f}%\".format(acc*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_root = \"/home/ccchang/localization_net/Kmnist_saved_model/emsemble/5_fold_ep80_lr1e-2\"\n",
    "ensemble_models = []\n",
    "epochs = 500\n",
    "period = 100\n",
    "model_num = epochs//period\n",
    "model = 5\n",
    "data_num = 0\n",
    "acc = 0\n",
    "\n",
    "for file_name in os.listdir(ensemble_root):\n",
    "    model = convNet(in_channels=1)\n",
    "    model.cuda()\n",
    "    model.load_state_dict(torch.load(\"{}/{}\".format(ensemble_root,file_name)))\n",
    "    model.eval()\n",
    "    ensemble_models.append(model)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, data in enumerate(validate_loader):\n",
    "        img, target = data\n",
    "        img, target = img.to(device), target.to(device,dtype=torch.long)\n",
    "\n",
    "        ###Single model\n",
    "#         pred = model(img)\n",
    "#         _,pred_class = torch.max(pred.data, 1)\n",
    "        \n",
    "        ###Average Ensemble\n",
    "#         pred_list = torch.Tensor([]).to(device)\n",
    "#         for i in range(model_num):\n",
    "#             pred = ensemble_models[i](img) #(batch_num,10)\n",
    "#             pred_list = torch.cat((pred_list,pred.unsqueeze(2)),dim=2) #pred_list: (batch_num,10,model_num)\n",
    "#         pred = torch.mean(pred_list,dim=2)   #(batch,10)\n",
    "        \n",
    "#         _,pred_class = torch.max(pred.data, 1)   #(batch_num,)\n",
    "#         val_loss += criterion(pred, target)\n",
    "\n",
    "        ###Voting Ensemble\n",
    "        pred_list = torch.LongTensor([]).to(device)\n",
    "        for i in range(model_num):\n",
    "            pred = ensemble_models[i](img) #(batch_num,10)\n",
    "            _,pred_class = torch.max(pred.data, 1)   #(batch_num,)\n",
    "            pred_list = torch.cat((pred_list,pred_class.unsqueeze(1)),dim=1)\n",
    "            \n",
    "        pred_class_list = torch.LongTensor([]).to(device)\n",
    "        for i in range(img.size(0)):\n",
    "            pred_np = pred_list[i].cpu().numpy()\n",
    "            unique_class,count = np.unique(pred_np,return_counts=True)\n",
    "            unique_class = np.array(unique_class[np.argmax(count)]).reshape(-1)   #unique class shape(1,)\n",
    "            class_voted= torch.from_numpy(unique_class).to(device)    #(1,)\n",
    "            pred_class_list = torch.cat((pred_class_list,class_voted))    \n",
    "    \n",
    "#         acc += (pred_class == target).sum().item()\n",
    "        acc += (pred_class_list == target).sum().item()\n",
    "        data_num += img.size(0)\n",
    "\n",
    "#     val_loss /= data_num\n",
    "    acc /= data_num\n",
    "    print(\"Acc:{:.4f}%\".format(acc*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.data = global_dataimport torch\n",
    "import numpy as np\n",
    "\n",
    "t1 = torch.Tensor([[1,2,3,4],[4,3,2,1],[1,5,3,3]])  #(3,4)\n",
    "t1 = t1.unsqueeze(2)\n",
    "\n",
    "t_list = torch.Tensor([])\n",
    "\n",
    "for i in range(3):\n",
    "    t_list = torch.cat((t_list,t1),dim=2)\n",
    "\n",
    "print(t_list.size())\n",
    "print(t_list)\n",
    "t_list = torch.mean(t_list,dim=2)\n",
    "print(t_list.size())\n",
    "print(t_list)\n",
    "\n",
    "\n",
    "# n1 = t1.cpu().numpy()\n",
    "\n",
    "# n1, count = np.unique(n1,return_counts=True,axis=0)\n",
    "# print(count)\n",
    "# n1 = np.argmax(count)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
